<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.49" />
  <meta name="author" content="Jerid Francom">

  
  
  
  
    
  
  <meta name="description" content="In this post I will provide an overview of the process of taking raw text and meta-data and organizing them into a tidy data set; that is, a tabular data format where each row is an observation and each column a corresponding attribute of the data.">

  
  <link rel="alternate" hreflang="en-us" href="https://francojc.github.io/2017/12/01/curate-language-data-organizing-meta-data/">

  


  

  

  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href=//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono>
  

  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/my-styles.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-57189160-2', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="https://francojc.github.io/index.xml" type="application/rss+xml" title="francojc ⟲">
  <link rel="feed" href="https://francojc.github.io/index.xml" type="application/rss+xml" title="francojc ⟲">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://francojc.github.io/2017/12/01/curate-language-data-organizing-meta-data/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="francojc ⟲">
  <meta property="og:url" content="https://francojc.github.io/2017/12/01/curate-language-data-organizing-meta-data/">
  <meta property="og:title" content="Curate language data (1/2): organizing meta-data | francojc ⟲">
  <meta property="og:description" content="In this post I will provide an overview of the process of taking raw text and meta-data and organizing them into a tidy data set; that is, a tabular data format where each row is an observation and each column a corresponding attribute of the data.">
  
  
    
  <meta property="og:image" content="https://francojc.github.io/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-12-01T00:00:00&#43;00:00">
  
  <meta property="article:modified_time" content="2017-12-01T00:00:00&#43;00:00">
  

  

  

  <title>Curate language data (1/2): organizing meta-data | francojc ⟲</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">francojc ⟲</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  





  <div class="article-container">
    <h1 itemprop="name">Curate language data (1/2): organizing meta-data</h1>

    

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jerid Francom">
  </span>
  

  <span class="article-date">
    
    <meta content="2017-12-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">
    <time datetime="2017-12-01 00:00:00 &#43;0000 UTC" itemprop="dateModified">
      Fri, Dec 1, 2017
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Jerid Francom">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    45 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    
    <a href="/categories/r/">R</a>, 
    
    <a href="/categories/recipe/">Recipe</a>
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      <link href="/rmarkdown-libs/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="/rmarkdown-libs/pagedtable/js/pagedtable.js"></script>


<!-- TODO:
- explore the possibility of adding an image left or right justified with flowing text
- 
-->
<p>When working with raw data, whether is comes from a corpus repository, web download, or a web scrape, it is important to recognize that the attributes that we want to organize can be stored or represented in various formats. The three I will cover here have to do with meta-data that is: (1) contained in the file name of a set of corpus files, (2) embedded in the corpus documents inline with the corpus text, and (3) stored separate from the the text data. Our goal will be to wrangle this information into a tidy dataset format where each row is an observation and each column a corresponding attribute of the data.</p>
<div class="alert alert-note">
  <p>The following code is available on GitHub <a href="https://github.com/francojc/recipes-curate_data"><code>recipes-curate_data</code></a> and is built on the <code>recipes-project_template</code> I have discussed in detail <a href="https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/">here</a> and made accessible <a href="https://github.com/francojc/recipes-project_template.git">here</a>. I encourage you to follow along by downloading the <code>recipes-project_template</code> with <code>git</code> from the Terminal or create a new RStudio R Project and select the “Version Control” option.</p>

</div>

<!-- Another important step when tidying raw data is to be aware of the potential erroneous data in the text that may be present that we would like to remove. For example, a corpus may contain resource-particular information that is not needed for the purposes of the analysis (ex. sound file alignment timestamps) or may be part of a scheme that depends on particular software to interpret (ex. formatting for CHAT software). We will also work to clean the data of the extraneous elements as well. -->
<div id="running-text-with-meta-data-in-file-names" class="section level2">
<h2>Running text with meta-data in file names</h2>
<p>A common format for storing meta-data for corpora is in the file names of the corpus documents. When this is the approach of the corpus designer, the names will contain the relevant attributes in some regular format, usually using some common character as the delimiter between the distinct attribute elements.</p>
<div id="download-corpus-data" class="section level3">
<h3>Download corpus data</h3>
<p>The <a href="https://github.com/francojc/activ-es">ACTIV-ES Corpus</a> is structured this way. ACTIV-ES is a corpus of TV/film transcripts from Argentina, Mexico, and Spain. Let’s use this corpus as an example. First we need to download the data. The ACTIV-ES corpus is stored in a GitHub repository. We can download the entire corpus using <code>git</code> to clone the repository, or we can access the specific corpus format (plain-text or part-of-speech annotated) as a compressed <code>.zip</code> file. Let’s download the compressed file for the plain text data. Navigate to the <code>https://github.com/francojc/activ-es/blob/master/activ-es-v.02/corpus/plain.zip</code> file and then copy the link for the ‘Download’ button. We can use the <code>get_zip_data()</code> function we developed in the <a href="https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/">Acquiring data for language research (1/3): direct downloads</a> post.</p>
<pre class="r"><code>get_zip_data(url = &quot;https://github.com/francojc/activ-es/raw/master/activ-es-v.02/corpus/plain.zip&quot;, 
             target_dir = &quot;data/original/actives/plain&quot;)</code></pre>
<p>Taking a look at the <code>data/original/actives/plain/</code> directory we can see the files. Below is a subset of files from each of the three countries.</p>
<pre><code>es_Argentina_2008_Lluvia_movie_Drama_1194615.run
es_Argentina_2008_Los-paranoicos_movie_Comedy_1178654.run
es_Mexico_2008_Rudo-y-Cursi_movie_Comedy_405393.run
es_Mexico_2009_Sin-nombre_movie_Adventure_1127715.run
es_Spain_2010_También-la-lluvia_movie_Drama_1422032.run
es_Spain_2010_Tres-metros-sobre-el-cielo_movie_Drama_1648216.run</code></pre>
</div>
<div id="tidy-the-corpus" class="section level3">
<h3>Tidy the corpus</h3>
<p>Each of the meta-data attributes is separated by an underscore <code>_</code>. The extension on these files is <code>.run</code>. There is nothing special about this extension, the data is plain text, but it is used to contrast the ‘running text’ version of these files with similar names that have linguistic annotations associated in other versions of the corpus. The delimited elements correspond to <code>language</code>, <code>country</code>, <code>year</code>, <code>title</code>, <code>type</code>, <code>genre</code>, and <code>imdb_id</code>.</p>
<p>Ideally we want a data set with columns for each of these attributes in the file names and an extra two columns for the <code>text</code> itself and an id to distinguish each document <code>doc_id</code>. The <a href="https://CRAN.R-project.org/package=readtext">readtext</a> package comes in handy here. So let’s load (or install) this package to read the corpus files and the tidyverse package for other miscellaneous helper functions.</p>
<pre class="r"><code>pacman::p_load(readtext, tidyverse) # use the pacman package to load-install</code></pre>
<p>The <code>readtext()</code> function is quite versatile. It allows us to read multiple files simultaneously and organize the data in a tidy dataset. The files argument will allow us to add the path to the directory where the files are located and use a pattern matching syntax known as a <a href="https://en.wikipedia.org/wiki/Regular_expression">Regular Expressions</a> to match only the files we want to extract the data from. Regular expressions are a powerful tool for manipulating character strings. Getting familiar with how they work is highly recommended.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> We will see them in action at various points throughout the rest of this series. In this case we want all the files from the <code>data/original/actives/plain/</code> directory that have the extension <code>.run</code>. So we using the Kleene start <code>*</code> as a wildcard match in combination with <code>.run</code> to match all files that end in <code>.run</code>.</p>
<p>Furthermore, the <code>readtext()</code> function allows for us to specify where the meta-data is to be found with the <code>docvarsfrom</code> argument, in our case <code>&quot;filenames&quot;</code>. The default separator value is the underscore, so we do not have to add this argument. In the case, however, the the separator is not an underscore, you will add this argument with the separator value necessary. The actual names we want to give to the attributes can be added with the <code>docvarnames</code> argument. Note that the <code>docvarnames</code> argument takes a character vector as a value. Remember to create a character vector we use the <code>c()</code> function with each element quoted.</p>
<pre class="r"><code>aes &lt;- 
  readtext(file = &quot;data/original/actives/plain/*.run&quot;, # read each file .run
           docvarsfrom = &quot;filenames&quot;, # get attributes from filename
           docvarnames = c(&quot;language&quot;, &quot;country&quot;, &quot;year&quot;, &quot;title&quot;, &quot;type&quot;, &quot;genre&quot;, &quot;imdb_id&quot;)) # add the column names we want for each attribute

glimpse(aes) # preview structure of the object</code></pre>
<pre class="plain"><code>Observations: 430
Variables: 9
$ doc_id   &lt;chr&gt; &quot;es_Argentina_1950_Esposa-último-modelo_movie_n_199500.run&quot;, &quot;es_Arge...
$ text     &lt;chr&gt; &quot;No está , señora . Aquí tampoco . No aparece , señora . ¿ Dónde se ha...
$ language &lt;chr&gt; &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;...
$ country  &lt;chr&gt; &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Arge...
$ year     &lt;int&gt; 1950, 1952, 1955, 1965, 1969, 1973, 1975, 1977, 1979, 1980, 1981, 1983...
$ title    &lt;chr&gt; &quot;Esposa-último-modelo&quot;, &quot;No-abras-nunca-esa-puerta&quot;, &quot;El-amor-nunca-m...
$ type     &lt;chr&gt; &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;video-...
$ genre    &lt;chr&gt; &quot;n&quot;, &quot;Mystery&quot;, &quot;Drama&quot;, &quot;Documentary&quot;, &quot;Horror&quot;, &quot;Adventure&quot;, &quot;Drama&quot;...
$ imdb_id  &lt;int&gt; 199500, 184782, 47823, 282622, 62433, 70250, 71897, 333883, 333954, 17...</code></pre>
<p>The output from <code>glimpse(aes)</code> shows us that there are 430 observations and 9 attributes corresponding to the 430 files in the corpus and the 7 meta-data information attributes in the file names plus the added columns <code>doc_id</code> and <code>text</code> which contain the name of the file and the text in the file for each file. The information in the <code>doc_id</code> is captured in our meta-data, yet the values are not ideal –seeing as they are quite long and informationally redundant. Although not strictly necessary, let’s change the <code>doc_id</code> values to unique numeric values. To transform the data overwriting <code>doc_id</code> with numerical values we can use the <code>mutate()</code> function from the tidyverse package in combination with the <code>row_number()</code> function.</p>
<pre class="r"><code>aes &lt;- 
  aes %&gt;% 
  mutate(doc_id = row_number()) # change doc_id to numbers

glimpse(aes) # preview structure of the object</code></pre>
<pre class="plain"><code>Observations: 430
Variables: 9
$ doc_id   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,...
$ text     &lt;chr&gt; &quot;No está , señora . Aquí tampoco . No aparece , señora . ¿ Dónde se ha...
$ language &lt;chr&gt; &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;, &quot;es&quot;...
$ country  &lt;chr&gt; &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Argentina&quot;, &quot;Arge...
$ year     &lt;int&gt; 1950, 1952, 1955, 1965, 1969, 1973, 1975, 1977, 1979, 1980, 1981, 1983...
$ title    &lt;chr&gt; &quot;Esposa-último-modelo&quot;, &quot;No-abras-nunca-esa-puerta&quot;, &quot;El-amor-nunca-m...
$ type     &lt;chr&gt; &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;movie&quot;, &quot;video-...
$ genre    &lt;chr&gt; &quot;n&quot;, &quot;Mystery&quot;, &quot;Drama&quot;, &quot;Documentary&quot;, &quot;Horror&quot;, &quot;Adventure&quot;, &quot;Drama&quot;...
$ imdb_id  &lt;int&gt; 199500, 184782, 47823, 282622, 62433, 70250, 71897, 333883, 333954, 17...</code></pre>
</div>
<div id="explore-the-tidy-dataset" class="section level3">
<h3>Explore the tidy dataset</h3>
<p>Now that we have the data in a tidy format where each row is one of our corpus files and each column is a meta-data attribute that describes each corpus file, let’s do some quick exploration of the distribution of the data to get a better feel for what our corpus is like. One thing we can do is to calculate the size of the corpus. A rudimentary approach to corpus size is the number of word tokens. The <a href="http://dx.doi.org/10.21105/joss.00037">tidytext</a> package provides a very useful function <code>unnest_tokens()</code> that provides a simple and efficient way to tokenize text while maintaining the tidy structure we have created. In combination with a set of functions from the tidyverse package, we can tokenize the text into words and count the number of words (<code>count()</code>).</p>
<p>Let’s take this in two steps so you can appreciate what <code>unnest_tokens()</code> does. First load (or install) tidytext.</p>
<pre class="r"><code>pacman::p_load(tidytext) # use the pacman package to load-install</code></pre>
<p>Now let’s tokenize the <code>text</code> column into word terms and preview the first 25 rows in the output.</p>
<pre class="r"><code>aes_tokens &lt;- 
  aes %&gt;% 
  unnest_tokens(output = terms, input = text) # tokenize `text` into words `terms`
aes_tokens %&gt;% 
  head(25) # view first 25 tokenized terms</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["doc_id"],"name":[1],"type":["int"],"align":["right"]},{"label":["language"],"name":[2],"type":["chr"],"align":["left"]},{"label":["country"],"name":[3],"type":["chr"],"align":["left"]},{"label":["year"],"name":[4],"type":["int"],"align":["right"]},{"label":["title"],"name":[5],"type":["chr"],"align":["left"]},{"label":["type"],"name":[6],"type":["chr"],"align":["left"]},{"label":["genre"],"name":[7],"type":["chr"],"align":["left"]},{"label":["imdb_id"],"name":[8],"type":["int"],"align":["right"]},{"label":["terms"],"name":[9],"type":["chr"],"align":["left"]}],"data":[{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"no"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"está"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"señora"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"aquí"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"tampoco"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"no"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"aparece"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"señora"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"dónde"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"se"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"habrá"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"metido"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"no"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"está"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"doña"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"carlota"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"no"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"aparece"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"por"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"ninguna"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"parte"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"qué"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"busca"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"ahí"},{"1":"1","2":"es","3":"Argentina","4":"1950","5":"Esposa-último-modelo","6":"movie","7":"n","8":"199500","9":"y"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We see in the previous table that a column <code>terms</code> has replaced <code>text</code> in our tidy dataset. The meta-data, however, is still in tact.</p>
<div class="alert alert-note">
  <p>The <code>unnest_tokens()</code> function from <code>tidytext</code> is very flexible. Here we have used the default arguments which produce word tokens. There are many other tokenization parameters that can be used, and we will use, to create sentence tokens, ngram tokens, and custom tokenization schemes. View <code>?unnest_tokens</code> to find out more in the R documentation.</p>

</div>

<p>After applying the <code>unnest_tokens()</code> function in the previous code, the rows correspond to tokenized words. Therefore the number of rows corresponds to the total number of words in the corpus. To find the total number of words we can use the <code>count()</code> function.</p>
<pre class="r"><code>aes_tokens %&gt;% 
  count() # count terms</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["n"],"name":[1],"type":["int"],"align":["right"]}],"data":[{"1":"2642338"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The <code>count()</code> function can be used with a data frame, like our <code>aes_tokens</code> object, to group our rows by the values of a particular column. A practical application for this functionality is to group the rows (word terms) by the values of <code>country</code> (‘Argentina’, ‘Mexico’, and ‘Spain’). This will give us the number of words in each country sub-corpus.</p>
<pre class="r"><code>aes_tokens %&gt;% 
  count(country) # count terms by `country`</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["country"],"name":[1],"type":["chr"],"align":["left"]},{"label":["n"],"name":[2],"type":["int"],"align":["right"]}],"data":[{"1":"Argentina","2":"838976"},{"1":"Mexico","2":"754955"},{"1":"Spain","2":"1048407"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>So now we know the total word count of the corpus and the number of words in each country sub-corpus. If we would like to have a description of the proportion of words from each sub-corpus in the total corpus, we can use the <code>mutate()</code> function to create a new column <code>prop</code> which calculates the total size of the corpus (<code>sum(n)</code>) and then divides each sub-corpus size (<code>n</code>) by this number.</p>
<pre class="r"><code>aes_country_props &lt;- 
  aes_tokens %&gt;% 
  count(country) %&gt;% # count terms by `country`
  mutate(prop = n / sum(n) ) # add the word term proportion for each country
aes_country_props</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["country"],"name":[1],"type":["chr"],"align":["left"]},{"label":["n"],"name":[2],"type":["int"],"align":["right"]},{"label":["prop"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"Argentina","2":"838976","3":"0.3175127"},{"1":"Mexico","2":"754955","3":"0.2857148"},{"1":"Spain","2":"1048407","3":"0.3967725"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>As we have seen in the previous examples tidy datasets are easy to work with. Another advantage to data frames is that we can use them to create graphics using the <a href="http://ggplot2.org">ggplot2</a> package. ggplot2 is a powerful package for creating graphics in R that applies what is known as the <a href="https://ramnathv.github.io/pycon2014-r/visualize/ggplot2.html">‘Grammar of Graphics’</a>. The Grammar of Graphics recognizes that there are three principle components to any graphic: (1) data, (2) mappings or ‘aesthetics’ as they are called, and (3) geometries, or ‘geoms’. Data is the data frame which contains our observations (rows) and our variables (columns). We connect certain variables of interest from our data set to certain parameters in the visual space. Typical parameters include the ‘x-axis’ and the ‘y-axis’. The x-axis corresponds to the horizontal plane and the y-axis the vertical plane. This sets up a base coordinate system for visualizing the data. Once our data has been mapped to a visual space, we then designate an appropriate geometry to represent this space (bar plot, line graphs, scatter plots, etc). There are many <a href="http://ggplot2.tidyverse.org/reference/#section-layer-geoms">geometries available in ggplot2 for relevant mapping types</a>.</p>
<p>Let’s visualize the <code>aes_country_props</code> object as a bar graph, as an example. ggplot2 is included as part of the tidyverse package so we already have access to it. So first we pass the <code>aes_country_props</code> data frame to the <code>ggplot()</code> function. Then we map the x-axis to the <code>country</code> column and the y-axis to the <code>prop</code> column. This mapping is then passed with the plus <code>+</code> operator to the <code>geom_col()</code> function to visualize the mapping in columns, or bars.</p>
<pre class="r"><code>aes_country_props %&gt;% # pass the data frame as our data source
  ggplot(aes(x = country, y = prop)) + # create x- and y-axis mappings
  geom_col() # visualize a column-wise geometry</code></pre>
<p>The above code will do the heavy lifting to create our plot. Below I’ve added the <code>labs()</code> function to create a more informative graphic with prettier x- and y-axis labels and a title and subtitle.</p>
<pre class="r"><code>aes_country_props %&gt;% # pass the data frame as our data source
  ggplot(aes(x = country, y = prop)) + # create x- and y-axis mappings
  geom_col() + # visualize a column-wise geometry
  labs(x = &quot;Country&quot;, y = &quot;Proportion (%)&quot;, title = &quot;ACTIV-ES Corpus Distribution&quot;, subtitle = &quot;Proportion of words in each country sub-corpus&quot;)</code></pre>
<p><img src="/post/2017-12-01-curate-language-data-organizing-metadata_files/figure-html/aes-country-plot-labs-1.png" width="100%" /></p>
<p>In this example we covered reading a corpus and the meta-data information contained withing the file names of this corpus with the readtext package. We then did some quick exploratory work to find the corpus size and the proportions of the corpus by country sub-corpus with the tidytext package and assorted functions from the tidyverse package. We rounded things out with a brief introduction to the ggplot2 package which we used to visualize the country sub-corpus proportions.</p>
</div>
</div>
<div id="running-text-with-inline-meta-data" class="section level2">
<h2>Running text with inline meta-data</h2>
<p>In the previous example, our corpus contained meta-data stored in the individual file names of the corpus. In some other cases the meta-data is stored inline with the corpus text itself. The goal in cases such as these is to separate the meta-data from the text and coerce all the information into a tidy dataset.</p>
<div id="download-corpus-data-1" class="section level3">
<h3>Download corpus data</h3>
<p>As an example we will work with the The Switchboard Dialog Act Corpus (SDAC) which extends the <a href="https://catalog.ldc.upenn.edu/LDC97S62">Switchboard Corpus</a> with speech act annotation. The SDAC dialogues (<code>swb1_dialogact_annot.tar.gz</code>) are available as a <a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">free download from the LDC</a>. The dialogues are contained within a compressed <code>.tar.gz</code> file. This file can be downloaded manually and its contents extracted to disk, but since we are working to create a reproducible workflow we will approach this task programmatically.</p>
<p>We have an available custom function that deals with <code>.zip</code> compressed files <code>get_zip_data()</code> but we need a function that works on <code>.tar.gz</code> files. R has a function to extract <code>.tar.gz</code> files <code>untar()</code> that we can use to mimic the same functionality as the <code>unzip()</code> function used in the <code>get_zip_data()</code> function. Instead of writing a new custom function to deal specifically with <code>.tar.gz</code> files, I’ve created a function that deals with both compressed file formats, named it <code>get_compressed_data()</code>, and added it to my <code>functions/acquire_functions.R</code> file.</p>
<pre class="r"><code>get_compressed_data &lt;- function(url, target_dir, force = FALSE) {
  # Get the extension of the target file
  ext &lt;- tools::file_ext(url)
  # Check to see if the target file is a compressed file
  if(!ext %in% c(&quot;zip&quot;, &quot;gz&quot;, &quot;tar&quot;)) stop(&quot;Target file given is not supported&quot;)
  # Check to see if the data already exists
  if(!dir.exists(target_dir) | force == TRUE) { # if data does not exist, download/ decompress
    cat(&quot;Creating target data directory \n&quot;) # print status message
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&quot;Downloading data... \n&quot;) # print status message
    temp &lt;- tempfile() # create a temporary space for the file to be written to
    download.file(url = url, destfile = temp) # download the data to the temp file
    # Decompress the temp file in the target directory
    if(ext == &quot;zip&quot;) {
      unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE) # zip files
    } else {
      untar(tarfile = temp, exdir = target_dir) # tar, gz files
    }
    cat(&quot;Data downloaded! \n&quot;) # print status message
  } else { # if data exists, don&#39;t download it again
    cat(&quot;Data already exists \n&quot;) # print status message
  }
}</code></pre>
<p>Once this function is loaded into R, either by sourcing the <code>functions/aquire_functions.R</code> file (<code>source(&quot;functions/acquire_functions.R&quot;)</code>) or running the code directly, we apply the function to the resource URL targeting the <code>data/original/sdac/</code> directory as the extraction location.</p>
<pre class="r"><code>get_compressed_data(url = &quot;https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz&quot;, 
                    target_dir = &quot;data/original/sdac/&quot;)</code></pre>
<p>The main directory structure of the <code>sdac/</code> data looks like this:</p>
<pre class="plain"><code>.
├── README
├── doc
├── sw00utt
├── sw01utt
├── sw02utt
├── sw03utt
├── sw04utt
├── sw05utt
├── sw06utt
├── sw07utt
├── sw08utt
├── sw09utt
├── sw10utt
├── sw11utt
├── sw12utt
└── sw13utt

15 directories, 1 file</code></pre>
<p>The <code>README</code> file contains basic information about the resource, the <code>doc/</code> directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with <code>sw...</code> contain individual conversation files. Here’s a peek at internal structure of the first couple directories.</p>
<pre class="plain"><code>.
├── README
├── doc
│   └── manual.august1.html
├── sw00utt
│   ├── sw_0001_4325.utt
│   ├── sw_0002_4330.utt
│   ├── sw_0003_4103.utt
│   ├── sw_0004_4327.utt
│   ├── sw_0005_4646.utt</code></pre>
<p>Let’s take a look at the first conversation file (<code>sw_0001_4325.utt</code>) to see how it is structured.</p>
<pre class="plain"><code>*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
*x*                                                                     *x*
*x*            Copyright (C) 1995 University of Pennsylvania            *x*
*x*                                                                     *x*
*x*    The data in this file are part of a preliminary version of the   *x*
*x*    Penn Treebank Corpus and should not be redistributed.  Any       *x*
*x*    research using this corpus or based on it should acknowledge     *x*
*x*    that fact, as well as the preliminary nature of the corpus.      *x*
*x*                                                                     *x*
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*


FILENAME:   4325_1632_1519
TOPIC#:     323
DATE:       920323
TRANSCRIBER:    glp
UTT_CODER:  tc
DIFFICULTY: 1
TOPICALITY: 3
NATURALNESS:    2
ECHO_FROM_B:    1
ECHO_FROM_A:    4
STATIC_ON_A:    1
STATIC_ON_B:    1
BACKGROUND_A:   1
BACKGROUND_B:   2
REMARKS:        None.

=========================================================================
  

o          A.1 utt1: Okay.  /
qw          A.1 utt2: {D So, }   

qy^d          B.2 utt1: [ [ I guess, +   

+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /</code></pre>
<p>There are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of <code>=</code> characters. Second the header contains meta-information of various types. Third, the text is interleaved with an annotation scheme.</p>
<p>Some of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let’s take a look at the <code>README</code> file. In this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 ‘DAMSL’ dialog act labels. The <code>README</code> file refers us to the <code>doc/manual.august1.html</code> file for more information on this scheme.</p>
<p>At this point we open the the <code>doc/manual.august1.html</code> file in a browser and do some investigation. We find out that ‘DAMSL’ stands for ‘Discourse Annotation and Markup System of Labeling’ and that the first characters of each line of the conversation text correspond to one or a combination of labels for each utterance. So for our first utterances we have:</p>
<pre class="plain"><code>o = &quot;Other&quot;
qw = &quot;Wh-Question&quot;
qy^d = &quot;Declarative Yes-No-Question&quot;
+ = &quot;Segment (multi-utterance)&quot;</code></pre>
<p>Each utterance is also labeled for speaker (‘A’ or ‘B’), speaker turn (‘1’, ‘2’, ‘3’, etc.), and each utterance within that turn (‘utt1’, ‘utt2’, etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.</p>
<p>Now let’s turn to the meta-data in the header. We see here that there is information about the creation of the file: ‘FILENAME’, ‘TOPIC’, ‘DATE’, etc. The <code>doc/manual.august1.html</code> file doesn’t have much to say about this information so I returned to the <a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">LDC Documentation</a> and found more information in the <a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">Online Documentation</a> section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the <code>caller_tab.csv</code> file. This tabular file does not contain column names, but the <code>caller_doc.txt</code> does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the ‘FILENAME’ information contained three pieces of useful information delimited by underscores <code>_</code>.</p>
<pre class="plain"><code>*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*


FILENAME:   4325_1632_1519
TOPIC#:     323
DATE:       920323
TRANSCRIBER:    glp</code></pre>
<p>The first information is the document id (<code>4325</code>), the second and third correspond to the speaker number: the first being speaker A (<code>1632</code>) and the second speaker B (<code>1519</code>).</p>
</div>
<div id="tidy-the-corpus-1" class="section level3">
<h3>Tidy the corpus</h3>
<p>In sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of <code>=</code> characters. The header section contains a ‘FILENAME’ line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let’s set out to create a tidy dataset with the following column structure:</p>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["doc_id"],"name":[1],"type":["chr"],"align":["left"]},{"label":["damsl_tag"],"name":[2],"type":["chr"],"align":["left"]},{"label":["speaker"],"name":[3],"type":["chr"],"align":["left"]},{"label":["turn_num"],"name":[4],"type":["chr"],"align":["left"]},{"label":["utterance_num"],"name":[5],"type":["chr"],"align":["left"]},{"label":["utterance_text"],"name":[6],"type":["chr"],"align":["left"]},{"label":["speaker_id"],"name":[7],"type":["chr"],"align":["left"]}],"data":[{"1":"4325","2":"o","3":"A","4":"1","5":"1","6":"Okay.  /","7":"1632","_rn_":"1"},{"1":"4325","2":"qw","3":"A","4":"1","5":"2","6":"{D So, }","7":"1632","_rn_":"2"},{"1":"4325","2":"qy^d","3":"B","4":"2","5":"1","6":"[ [ I guess, +","7":"1519","_rn_":"3"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Let’s begin by reading one of the conversation files into R as a character vector using the <code>read_lines()</code> function.</p>
<pre class="r"><code>doc &lt;- read_lines(file = &quot;data/original/sdac/sw00utt/sw_0001_4325.utt&quot;) # read file by lines</code></pre>
<p>To isolate the vector element that contains the document and speaker ids, we use <code>str_detect()</code> from the <a href="https://CRAN.R-project.org/package=stringr">stringr</a> package. This function takes two arguments, a string and a pattern, and returns a logical value, <code>TRUE</code> if the pattern is matched or <code>FALSE</code> if not. We can use the output of this function, then, to subset the <code>doc</code> character vector and only return the vector element (line) that contains <code>digits_digits_digits</code> with a regular expression. The expression combines the digit matching operator <code>\\d</code> with the <code>+</code> operator to match 1 or more contiguous digits. We then separate three groups of <code>\\d+</code> with underscores <code>_</code>. The result is <code>\\d+_\\d+_\\d+</code>.</p>
<pre class="r"><code>pacman::p_load(stringr) # load-install `stringr` package
doc[str_detect(doc, pattern = &quot;\\d+_\\d+_\\d+&quot;)] # isolate pattern</code></pre>
<pre><code>## [1] &quot;FILENAME:\t4325_1632_1519&quot;</code></pre>
<p>The next step is to extract the three digit sequences that correspond to the <code>doc_id</code>, <code>speaker_a_id</code>, and <code>speaker_b_id</code>. First we extract the pattern that we have identified with <code>str_extract()</code> and then we can break up the single character vector into multiple parts based on the underscore <code>_</code>. The <code>str_split()</code> function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors.</p>
<pre class="r"><code>doc[str_detect(doc, &quot;\\d+_\\d+_\\d+&quot;)] %&gt;% # isolate pattern
  str_extract(pattern = &quot;\\d+_\\d+_\\d+&quot;) %&gt;% # extract the pattern
  str_split(pattern = &quot;_&quot;) # split the character vector</code></pre>
<pre><code>## [[1]]
## [1] &quot;4325&quot; &quot;1632&quot; &quot;1519&quot;</code></pre>
<p>A <strong>list</strong> is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same –hence the tabular format). In this case we have a list of length 1, whose sole element is a character vector of length 3 –one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our <code>str_split()</code> function we don’t want the results to be conflated as a single character vector blurring the distinction between the individual character vectors. If we <em>would</em> like to conflate, or <em>flatten</em> a list, we can use the <code>unlist()</code> function.</p>
<pre class="r"><code>doc[str_detect(doc, &quot;\\d+_\\d+_\\d+&quot;)] %&gt;% # isolate pattern
  str_extract(pattern = &quot;\\d+_\\d+_\\d+&quot;) %&gt;% # extract the pattern
  str_split(pattern = &quot;_&quot;) %&gt;% # split the character vector
  unlist() # flatten the list to a character vector</code></pre>
<pre><code>## [1] &quot;4325&quot; &quot;1632&quot; &quot;1519&quot;</code></pre>
<!-- In this case we won't flatten the list object just yet. Instead we want to extract the digits from our three character vectors. The `str_extract_all()` will take the list output of character vectors and allow us to provide a pattern to match and extract. Where in the `str_split()` case our pattern was a fixed character (`_`), the pattern to match for digits is more abstract --we want the contiguous sequences of digits. For this we will build a regular expression to match the digits. The expression we want is `\\d+` which means match one or more contiguous digits. Regular expressions provide hooks for many different character types. We will soon see a number of them in action as we proceed to curate the `sdac/` data. -->
<!-- ```{r sdac-doc-info-4, echo=TRUE, eval=TRUE} -->
<!-- doc[str_detect(doc, "FILENAME:")] %>%  -->
<!--   str_split(pattern = "_") %>%  -->
<!--   str_extract_all(pattern = "\\d+") -->
<!-- ``` -->
<!-- The `str_extract_all()` took our list and returned a list. At this point we can flatten and assign the results to the variable `doc_speaker_info`. -->
<!-- ```{r sdac-doc-info-5, echo=TRUE, eval=TRUE} -->
<!-- doc_speaker_info <-  -->
<!--   doc[str_detect(doc, "FILENAME:")] %>%  -->
<!--   str_split(pattern = "_") %>%  -->
<!--   str_extract_all(pattern = "\\d+") %>%  -->
<!--   unlist() -->
<!-- ``` -->
<p>Let’s flatten the list in this case, as we have a single character vector, and assign this result to <code>doc_speaker_info</code>.</p>
<pre class="r"><code>doc_speaker_info &lt;- 
  doc[str_detect(doc, &quot;\\d+_\\d+_\\d+&quot;)] %&gt;% # isolate pattern
  str_extract(pattern = &quot;\\d+_\\d+_\\d+&quot;) %&gt;% # extract the pattern
  str_split(pattern = &quot;_&quot;) %&gt;%  # split the character vector
  unlist() # flatten the list to a character vector</code></pre>
<p><code>doc_speaker_info</code> is now a character vector of length three. Let’s subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.</p>
<pre class="r"><code>doc_id &lt;- doc_speaker_info[1]
speaker_a_id &lt;- doc_speaker_info[2]
speaker_b_id &lt;- doc_speaker_info[3]</code></pre>
<p>The next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of <code>=</code> separates the header section from the text section. What we need to do is to index the point in our character vector <code>doc</code> where that line occurs and then subset the <code>doc</code> from that point until the end of the character vector. Let’s first find the point where the <code>=</code> sequence occurs. We will again use the <code>str_detect()</code> function to find the pattern we are looking for (a contiguous sequence of <code>=</code>), but then we will pass the logical result to the <code>which()</code> function which will return the element index number of this match.</p>
<pre class="r"><code>doc %&gt;% 
  str_detect(pattern = &quot;=+&quot;) %&gt;% # match 1 or more `=`
  which() # find vector index</code></pre>
<pre><code>## [1] 31</code></pre>
<p>So for this file <code>31</code> is the index in <code>doc</code> where the <code>=</code> sequence occurs. Now it is important to keep in mind that we are working with a single file from the <code>sdac/</code> data. We need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the <code>=+</code> pattern will match <code>=</code>, or <code>==</code>, or <code>===</code>, etc. it is not implausible to believe that there might be a <code>=</code> character on some other line in one of the other files. Let’s update our regular expression to avoid this potential scenario by only matching sequences of three or more <code>=</code>. In this case we will make use of the curly bracket operators <code>{}</code>.</p>
<pre class="r"><code>doc %&gt;% 
  str_detect(pattern = &quot;={3,}&quot;) %&gt;% # match 3 or more `=`
  which() # find vector index</code></pre>
<pre><code>## [1] 31</code></pre>
<p>We will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for <code>===</code>, <code>====</code>, etc.</p>
<p><code>31</code> is the index for the <code>=</code> sequence, but we want the next line to be where we start reading the text section. To do this we increment the index by 1.</p>
<pre class="r"><code>text_start_index &lt;- 
  doc %&gt;% 
  str_detect(pattern = &quot;={3,}&quot;) %&gt;% # match 3 or more `=` 
  which() # find vector index
text_start_index &lt;- text_start_index + 1 # increment index by 1</code></pre>
<p>The index for the end of the text is simply the length of the <code>doc</code> vector. We can use the <code>length()</code> function to get this index.</p>
<pre class="r"><code>text_end_index &lt;- length(doc)</code></pre>
<p>We now have the bookends, so to speak, for our text section. To extract the text we subset the <code>doc</code> vector by these indices.</p>
<pre class="r"><code>text &lt;- doc[text_start_index:text_end_index] # extract text
head(text) # preview first lines of `text`</code></pre>
<pre><code>## [1] &quot;  &quot;                                       
## [2] &quot;&quot;                                         
## [3] &quot;o          A.1 utt1: Okay.  /&quot;            
## [4] &quot;qw          A.1 utt2: {D So, }   &quot;        
## [5] &quot;&quot;                                         
## [6] &quot;qy^d          B.2 utt1: [ [ I guess, +   &quot;</code></pre>
<p>The text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the <code>str_trim()</code> function which by default will remove leading and trailing whitespace from each line.</p>
<pre class="r"><code>text &lt;- str_trim(text) # remove leading and trailing whitespace
head(text) # preview first lines of `text`</code></pre>
<pre><code>## [1] &quot;&quot;                                      
## [2] &quot;&quot;                                      
## [3] &quot;o          A.1 utt1: Okay.  /&quot;         
## [4] &quot;qw          A.1 utt2: {D So, }&quot;        
## [5] &quot;&quot;                                      
## [6] &quot;qy^d          B.2 utt1: [ [ I guess, +&quot;</code></pre>
<p>To remove blank lines we will create a logical expression to subset the <code>text</code> vector. <code>text != &quot;&quot;</code> means return TRUE where lines are not blank, and FALSE where they are.</p>
<pre class="r"><code>text &lt;- text[text != &quot;&quot;] # remove blank lines
head(text) # preview first lines of `text`</code></pre>
<pre><code>## [1] &quot;o          A.1 utt1: Okay.  /&quot;                                                                  
## [2] &quot;qw          A.1 utt2: {D So, }&quot;                                                                 
## [3] &quot;qy^d          B.2 utt1: [ [ I guess, +&quot;                                                         
## [4] &quot;+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /&quot;
## [5] &quot;+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /&quot;                        
## [6] &quot;qy          A.5 utt1: Does it say something? /&quot;</code></pre>
<p>Our first step towards a tidy dataset is to now combine the <code>doc_id</code> and each element of <code>text</code> in a data frame.</p>
<pre class="r"><code>data &lt;- data.frame(doc_id, text) # tidy format `doc_id` and `text`
head(data) # preview first lines of `text`</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["doc_id"],"name":[1],"type":["chr"],"align":["left"]},{"label":["text"],"name":[2],"type":["chr"],"align":["left"]}],"data":[{"1":"4325","2":"o          A.1 utt1: Okay.  /","_rn_":"1"},{"1":"4325","2":"qw          A.1 utt2: {D So, }","_rn_":"2"},{"1":"4325","2":"qy^d          B.2 utt1: [ [ I guess, +","_rn_":"3"},{"1":"4325","2":"+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /","_rn_":"4"},{"1":"4325","2":"+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /","_rn_":"5"},{"1":"4325","2":"qy          A.5 utt1: Does it say something? /","_rn_":"6"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>With our data now in a data frame, its time to parse the <code>text</code> column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns. To do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row of <code>data$text</code> and extract it.</p>
<p>The best way to learn regular expressions is to use them. To this end I’ve included a window to the interactive regular expression practice website <a href="https://regex101.com">regex101</a> in Figure <a href="#fig:regex">1</a>.</p>
<p>Copy the text below into the ‘TEST STRING’ field.</p>
<pre class="plain"><code>o          A.1 utt1: Okay.  /
qw          A.1 utt2: {D So, }
qy^d          B.2 utt1: [ [ I guess, +
+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /
+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
qy          A.5 utt1: Does it say something? /
sd          B.6 utt1: I think it usually does.  /
ad          B.6 utt2: You might try, {F uh, }  /
h          B.6 utt3: I don&#39;t know,  /
ad          B.6 utt4: hold it down a little longer,  /</code></pre>
<div class="figure"><span id="fig:regex"></span>
<iframe src="https://regex101.com/?flags=gm" width="100%" height="600px">
</iframe>
<p class="caption">
Figure 1: Interactive interface to the <a href="https://regex101.com">regex101</a> practice website.
</p>
</div>
<p>Now manually type the following regular expressions into the ‘REGULAR EXPRESSION’ field one-by-one (each is on a separate line). Notice what is matched as you type and when you’ve finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.</p>
<pre class="plain"><code>^.+?\s
[AB]\.\d+
utt\d+
:.+$</code></pre>
<p>As you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text. To apply these expressions to our data and extract this information into separate columns we will make use of the <code>mutate()</code> and <code>str_extract()</code> functions. <code>mutate()</code> will take our data frame and create new columns with values we match and extract from each row in the data frame with <code>str_extract()</code>. Notice that <code>str_extract()</code> is different than <code>str_extract_all()</code>. When we work with <code>mutate()</code> each row will be evaluated in turn, therefore we only need to make one match per row in <code>data$text</code>.</p>
<p>I’ve chained each of these steps in the code below, dropping the original <code>text</code> column with <code>select(-text)</code>, and overwriting <code>data</code> with the results.</p>
<pre class="r"><code>data &lt;- # extract column information from `text`
  data %&gt;% 
  mutate(damsl_tag = str_extract(string = text, pattern = &quot;^.+?\\s&quot;)) %&gt;%  # extract damsl tags
  mutate(speaker_turn = str_extract(string = text, pattern = &quot;[AB]\\.\\d+&quot;)) %&gt;% # extract speaker_turn pairs
  mutate(utterance_num = str_extract(string = text, pattern = &quot;utt\\d+&quot;)) %&gt;% # extract utterance number
  mutate(utterance_text = str_extract(string = text, pattern = &quot;:.+$&quot;)) %&gt;%  # extract utterance text
  select(-text) # drop the `text` column

glimpse(data) # preview the data set</code></pre>
<pre><code>## Observations: 159
## Variables: 5
## $ doc_id         &lt;chr&gt; &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;o &quot;, &quot;qw &quot;, &quot;qy^d &quot;, &quot;+ &quot;, &quot;+ &quot;, &quot;qy &quot;, &quot;sd &quot;,...
## $ speaker_turn   &lt;chr&gt; &quot;A.1&quot;, &quot;A.1&quot;, &quot;B.2&quot;, &quot;A.3&quot;, &quot;B.4&quot;, &quot;A.5&quot;, &quot;B.6&quot;...
## $ utterance_num  &lt;chr&gt; &quot;utt1&quot;, &quot;utt2&quot;, &quot;utt1&quot;, &quot;utt1&quot;, &quot;utt1&quot;, &quot;utt1&quot;,...
## $ utterance_text &lt;chr&gt; &quot;: Okay.  /&quot;, &quot;: {D So, }&quot;, &quot;: [ [ I guess, +&quot;,...</code></pre>
<div class="alert alert-warning">
  <p>One twist you will notice is that regular expressions in R require double backslashes (<code>\\</code>) where other programming environments use a single backslash (<code>\</code>).</p>

</div>

<p>There are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the <code>speaker_turn</code> column into <code>speaker</code> and <code>turn_num</code> columns and second we need to remove unwanted characters from the <code>damsl_tag</code>, <code>utterance_num</code>, and <code>utterance_text</code> columns.</p>
<p>To separate the values of a column into two columns we use the <code>separate()</code> function. It takes a column to separate and character vector of the names of the new columns to create. By default the values of the input column will be separated by non-alphanumeric characters. In our case this means the <code>.</code> will be our separator.</p>
<pre class="r"><code>data &lt;-
  data %&gt;% 
  separate(col = speaker_turn, into = c(&quot;speaker&quot;, &quot;turn_num&quot;)) # separate speaker_turn into distinct columns

glimpse(data) # preview the data set</code></pre>
<pre><code>## Observations: 159
## Variables: 6
## $ doc_id         &lt;chr&gt; &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;o &quot;, &quot;qw &quot;, &quot;qy^d &quot;, &quot;+ &quot;, &quot;+ &quot;, &quot;qy &quot;, &quot;sd &quot;,...
## $ speaker        &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B...
## $ turn_num       &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6...
## $ utterance_num  &lt;chr&gt; &quot;utt1&quot;, &quot;utt2&quot;, &quot;utt1&quot;, &quot;utt1&quot;, &quot;utt1&quot;, &quot;utt1&quot;,...
## $ utterance_text &lt;chr&gt; &quot;: Okay.  /&quot;, &quot;: {D So, }&quot;, &quot;: [ [ I guess, +&quot;,...</code></pre>
<p>To remove unwanted leading or trailing whitespace we apply the <code>str_trim()</code> function. For removing other characters we matching the character(s) and replace them with an empty string (<code>&quot;&quot;</code>) with the <code>str_replace()</code> function. Again, I’ve chained these functions together and overwritten <code>data</code> with the results.</p>
<pre class="r"><code>data &lt;- # clean up column information
  data %&gt;% 
  mutate(damsl_tag = str_trim(damsl_tag)) %&gt;% # remove leading/ trailing whitespace
  mutate(utterance_num = str_replace(string = utterance_num, pattern = &quot;utt&quot;, replacement = &quot;&quot;)) %&gt;% # remove &#39;utt&#39;
  mutate(utterance_text = str_replace(string = utterance_text, pattern = &quot;:\\s&quot;, replacement = &quot;&quot;)) %&gt;% # remove &#39;: &#39;
  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace

glimpse(data) # preview the data set</code></pre>
<pre><code>## Observations: 159
## Variables: 6
## $ doc_id         &lt;chr&gt; &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;o&quot;, &quot;qw&quot;, &quot;qy^d&quot;, &quot;+&quot;, &quot;+&quot;, &quot;qy&quot;, &quot;sd&quot;, &quot;ad&quot;, ...
## $ speaker        &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B...
## $ turn_num       &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6...
## $ utterance_num  &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4...
## $ utterance_text &lt;chr&gt; &quot;Okay.  /&quot;, &quot;{D So, }&quot;, &quot;[ [ I guess, +&quot;, &quot;What...</code></pre>
<p>To round out our tidy dataset for this single conversation file we will connect the <code>speaker_a_id</code> and <code>speaker_b_id</code> with speaker A and B in our current dataset adding a new column <code>speaker_id</code>. The <code>case_when()</code> function does exactly this: allows us to map rows of <code>speaker</code> with the value “A” to <code>speaker_a_id</code> and rows with value “B” to <code>speaker_b_id</code>.</p>
<pre class="r"><code>data &lt;- # link speaker with speaker_id
  data %&gt;% 
  mutate(speaker_id = case_when(
    speaker == &quot;A&quot; ~ speaker_a_id,
    speaker == &quot;B&quot; ~ speaker_b_id
  ))

glimpse(data) # preview the data set</code></pre>
<pre><code>## Observations: 159
## Variables: 7
## $ doc_id         &lt;chr&gt; &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;o&quot;, &quot;qw&quot;, &quot;qy^d&quot;, &quot;+&quot;, &quot;+&quot;, &quot;qy&quot;, &quot;sd&quot;, &quot;ad&quot;, ...
## $ speaker        &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B...
## $ turn_num       &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6...
## $ utterance_num  &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4...
## $ utterance_text &lt;chr&gt; &quot;Okay.  /&quot;, &quot;{D So, }&quot;, &quot;[ [ I guess, +&quot;, &quot;What...
## $ speaker_id     &lt;chr&gt; &quot;1632&quot;, &quot;1632&quot;, &quot;1519&quot;, &quot;1632&quot;, &quot;1519&quot;, &quot;1632&quot;,...</code></pre>
<p>We now have the tidy dataset we set out to create. But this dataset only includes on conversation file. We want to apply this code to all 1155 conversation files in the <code>sdac/</code> corpus. The approach will be to create a custom function which groups the code we’ve done for this single file and then iterative send each file from the corpus through this function and combine the results into one data frame.</p>
<p>Here’s the function with some extra code to print a progress message for each file when it runs.</p>
<pre class="r"><code>extract_sdac_metadata &lt;- function(file) {
  # Function: to read a Switchboard Corpus Dialogue file and extract meta-data
  cat(&quot;Reading&quot;, basename(file), &quot;...&quot;)
  
  # Read `file` by lines
  doc &lt;- read_lines(file) 
  
  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`
  doc_speaker_info &lt;- 
    doc[str_detect(doc, &quot;\\d+_\\d+_\\d+&quot;)] %&gt;% # isolate pattern
    str_extract(&quot;\\d+_\\d+_\\d+&quot;) %&gt;% # extract the pattern
    str_split(pattern = &quot;_&quot;) %&gt;% # split the character vector
    unlist() # flatten the list to a character vector
  doc_id &lt;- doc_speaker_info[1] # extract `doc_id`
  speaker_a_id &lt;- doc_speaker_info[2] # extract `speaker_a_id`
  speaker_b_id &lt;- doc_speaker_info[3] # extract `speaker_b_id`
  
  # Extract `text`
  text_start_index &lt;- # find where header info stops
    doc %&gt;% 
    str_detect(pattern = &quot;={3,}&quot;) %&gt;% # match 3 or more `=`
    which() # find vector index
  
  text_start_index &lt;- text_start_index + 1 # increment index by 1
  text_end_index &lt;- length(doc) # get the end of the text section
  
  text &lt;- doc[text_start_index:text_end_index] # extract text
  text &lt;- str_trim(text) # remove leading and trailing whitespace
  text &lt;- text[text != &quot;&quot;] # remove blank lines
  
  data &lt;- data.frame(doc_id, text) # tidy format `doc_id` and `text`
  
  data &lt;- # extract column information from `text`
    data %&gt;% 
    mutate(damsl_tag = str_extract(string = text, pattern = &quot;^.+?\\s&quot;)) %&gt;%  # extract damsl tags
    mutate(speaker_turn = str_extract(string = text, pattern = &quot;[AB]\\.\\d+&quot;)) %&gt;% # extract speaker_turn pairs
    mutate(utterance_num = str_extract(string = text, pattern = &quot;utt\\d+&quot;)) %&gt;% # extract utterance number
    mutate(utterance_text = str_extract(string = text, pattern = &quot;:.+$&quot;)) %&gt;%  # extract utterance text
    select(-text)
  
  data &lt;- # separate speaker_turn into distinct columns
    data %&gt;% 
    separate(col = speaker_turn, into = c(&quot;speaker&quot;, &quot;turn_num&quot;)) 
  
  data &lt;- # clean up column information
    data %&gt;% 
    mutate(damsl_tag = str_trim(damsl_tag)) %&gt;% # remove leading/ trailing whitespace
    mutate(utterance_num = str_replace(string = utterance_num, pattern = &quot;utt&quot;, replacement = &quot;&quot;)) %&gt;% # remove &#39;utt&#39;
    mutate(utterance_text = str_replace(string = utterance_text, pattern = &quot;:\\s&quot;, replacement = &quot;&quot;)) %&gt;% # remove &#39;: &#39;
    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace
  
  data &lt;- # link speaker with speaker_id
    data %&gt;% 
    mutate(speaker_id = case_when(
      speaker == &quot;A&quot; ~ speaker_a_id,
      speaker == &quot;B&quot; ~ speaker_b_id
    )) 
  cat(&quot; done.\n&quot;)
  return(data) # return the data frame object
}</code></pre>
<p>As a sanity check we will run the <code>extract_sdac_metadata()</code> function on a the conversation file we were just working on to make sure it works as expected.</p>
<pre class="r"><code>extract_sdac_metadata(file = &quot;data/original/sdac/sw00utt/sw_0001_4325.utt&quot;) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Reading sw_0001_4325.utt ... done.
## Observations: 159
## Variables: 7
## $ doc_id         &lt;chr&gt; &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;o&quot;, &quot;qw&quot;, &quot;qy^d&quot;, &quot;+&quot;, &quot;+&quot;, &quot;qy&quot;, &quot;sd&quot;, &quot;ad&quot;, ...
## $ speaker        &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B...
## $ turn_num       &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6...
## $ utterance_num  &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4...
## $ utterance_text &lt;chr&gt; &quot;Okay.  /&quot;, &quot;{D So, }&quot;, &quot;[ [ I guess, +&quot;, &quot;What...
## $ speaker_id     &lt;chr&gt; &quot;1632&quot;, &quot;1632&quot;, &quot;1519&quot;, &quot;1632&quot;, &quot;1519&quot;, &quot;1632&quot;,...</code></pre>
<p>Looks good so now it’s time to create a vector with the paths to all of the conversation files. <code>list_files()</code> interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (<code>\\.utt</code>) so we don’t accidently include other files in the corpus. <code>full.names</code> and <code>recursive</code> set to <code>TRUE</code> means we will get the full path to each file and files in all sub-directories will be returned.</p>
<pre class="r"><code>files &lt;- 
  list.files(path = &quot;data/original/sdac&quot;, # path to main directory 
             pattern = &quot;\\.utt&quot;, # files to match
             full.names = TRUE, # extract full path to each file
             recursive = TRUE) # drill down in each sub-directory of `sdac/`
head(files) # preview character vector</code></pre>
<pre><code>## [1] &quot;data/original/sdac/sw00utt/sw_0001_4325.utt&quot;
## [2] &quot;data/original/sdac/sw00utt/sw_0002_4330.utt&quot;
## [3] &quot;data/original/sdac/sw00utt/sw_0003_4103.utt&quot;
## [4] &quot;data/original/sdac/sw00utt/sw_0004_4327.utt&quot;
## [5] &quot;data/original/sdac/sw00utt/sw_0005_4646.utt&quot;
## [6] &quot;data/original/sdac/sw00utt/sw_0006_4108.utt&quot;</code></pre>
<p>To pass each conversation file in the vector of paths to our conversation files iteratively to the <code>extract_sdac_metadata()</code> function we use <code>map()</code>. This will apply the function to each conversation file and return a data frame for each. <code>bind_rows()</code> will then join the resulting data frames by rows to give us a single tidy dataset for all 1155 conversations. Note there is a lot of processing going on here so be patient.</p>
<pre class="r"><code># Read files and return a tidy dataset
sdac &lt;- 
  files %&gt;% # pass file names
  map(extract_sdac_metadata) %&gt;% # read and tidy iteratively 
  bind_rows() # bind the results into a single data frame</code></pre>
<pre class="r"><code>glimpse(sdac) # preview the dataset</code></pre>
<pre><code>## Observations: 223,606
## Variables: 7
## $ doc_id         &lt;chr&gt; &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;o&quot;, &quot;qw&quot;, &quot;qy^d&quot;, &quot;+&quot;, &quot;+&quot;, &quot;qy&quot;, &quot;sd&quot;, &quot;ad&quot;, ...
## $ speaker        &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B...
## $ turn_num       &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6...
## $ utterance_num  &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4...
## $ utterance_text &lt;chr&gt; &quot;Okay.  /&quot;, &quot;{D So, }&quot;, &quot;[ [ I guess, +&quot;, &quot;What...
## $ speaker_id     &lt;chr&gt; &quot;1632&quot;, &quot;1632&quot;, &quot;1519&quot;, &quot;1632&quot;, &quot;1519&quot;, &quot;1632&quot;,...</code></pre>
</div>
<div id="explore-the-tidy-dataset-1" class="section level3">
<h3>Explore the tidy dataset</h3>
<p>It is always a good idea to perform some diagnostics on the data to confirm the integrity of the data. One thing that can go wrong in tidying a dataset, as we’ve done here, is that our pattern matching failed and did not return what we expected. This can be because our patterns were not specific enough or can arise from transcriber/annotator error. In any case this can lead to missing values, or <code>NA</code> values. R provides the function <code>complete.cases()</code> to test for <code>NA</code> values, returning <code>TRUE</code> for rows in a data frame which do not include <code>NA</code> values. We can use this to subset the same data set to identify any rows in <code>sdac</code> dataset that are missing. Note that because we are subsetting a data frame by rows, we will add our expression to the row position in the subsetting operation, i.e. ‘data_frame_name[<strong>row</strong>, column]’.</p>
<pre class="r"><code>sdac[!complete.cases(sdac), ] # check for missing values</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["doc_id"],"name":[1],"type":["chr"],"align":["left"]},{"label":["damsl_tag"],"name":[2],"type":["chr"],"align":["left"]},{"label":["speaker"],"name":[3],"type":["chr"],"align":["left"]},{"label":["turn_num"],"name":[4],"type":["chr"],"align":["left"]},{"label":["utterance_num"],"name":[5],"type":["chr"],"align":["left"]},{"label":["utterance_text"],"name":[6],"type":["chr"],"align":["left"]},{"label":["speaker_id"],"name":[7],"type":["chr"],"align":["left"]}],"data":[],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Great! No missing data. Now let’s make sure we have captured all 1155 conversation files. We will pipe the <code>sdac$doc_id</code> column to the <code>unique()</code> function which returns the unique values of the column. Then we can get the length of that result to find out how many unique conversation files we have in our dataset.</p>
<pre class="r"><code>sdac$doc_id %&gt;% unique() %&gt;% length() # check for unique files</code></pre>
<pre><code>## [1] 1155</code></pre>
<p>Also good news, we have all 1155 conversations in the dataset.</p>
<p>Let’s find out how many individual speakers are in the dataset.</p>
<pre class="r"><code>sdac$speaker_id %&gt;% unique() %&gt;% length() # check for unique speakers</code></pre>
<pre><code>## [1] 441</code></pre>
<p>Good to know before we proceed to adding speaker meta-data from the stand-off file <code>caller_tab.csv</code>.</p>
</div>
</div>
<div id="running-text-with-stand-off-meta-data-files" class="section level2">
<h2>Running text with stand-off meta-data files</h2>
<p>The <code>sdac</code> dataset now contains various pieces of linguistic and non-linguistic meta-data that we extracted from the conversation files in the <code>sdac/</code> corpus. As part of that extraction we isolated the ids of the speakers involved in each conversation. As noted during the preliminary investigation portion of the curation of the data, these ids appear in a stand-off meta-data file named <code>caller_tab.csv</code> in the <a href="https://catalog.ldc.upenn.edu/docs/LDC97S62/">online documentation for the Switchboard Dialog Act Corpus</a>. These ids provide us a link between the corpus data and the speaker meta-data that we can exploit to incorporate that meta-data into our existing <code>sdac</code> tidy dataset.</p>
<p>It is common that stand-off meta-data files are in structured format. That is, they will typically be stored in a <code>.csv</code> file or an <code>.xml</code> document. The goal then is to read the data into R as a data frame and then join that data with the existing tidy corpus dataset. To read a <code>.csv</code> file, like the <code>caller_tab.csv</code> we use the <code>read_csv()</code> function. Before we read it we should manually download and inspect the data for a couple things: (1) how is the file delimited? and (2) is there a header row that names the columns in the data?</p>
<p>We can generally assume that a <code>.csv</code> file will be comma-separated, but this is not always the case sometimes the file will be delimited by semi-colons (<code>;</code>), tabs (<code>\\t</code>), or single or multiple spaces (<code>\\s+</code>). Whether there will be a header row or not can vary. If a header row does not exist in the file itself there is a good chance there is some file that documents what each column in the data represents.</p>
<p>Let’s take a look at a few rows from the <code>caller_tab.csv</code> to see what we have.</p>
<pre class="plain"><code>1000, 32, &quot;N&quot;, &quot;FEMALE&quot;, 1954, &quot;SOUTH MIDLAND&quot;, 1, 0, &quot;CASH&quot;, 15, &quot;N&quot;, &quot;&quot;, 2, &quot;DN2&quot;
1001, 102, &quot;N&quot;, &quot;MALE&quot;, 1940, &quot;WESTERN&quot;, 3, 0, &quot;GIFT&quot;, 10, &quot;N&quot;, &quot;&quot;, 0, &quot;XP&quot;
1002, 104, &quot;N&quot;, &quot;FEMALE&quot;, 1963, &quot;SOUTHERN&quot;, 2, 0, &quot;GIFT&quot;, 11, &quot;N&quot;, &quot;&quot;, 0, &quot;XP&quot;</code></pre>
<p>First we see that columns are indeed separated by commas. Second we see there is no header row. Some of the columns seem interpretable, like column 4, but we should try to find documentation to guide us. Poking around in the online documentation I noticed the <code>caller_doc.txt</code> file has names for the columns. This is a file used to generate a database table, but it contains the information we need so we’ll use it to assign names to our columns in <code>caller_tab.csv</code>.</p>
<pre class="plain"><code>CREATE TABLE caller (
    caller_no numeric(4) NOT NULL,
    pin numeric(4) NOT NULL,
    target character(1),
    sex character(6),
    birth_year numeric(4),
    dialect_area character(13),
    education numeric(1),
    ti numeric(1),
    payment_type character(5),
    amt_pd numeric(6),
    con character(1),
    remarks character(120),
    calls_deleted numeric(3),
    speaker_partition character(3)
);</code></pre>
<p>We can combine this information with the <code>read_csv()</code> function to read the <code>caller_tab.csv</code> and add the column names. Note that I’ve changed the <code>caller_no</code> name to <code>speaker_id</code> to align the nomenclature with the current <code>sdac</code> dataset. This renaming will facilitate the upcoming step to join the tidy dataset and this meta-data.</p>
<pre class="r"><code>sdac_speaker_meta &lt;- 
  read_csv(file = &quot;https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv&quot;, 
           col_names = c(&quot;speaker_id&quot;, # changed from `caller_no`
                         &quot;pin&quot;,
                         &quot;target&quot;,
                         &quot;sex&quot;,
                         &quot;birth_year&quot;,
                         &quot;dialect_area&quot;,
                         &quot;education&quot;,
                         &quot;ti&quot;,
                         &quot;payment_type&quot;,
                         &quot;amt_pd&quot;,
                         &quot;con&quot;,
                         &quot;remarks&quot;,
                         &quot;calls_deleted&quot;,
                         &quot;speaker_partition&quot;))

glimpse(sdac_speaker_meta) # preview the dataset</code></pre>
<pre><code>## Observations: 543
## Variables: 14
## $ speaker_id        &lt;int&gt; 1000, 1001, 1002, 1003, 1004, 1005, 1007, 10...
## $ pin               &lt;int&gt; 32, 102, 104, 5656, 123, 166, 274, 322, 445,...
## $ target            &lt;chr&gt; &quot;\&quot;N\&quot;&quot;, &quot;\&quot;N\&quot;&quot;, &quot;\&quot;N\&quot;&quot;, &quot;\&quot;N\&quot;&quot;, &quot;\&quot;N\&quot;&quot;,...
## $ sex               &lt;chr&gt; &quot;\&quot;FEMALE\&quot;&quot;, &quot;\&quot;MALE\&quot;&quot;, &quot;\&quot;FEMALE\&quot;&quot;, &quot;\&quot;M...
## $ birth_year        &lt;int&gt; 1954, 1940, 1963, 1947, 1958, 1956, 1965, 19...
## $ dialect_area      &lt;chr&gt; &quot;\&quot;SOUTH MIDLAND\&quot;&quot;, &quot;\&quot;WESTERN\&quot;&quot;, &quot;\&quot;SOUTH...
## $ education         &lt;int&gt; 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3,...
## $ ti                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ payment_type      &lt;chr&gt; &quot;\&quot;CASH\&quot;&quot;, &quot;\&quot;GIFT\&quot;&quot;, &quot;\&quot;GIFT\&quot;&quot;, &quot;\&quot;NONE\...
## $ amt_pd            &lt;int&gt; 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, ...
## $ con               &lt;chr&gt; &quot;\&quot;N\&quot;&quot;, &quot;\&quot;N\&quot;&quot;, &quot;\&quot;N\&quot;&quot;, &quot;\&quot;Y\&quot;&quot;, &quot;\&quot;N\&quot;&quot;,...
## $ remarks           &lt;chr&gt; &quot;\&quot;\&quot;&quot;, &quot;\&quot;\&quot;&quot;, &quot;\&quot;\&quot;&quot;, &quot;\&quot;\&quot;&quot;, &quot;\&quot;\&quot;&quot;, &quot;\&quot;\...
## $ calls_deleted     &lt;int&gt; 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,...
## $ speaker_partition &lt;chr&gt; &quot;\&quot;DN2\&quot;&quot;, &quot;\&quot;XP\&quot;&quot;, &quot;\&quot;XP\&quot;&quot;, &quot;\&quot;DN2\&quot;&quot;, &quot;\...</code></pre>
<p>The columns mapped to the data as expected. The character columns contain double quotes (<code>&quot;\&quot;</code>), however. We could proceed without issue (R will treat them as character values just the same) but I would like to clean up the character values for aesthetic purposes. To do this I applied the following code.</p>
<pre class="r"><code>sdac_speaker_meta &lt;- # remove double quotes
  sdac_speaker_meta %&gt;% 
  map(str_replace_all, pattern = &#39;&quot;&#39;, replacement = &#39;&#39;) %&gt;% # iteratively remove doubled quotes
  bind_rows() %&gt;%  # combine the results by rows
  type_convert() # return columns to orignal data types

glimpse(sdac_speaker_meta) # preview the dataset</code></pre>
<pre><code>## Observations: 543
## Variables: 14
## $ speaker_id        &lt;int&gt; 1000, 1001, 1002, 1003, 1004, 1005, 1007, 10...
## $ pin               &lt;int&gt; 32, 102, 104, 5656, 123, 166, 274, 322, 445,...
## $ target            &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;,...
## $ sex               &lt;chr&gt; &quot;FEMALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;...
## $ birth_year        &lt;int&gt; 1954, 1940, 1963, 1947, 1958, 1956, 1965, 19...
## $ dialect_area      &lt;chr&gt; &quot;SOUTH MIDLAND&quot;, &quot;WESTERN&quot;, &quot;SOUTHERN&quot;, &quot;NOR...
## $ education         &lt;int&gt; 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3,...
## $ ti                &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ payment_type      &lt;chr&gt; &quot;CASH&quot;, &quot;GIFT&quot;, &quot;GIFT&quot;, &quot;NONE&quot;, &quot;GIFT&quot;, &quot;GIF...
## $ amt_pd            &lt;int&gt; 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, ...
## $ con               &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;,...
## $ remarks           &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ calls_deleted     &lt;int&gt; 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,...
## $ speaker_partition &lt;chr&gt; &quot;DN2&quot;, &quot;XP&quot;, &quot;XP&quot;, &quot;DN2&quot;, &quot;XP&quot;, &quot;ET&quot;, &quot;DN1&quot;,...</code></pre>
<p>From the preview of the <code>sdac_speaker_meta</code> dataset we can see that there are 14 columns, including the <code>speaker_id</code>. We also see that there are 543 observations. We can assume that each row corresponds to an individual speaker, but to make sure let’s find the length of the unique values of <code>sdac_speaker_meta$speaker_id</code>.</p>
<pre class="r"><code>sdac_speaker_meta$speaker_id %&gt;% unique() %&gt;% length() # check for unique speakers</code></pre>
<pre><code>## [1] 543</code></pre>
<p>So this confirms each row in <code>sdac_speaker_meta</code> corresponds to an individual speaker. It is also clear now that the <code>sdac</code> dataset, which contains 441 individual speakers, is a subset of all the data collected in the Switchboard Corpus project.</p>
<p>Let’s select the columns that seem most interesting for a future analysis dropping the other columns. The <code>select()</code> function allows us to specify columns to keep (or drop).</p>
<pre class="r"><code>sdac_speaker_meta &lt;- # select columns of interest
  sdac_speaker_meta %&gt;% 
  select(speaker_id, sex, birth_year, dialect_area, education)

glimpse(sdac_speaker_meta) # preview the dataset</code></pre>
<pre><code>## Observations: 543
## Variables: 5
## $ speaker_id   &lt;int&gt; 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1...
## $ sex          &lt;chr&gt; &quot;FEMALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;MALE&quot;, &quot;FEMALE&quot;, &quot;FE...
## $ birth_year   &lt;int&gt; 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1...
## $ dialect_area &lt;chr&gt; &quot;SOUTH MIDLAND&quot;, &quot;WESTERN&quot;, &quot;SOUTHERN&quot;, &quot;NORTH MI...
## $ education    &lt;int&gt; 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2...</code></pre>
<div id="tidy-the-corpus-2" class="section level3">
<h3>Tidy the corpus</h3>
<p>The next step is to join the two datasets linking the values of <code>sdac$speaker_id</code> with the values of <code>sdac_speaker_meta$speaker_id</code>. We want to keep all the data in the <code>sdac</code> dataset and only include data from <code>sdac_speaker_meta</code> where there are matching speaker ids. To do this we use the <code>left_join()</code> function. <code>left_join()</code> requires two arguments which correspond to two data frames. We can optionally specify which column(s) to use as the columns to use as the joining condition, but by default it will use any column names that match in the two data frames. In our case the only column that matches is the <code>speaker_id</code> column so we can proceed without explicitly specifying the join column.</p>
<pre class="r"><code>sdac &lt;- left_join(sdac, sdac_speaker_meta) # join by `speaker_id`</code></pre>
<pre><code>## Error in left_join_impl(x, y, by$x, by$y, suffix$x, suffix$y, check_na_matches(na_matches)): Can&#39;t join on &#39;speaker_id&#39; x &#39;speaker_id&#39; because of incompatible types (integer / character)</code></pre>
<p>We get an error! Reading the error it appears we are trying to join columns of differing data types; the <code>sdac$speaker_id</code> is of type character and <code>sdac_speaker_meta$speaker_id</code> is of type integer.</p>
<div class="alert alert-note">
  <p>Error messages can be difficult to make sense of. If the issue is not clear to you, copy the error and search the web to see if others have had the same issue. Chances are someone has! If not, <a href="https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example">follow these steps to create a reproducible example</a> and post it to a site such as <a href="https://stackoverflow.com">StackOverflow</a>.</p>

</div>

<p>To remedy the situation we need to coerce the <code>sdac$speaker_id</code> column into a integer. The <code>as.numeric()</code> function will do this.</p>
<pre class="r"><code>sdac$speaker_id &lt;- sdac$speaker_id %&gt;% as.numeric() # convert to integer</code></pre>
<p>Now let’s apply our join operation again.</p>
<pre class="r"><code>sdac &lt;- left_join(sdac, sdac_speaker_meta) # join by `speaker_id`

glimpse(sdac) # preview the joined dataset</code></pre>
<pre><code>## Observations: 223,606
## Variables: 11
## $ doc_id         &lt;chr&gt; &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;, &quot;4325&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;o&quot;, &quot;qw&quot;, &quot;qy^d&quot;, &quot;+&quot;, &quot;+&quot;, &quot;qy&quot;, &quot;sd&quot;, &quot;ad&quot;, ...
## $ speaker        &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B...
## $ turn_num       &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6...
## $ utterance_num  &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4...
## $ utterance_text &lt;chr&gt; &quot;Okay.  /&quot;, &quot;{D So, }&quot;, &quot;[ [ I guess, +&quot;, &quot;What...
## $ speaker_id     &lt;dbl&gt; 1632, 1632, 1519, 1632, 1519, 1632, 1519, 1519,...
## $ sex            &lt;chr&gt; &quot;FEMALE&quot;, &quot;FEMALE&quot;, &quot;FEMALE&quot;, &quot;FEMALE&quot;, &quot;FEMALE...
## $ birth_year     &lt;int&gt; 1962, 1962, 1971, 1962, 1971, 1962, 1971, 1971,...
## $ dialect_area   &lt;chr&gt; &quot;WESTERN&quot;, &quot;WESTERN&quot;, &quot;SOUTH MIDLAND&quot;, &quot;WESTERN...
## $ education      &lt;int&gt; 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2,...</code></pre>
<p>Result! Now let’s check our data for any missing data points generated in the join.</p>
<pre class="r"><code>sdac[!complete.cases(sdac), ] %&gt;% glimpse # view incomplete cases</code></pre>
<pre><code>## Observations: 100
## Variables: 11
## $ doc_id         &lt;chr&gt; &quot;3554&quot;, &quot;3554&quot;, &quot;3554&quot;, &quot;3554&quot;, &quot;3554&quot;, &quot;3554&quot;,...
## $ damsl_tag      &lt;chr&gt; &quot;sd@&quot;, &quot;+@&quot;, &quot;sv@&quot;, &quot;+@&quot;, &quot;+&quot;, &quot;sd&quot;, &quot;+&quot;, &quot;+&quot;, ...
## $ speaker        &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A...
## $ turn_num       &lt;chr&gt; &quot;1&quot;, &quot;3&quot;, &quot;5&quot;, &quot;7&quot;, &quot;9&quot;, &quot;9&quot;, &quot;11&quot;, &quot;13&quot;, &quot;13&quot;,...
## $ utterance_num  &lt;chr&gt; &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;1&quot;, &quot;1&quot;, &quot;2&quot;, &quot;1...
## $ utterance_text &lt;chr&gt; &quot;Of a exercise program you have.&quot;, &quot;Right. /&quot;, ...
## $ speaker_id     &lt;dbl&gt; 155, 155, 155, 155, 155, 155, 155, 155, 155, 15...
## $ sex            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...
## $ birth_year     &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...
## $ dialect_area   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...
## $ education      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...</code></pre>
<p>We have 100 observations that are missing data. Inspecting the dataset preview it appears that there was at least one <code>speaker_id</code> that appears in the conversation files that does not appear in the speaker meta-data. Let’s check to see how many speakers this might affect.</p>
<pre class="r"><code>sdac[!complete.cases(sdac), ] %&gt;% select(speaker_id) %&gt;% unique()</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["speaker_id"],"name":[1],"type":["dbl"],"align":["right"]}],"data":[{"1":"155","_rn_":"167190"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Just one speaker. This could very well be annotator error. Since it effects a relatively small proportion of the data, let’s drop this speaker from the dataset. We can use the <code>filter()</code> function to select the values of <code>speaker_id</code> that are not equal to <code>155</code>.</p>
<pre class="r"><code>sdac &lt;- # remove rows where speaker_id == 155
  sdac %&gt;% 
  filter(speaker_id != 155)

sdac[!complete.cases(sdac), ] %&gt;% glimpse # view incomplete cases</code></pre>
<pre><code>## Observations: 0
## Variables: 11
## $ doc_id         &lt;chr&gt; 
## $ damsl_tag      &lt;chr&gt; 
## $ speaker        &lt;chr&gt; 
## $ turn_num       &lt;chr&gt; 
## $ utterance_num  &lt;chr&gt; 
## $ utterance_text &lt;chr&gt; 
## $ speaker_id     &lt;dbl&gt; 
## $ sex            &lt;chr&gt; 
## $ birth_year     &lt;int&gt; 
## $ dialect_area   &lt;chr&gt; 
## $ education      &lt;int&gt;</code></pre>
</div>
<div id="explore-the-tidy-dataset-2" class="section level3">
<h3>Explore the tidy dataset</h3>
<p>At this point we have a well-curated dataset which includes linguistic and non-linguistic meta-data. As we did for the ACTIV-ES corpus, let’s get a sense of the distribution of some of the meta-data.</p>
<p>First we will visualize the number of utterances from speakers of the different dialect regions.</p>
<pre class="r"><code>sdac %&gt;% 
  group_by(dialect_area) %&gt;% 
  count() %&gt;%
  ggplot(aes(x = dialect_area, y = n)) + 
  geom_col() +
  labs(x = &quot;Dialect region&quot;, y = &quot;Utterance count&quot;, title = &quot;Switchboard Dialog Act Corpus&quot;, subtitle = &quot;Utterances per dialect region&quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))</code></pre>
<p><img src="/post/2017-12-01-curate-language-data-organizing-metadata_files/figure-html/sdac-dialect-plot-1.png" width="100%" /></p>
<p>Let’s see how men and women figure across the dialect areas.</p>
<pre class="r"><code>sdac %&gt;% 
  group_by(dialect_area, sex) %&gt;% 
  count() %&gt;% 
  ggplot(aes(x = dialect_area, y = n, fill = sex)) + 
  geom_col() +
  labs(x = &quot;Dialect region&quot;, y = &quot;Utterance count&quot;, title = &quot;Switchboard Dialog Act Corpus&quot;, subtitle = &quot;Utterances per dialect region and sex&quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))</code></pre>
<p><img src="/post/2017-12-01-curate-language-data-organizing-metadata_files/figure-html/sdac-dialect-sex-plot-1.png" width="100%" /></p>
<p>There are many other ways to group and count the dataset but I’ll leave that to you to look at!</p>
</div>
</div>
<div id="round-up" class="section level2">
<h2>Round up</h2>
<p>In this post I covered tidying a corpus from running text files. We looked at three cases where meta-data is typically stored: in filenames, embedded inline with the text itself, and in stand-off files. As usual we made extensive use of the tidyverse package set (readr, dplyr, ggplot2, etc.) and included discussion of other packages: readtext for reading and organizing meta-data from file names, tidytext for tokenizing text, and stringr for text cleaning and pattern matching. I also briefly introduced the ggplot2 package for creating plots based on the Grammar of Graphics philosophy. Along the way we continued to extend our knowledge of R data and object types working with vectors, data frames, and lists manipulating them in various ways (subsetting, sorting, transforming, and summarizing).</p>
<p>In the next post I will turn to working with meta-data in structured documents, specifically <code>.xml</code> documents. These type of documents tend to have rich meta-data including linguistic and non-linguistic information. We will focus on working with linguistic annnotations such as part-of-speech and syntactic structure. We will work to parse the linguistic information in these documents into a tidy dataset and also see how to create linguistic annotations for data does not already contain them.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-R-readtext">
<p>Benoit, Kenneth, and Adam Obeng. 2017. <em>Readtext: Import and Handling for Plain and Formatted Text Files</em>. <a href="https://CRAN.R-project.org/package=readtext" class="uri">https://CRAN.R-project.org/package=readtext</a>.</p>
</div>
<div id="ref-R-purrr">
<p>Henry, Lionel, and Hadley Wickham. 2017. <em>Purrr: Functional Programming Tools</em>. <a href="https://CRAN.R-project.org/package=purrr" class="uri">https://CRAN.R-project.org/package=purrr</a>.</p>
</div>
<div id="ref-R-tidytext">
<p>Robinson, David, and Julia Silge. 2018. <em>Tidytext: Text Mining Using ’Dplyr’, ’Ggplot2’, and Other Tidy Tools</em>. <a href="https://CRAN.R-project.org/package=tidytext" class="uri">https://CRAN.R-project.org/package=tidytext</a>.</p>
</div>
<div id="ref-R-stringr">
<p>Wickham, Hadley. 2018. <em>Stringr: Simple, Consistent Wrappers for Common String Operations</em>. <a href="https://CRAN.R-project.org/package=stringr" class="uri">https://CRAN.R-project.org/package=stringr</a>.</p>
</div>
<div id="ref-R-ggplot2">
<p>Wickham, Hadley, and Winston Chang. 2018. <em>Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics</em>.</p>
</div>
<div id="ref-R-dplyr">
<p>Wickham, Hadley, Romain Francois, Lionel Henry, and Kirill Müller. 2017. <em>Dplyr: A Grammar of Data Manipulation</em>. <a href="https://CRAN.R-project.org/package=dplyr" class="uri">https://CRAN.R-project.org/package=dplyr</a>.</p>
</div>
<div id="ref-R-readr">
<p>Wickham, Hadley, Jim Hester, and Romain Francois. 2017. <em>Readr: Read Rectangular Text Data</em>. <a href="https://CRAN.R-project.org/package=readr" class="uri">https://CRAN.R-project.org/package=readr</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://regex101.com" class="uri">https://regex101.com</a> is a great place to learn more about Regular Expressions and to practice using them.<a href="#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/dataset/">dataset</a>
  
  <a class="badge badge-light" href="/tags/tidy/">tidy</a>
  
  <a class="badge badge-light" href="/tags/meta-data/">meta-data</a>
  
  <a class="badge badge-light" href="/tags/readtext/">readtext</a>
  
  <a class="badge badge-light" href="/tags/tidytext/">tidytext</a>
  
  <a class="badge badge-light" href="/tags/ggplot2/">ggplot2</a>
  
  <a class="badge badge-light" href="/tags/regular-expressions/">regular expressions</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/2015/08/12/creating-hexbin-plots/">Creating hexbin plots</a></li>
        
      </ul>
    </div>
    

    

    
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "francojc" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>



  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    Jerid Francom &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/css.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/markdown.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/shell.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/sql.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/tex.min.js"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    
      <script async defer src="//maps.googleapis.com/maps/api/js?key=AIzaSyCp-PTX7EwVGvo1lOhmK8PHZgakQLoz_RA"></script>
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/gmaps.js/0.4.25/gmaps.min.js" integrity="sha256-7vjlAeb8OaTrCXZkCNun9djzuB2owUsaO72kXaFDBJs=" crossorigin="anonymous"></script>
      
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "Search Results",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

  </body>
</html>

