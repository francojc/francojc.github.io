<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Frames on francojc ⟲</title>
    <link>https://francojc.github.io/tags/data-frames/</link>
    <description>Recent content in Data Frames on francojc ⟲</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jerid Francom</copyright>
    <lastBuildDate>Thu, 02 Nov 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/data-frames/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Acquiring data for language research (3/3): web scraping </title>
      <link>https://francojc.github.io/2017/11/02/acquiring-data-for-language-research-web-scraping/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/11/02/acquiring-data-for-language-research-web-scraping/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;!-- TODO:
--&gt;
&lt;div id=&#34;web-scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Web scraping&lt;/h2&gt;
&lt;p&gt;There are many resources available through direct downloads from repositories and individual sites and R package interfaces to web resources with APIs, but these resources are relatively limited to the amount of public-facing textual data recorded on the web. In the case that you want to acquire data from webpages R can be used to access the web programmatically through a process known as web scraping. The complexity of web scrapes can vary but in general it requires more advanced knowledge of R as well as the structure of the language of the web: HTML (Hypertext Markup Language).&lt;/p&gt;
&lt;div id=&#34;a-toy-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A toy example&lt;/h3&gt;
&lt;p&gt;HTML is a cousin of XML and as such organizes web documents in a hierarchical format that is read by your browser as you navigate the web. Take for example the toy webpage I created for this demonstration in Figure &lt;a href=&#34;#fig:example-webpage&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:example-webpage&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/example-webpage-1.png&#34; alt=&#34;Example web page.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Example web page.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The file accessed by my browser to render this webpage is &lt;code&gt;test.html&lt;/code&gt; and in plain-text format looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;My website&amp;lt;/title&amp;gt;
  &amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;
      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;
      &amp;lt;p&amp;gt;This is my first website. &amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;table&amp;gt;
      &amp;lt;tr&amp;gt;
        &amp;lt;td&amp;gt;Contact me:&amp;lt;/td&amp;gt;
        &amp;lt;td&amp;gt;
          &amp;lt;a href=&amp;quot;mailto:francojc@wfu.edu&amp;quot;&amp;gt;francojc@wfu.edu&amp;lt;/a&amp;gt;
        &amp;lt;/td&amp;gt;
      &amp;lt;/tr&amp;gt;
    &amp;lt;/table&amp;gt;
    &amp;lt;div class=&amp;quot;conc&amp;quot;&amp;gt;
      &amp;lt;p&amp;gt;Good-bye!&amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each element in this file is delineated by an opening and closing tag, &lt;code&gt;&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&lt;/code&gt;. Tags are nested within other tags to create the structural hierarchy. Tags can take class and id labels to distinguish them from other tags and often contain other attributes that dictate how the tag is to behave when rendered visually by a browser. For example, there are two &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; tags in our toy example: one has the label &lt;code&gt;class = &amp;quot;intro&amp;quot;&lt;/code&gt; and the other &lt;code&gt;class = &amp;quot;conc&amp;quot;&lt;/code&gt;. &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; tags are often used to separate sections of a webpage that may require special visual formatting. The &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tag, on the other hand, creates a web link. As part of this tag’s function, it requires the attribute &lt;code&gt;href=&lt;/code&gt; and a web protocol –in this case it is a link to an email address &lt;code&gt;mailto:francojc@wfu.edu&lt;/code&gt;. More often than not, however, the &lt;code&gt;href=&lt;/code&gt; contains a URL (Uniform Resource Locator). A working example might look like this: &lt;code&gt;&amp;lt;a href=&amp;quot;https://francojc.github.io/&amp;quot;&amp;gt;My homepage&amp;lt;/a&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The aim of a web scrape is to download the HTML file, parse the document structure, and extract the elements containing the relevant information we wish to capture. Let’s attempt to extract some information from our toy example. To do this we will need the &lt;a href=&#34;https://CRAN.R-project.org/package=rvest&#34;&gt;rvest&lt;/a&gt; package. First, install/load the package, then, read and parse the HTML from the character vector named &lt;code&gt;web_file&lt;/code&gt; assigning the result to &lt;code&gt;html&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(rvest) # install/ load `rvest`
html &amp;lt;- read_html(web_file) # read the raw html
html&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_document}
## &amp;lt;html&amp;gt;
## [1] &amp;lt;head&amp;gt;\n&amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset= ...
## [2] &amp;lt;body&amp;gt;\n    &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;\n      &amp;lt;p&amp;gt;Thi ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;read_html()&lt;/code&gt; parses the raw HTML into an object of class &lt;code&gt;xml_document&lt;/code&gt;. The summary output above shows that tags the HTML structure have been parsed into ‘nodes’. The tag nodes can be accessed by using the &lt;code&gt;html_nodes()&lt;/code&gt; function by specifying the tag to isolate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (2)}
## [1] &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;\n      &amp;lt;p&amp;gt;This is my firs ...
## [2] &amp;lt;div class=&amp;quot;conc&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Good-bye!&amp;lt;/p&amp;gt;\n    &amp;lt;/div&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The &lt;code&gt;%&amp;gt;%&lt;/code&gt; operator is used to ‘pipe’ the output of one R operation to the input of the next operation. Piping is equivalent to embedding functions but tends to lead to more legible code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(1:5) # embedding example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:5 %&amp;gt;% sum() # piping example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default the subsequent function assumes that the output will be used as the first argument. If this is not the case, the &lt;code&gt;.&lt;/code&gt; operator can be used to match the output to the correct argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:5 %&amp;gt;% paste(&amp;quot;Number&amp;quot;, .) # directing output with &lt;code&gt;.&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number 1&amp;quot; &amp;quot;Number 2&amp;quot; &amp;quot;Number 3&amp;quot; &amp;quot;Number 4&amp;quot; &amp;quot;Number 5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Notice that &lt;code&gt;html_nodes(&amp;quot;div&amp;quot;)&lt;/code&gt; has returned both &lt;code&gt;div&lt;/code&gt; tags. To isolate one of tags by its class, we add the class name to the tag separating it with a &lt;code&gt;.&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;\n      &amp;lt;p&amp;gt;This is my firs ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great. Now say we want to drill down and isolate the subordinate &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; nodes. We can add &lt;code&gt;p&lt;/code&gt; to our node filter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro p&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (2)}
## [1] &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;
## [2] &amp;lt;p&amp;gt;This is my first website. &amp;lt;/p&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract the text contained within a node we use the &lt;code&gt;html_text()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro p&amp;quot;) %&amp;gt;% 
  html_text()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Welcome!&amp;quot;                   &amp;quot;This is my first website. &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a character vector with two elements corresponding to the text contained in each &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tag. If you were paying close attention you might have noticed that the second element in our vector includes extra whitespace after the period. To trim leading and trailing whitespace from text we can add the &lt;code&gt;trim = TRUE&lt;/code&gt; argument to &lt;code&gt;html_text()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro p&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Welcome!&amp;quot;                  &amp;quot;This is my first website.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here we would then work to organize the text into a format we want to store it in and write the results to disk. Let’s leave writing data to disk for later in the post. For now keep our focus on working with &lt;code&gt;rvest&lt;/code&gt; to acquire data from html documents working with a more practical example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-practical-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A practical example&lt;/h3&gt;
&lt;p&gt;With some basic understanding of HTML and how to use the &lt;code&gt;rvest&lt;/code&gt; package, let’s turn to a realistic example. Say we want to acquire text from the Spanish news site &lt;a href=&#34;https://elpais.com/&#34;&gt;elpais.com&lt;/a&gt;. The first step in any web scrape is to investigate the site and page(s) we want to scrape. Minimally this includes identifying the URL we want to target and exploring the structure of the HTML document. Take the following webpage I have identified, seen in Figure &lt;a href=&#34;#fig:example-page-elpais&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:example-page-elpais&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/example-page-elpais-1.png&#34; alt=&#34;Content page from the Spanish new site El País.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Content page from the Spanish new site El País.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As in our toy example, first we want to feed the HTML document to the &lt;code&gt;read_html()&lt;/code&gt; function to parse the tags into nodes. In this case we will assign the web address to the variable &lt;code&gt;url&lt;/code&gt;. &lt;code&gt;read_html()&lt;/code&gt; will automatically connect to the web and download the raw html.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;https://elpais.com/elpais/2017/10/17/opinion/1508258340_992960.html&amp;quot;
html &amp;lt;- read_html(url)
html&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_document}
## &amp;lt;html lang=&amp;quot;es&amp;quot;&amp;gt;
## [1] &amp;lt;head&amp;gt;\n&amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset= ...
## [2] &amp;lt;body id=&amp;quot;salida_articulo&amp;quot; class=&amp;quot;salida_articulo&amp;quot;&amp;gt;\n&amp;lt;div id=&amp;quot;pxlhdd ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have captured and parsed the raw HTML assigning it to the object named &lt;code&gt;html&lt;/code&gt;. The next step is to identify the node or nodes that contain the information we want to extract from the page. To do this it is helpful to use a browser to inspect specific elements of the webpage. Your browser will be equipped with a command that you can enable by hovering your mouse over the element of the page you want to target and using a right click to select “Inspect Element”. This will split your browser window horizontally showing you the raw HTML underlying the webpage.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:inspect-element&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/inspect-element-1.png&#34; alt=&#34;Using the &amp;quot;Inspect Element&amp;quot; command to explore raw html.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Using the “Inspect Element” command to explore raw html.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From Figure &lt;a href=&#34;#fig:inspect-element&#34;&gt;3&lt;/a&gt; we see that the node we want to target is &lt;code&gt;h1&lt;/code&gt;. Now this tag is common and we don’t want to extract every &lt;code&gt;h1&lt;/code&gt; so we use the class &lt;code&gt;articulo-titulo&lt;/code&gt; to specify we only want the title of the article. Using the convention described in our toy example, we can isolate the title of the page.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;h1.articulo-titulo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;h1 class=&amp;quot;articulo-titulo &amp;quot; id=&amp;quot;articulo-titulo&amp;quot; itemprop=&amp;quot;headline ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then extract the text with &lt;code&gt;html_text()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;title &amp;lt;- 
  html %&amp;gt;% 
  html_nodes(&amp;quot;h1.articulo-titulo&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)
title&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Crímenes contra el periodismo en el seno de la UE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s extract the author’s name and the article text in the same way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Author
author &amp;lt;- 
  html %&amp;gt;% 
  html_node(&amp;quot;span.autor-nombre&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)
# Article text
text &amp;lt;- 
  html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-cuerpo p&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another piece of information we might want to include in our web scrape is the date the article was published. Again, we use the “Inspect Element” tool in your browser to locate the tag we intend to isolate. This time, however, the information that returned by &lt;code&gt;html_text()&lt;/code&gt; is less than ideal –the date is inter-spliced with text formatting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-datos time&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;18 OCT 2017 - 14:26\t\t\t\t\tCEST&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the &lt;code&gt;time&lt;/code&gt; node provides another angle: a clean date is contained as the &lt;code&gt;datetime&lt;/code&gt; attribute of the &lt;code&gt;time&lt;/code&gt; tag.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-datos time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;time datetime=&amp;quot;2017-10-18T14:26:30+02:00&amp;quot; class=&amp;quot;articulo-actualiza ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract a tag’s attribute we use the &lt;code&gt;html_attr()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Date
date &amp;lt;- 
  html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-datos time&amp;quot;) %&amp;gt;% 
  html_attr(&amp;quot;datetime&amp;quot;)
date&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2017-10-18T14:26:30+02:00&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have isolated and extracted the title, author, date, and text from the webpage. Each of these elements are stored in character vectors in our R session. To complete our task we need to write this data to disk as plain text. With an eye towards a tidy dataset, an ideal format to store the data is in a CSV file where each column corresponds to one of the elements from our scrape and each row an observation. The observations will contain the text from each &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tag. A CSV file is a tabular format and so before we can write the data to disk let’s coerce the data that we have into tabular format. We will use the &lt;code&gt;tibble()&lt;/code&gt; function here to streamline our data frame creation.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Feeding each of the vectors &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;date&lt;/code&gt;, and &lt;code&gt;text&lt;/code&gt; as arguments to &lt;code&gt;tibble()&lt;/code&gt; creates the tabular format we are looking for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(title, author, date, text)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;title&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;author&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;date&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;text&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;México, Irak y Siria encabezan de manera destacada la lista de los países más peligrosos para los periodistas; allí donde los profesionales de la información están especialmente expuestos a la violencia, figuran en la diana de los conflictos bélicos o su trabajo se ve cercenado por Gobiernos totalitarios. El barómetro de las violaciones de la libertad de prensa de Reporteros sin Fronteras contabiliza en lo que va de año 11 crímenes en México, 8 en Siria y 7 en Irak. Yemen, Afganistán, Honduras, Brasil o Somalia aparecen también entre los Estados en los que los informadores son vilmente asesinados. Cuando parecía que la Unión Europea estaba libre de este tipo de ataques atroces a la libertad de prensa, dos países —Dinamarca y Malta— han pasado a engrosar la lista de la vergüenza.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Una bomba lapa situada en su coche acabó esta semana brutalmente con la vida de la periodista maltesa Daphne Caruana Galizia, de 53 años. Estaba involucrada en una investigación sobre los papeles de Malta, una derivación de los llamados papeles de Panamá,que revelaron en mayo cómo la pequeña isla mediterránea se había convertido en un paraíso fiscal dentro de la propia UE. Sus indagaciones salpicaron a la esposa del primer ministro y a varios miembros del Ejecutivo. Abocaron a un adelanto electoral y, pese a las revelaciones, el laborista Joseph Muscat volvió a ganar en junio.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Caruana Galizia, la víctima mortal número 41 computada por RSF en lo que va de año, estaba en el punto de mira. Pocos días antes de ser asesinada presentó una denuncia en la que aseguraba haber recibido amenazas de muerte. Ahora su hijo culpa al Gobierno de Muscat de permitir el crimen, la corrupción y una cultura de impunidad. “Mi madre ha sido asesinada porque se interponía entre el Estado de derecho y quienes quieren violarlo, como muchos otros fuertes periodistas”, ha denunciado Matthew Caruana Galizia.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;En el otro extremo de la UE, en la costa sur de Copenhague, la policía encontró a finales de agosto parte del cuerpo de la periodista sueca Kim Wall, de 30 años, que según todos los indicios fue asesinada cuando se encontraba a bordo de un submarino para realizar un reportaje. Su cadáver, mutilado salvajemente, fue hallado en el mar Báltico. Peter Madsen, excéntrico inventor y propietario del sumergible Nautilus, ha sido acusado de homicidio.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Crímenes destinados a acallar la voz de la prensa son moneda común en los países donde el narcotráfico, los paramilitares o los Estados corruptos se han hecho fuertes. Pero que estos ataques se produzcan en el seno de la Unión Europea son una noticia inquietante. La Comisión Europea, con su presidente, Jean-Claude Juncker en primera fila, se ha apresurado a condenar el asesinato de la reportera maltesa con una contundente declaración de intenciones: “El derecho de un periodista a investigar, hacer preguntas incómodas e informar de manera efectiva está en el corazón de nuestros valores y debe garantizarse siempre”.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Puedes seguir EL PAÍS Opinión en Facebook, Twitter o suscribirte aquí a la Newsletter.&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Notice that there are six rows in this data frame, one corresponding to each paragraph in &lt;code&gt;text&lt;/code&gt;. R has a bias towards working with vectors of the same length. As such each of the other vectors (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;author&lt;/code&gt;, and &lt;code&gt;date&lt;/code&gt;) are replicated, or recycled, until they are the same length as the longest vector &lt;code&gt;text&lt;/code&gt;, which a length of six.&lt;/p&gt;
&lt;p&gt;For good documentation let’s add our object &lt;code&gt;url&lt;/code&gt; to the data frame, which contains the actual web link to this page, and assign the result to &lt;code&gt;webpage_data&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;webpage_data &amp;lt;- tibble(title, author, date, text, url)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final step is to write this data to disk. To do this we will use the &lt;code&gt;write_csv()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(x = webpage_data, path = &amp;quot;data/original/elpais_webpage.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-all-together&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting it all together&lt;/h3&gt;
&lt;p&gt;At this point you may be think, ‘Great, I can download data from a single page, but what about downloading multiple pages?’ Good question. That’s really where the strength of a programming approach takes hold. Extracting information from multiple pages is not fundamentally different than working with a single page. However, it does require more sophisticated code. I will not document the code in this post but you are encouraged to download the GitHub repository which contains the working code and peruse the &lt;code&gt;functions/aquire_functions.R&lt;/code&gt; script to see the details and replicate the processing covered here. Yet I will give you a gist of the steps taken to scrape multiple pages from the El País website.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier in this section, the first step in any web scrape is to investigate the structure of the site and page(s) we want to scrape. The El País site is organized such that each article is ‘tagged’ with some meta-category. After doing some browsing on their site, I discovered there is a searchable archive page that lists all the ‘tags’ used on the site. By selecting a tag, a paginated interface listing all of the articles associated with said tag is made available.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:read-archives-elpais&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/read-archives-elpais-1.png&#34; alt=&#34;El País archives page for the `politica` tag.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: El País archives page for the &lt;code&gt;politica&lt;/code&gt; tag.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In a nutshell, the approach then is to leverage these archives to harvest links to article pages with a specific tag, download the content of these links and then organize and write the data to disk in CSV format. In more detail I’ve provided concrete steps with the custom functions I wrote to accomplish each:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Get the total number of archive pages available.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Includes an optional argument &lt;code&gt;sample_size&lt;/code&gt; to specify the number of archive pages to harvest links from. The default is &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_archive_pages &amp;lt;- function(tag_name, sample_size = 1) {
  # Function: Scrape tag main page and return selected number of archive pages
  url &amp;lt;- paste0(&amp;quot;https://elpais.com/tag/&amp;quot;, tag_name)
  html &amp;lt;- read_html(url) # load html from selected url
  pages_available &amp;lt;- 
    html %&amp;gt;% # pass html
    html_node(&amp;quot;li.paginacion-siguiente a&amp;quot;) %&amp;gt;% # isolate &amp;#39;next page&amp;#39; link
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% # extract &amp;#39;next page&amp;#39; link
    str_extract(&amp;quot;\\d+$&amp;quot;) %&amp;gt;% # extract the numeric value (num pages of links) in link
    as.numeric() + 1 # covert to a numeric vector and add 1 (to include first page)
  cat(pages_available, &amp;quot;pages available for the&amp;quot;, tag_name, &amp;quot;tag.\n&amp;quot;)
  archive_pages &amp;lt;- paste0(url, &amp;quot;/a/&amp;quot;, (pages_available - (sample_size - 1)):pages_available) # compile urls
  cat(sample_size, &amp;quot;pages selected.\n&amp;quot;)
  return(archive_pages)
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Harvest the links to the content pages.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;str_replace()&lt;/code&gt; function from the &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;stringr&lt;/a&gt; library is used here to create valid URLs by replacing the &lt;code&gt;//&lt;/code&gt; with &lt;code&gt;https://&lt;/code&gt; in the links harvested directly from the webpage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_content_links &amp;lt;- function(url) {
  # Function: Scrape the content links from a tag archive page
  html &amp;lt;- read_html(url) # load html from selected url
  urls &amp;lt;- 
    html %&amp;gt;% # pass html
    html_nodes(&amp;quot;h2.articulo-titulo a&amp;quot;) %&amp;gt;% # isolate links
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% # extract urls
    str_replace(pattern = &amp;quot;//&amp;quot;, replacement = &amp;quot;https://&amp;quot;) # create valid urls
  cat(length(urls),&amp;quot;content links scraped from tag archives.\n&amp;quot;)
  return(urls)
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Get the content for a given link and organize it into tabular format.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A conditional statement is included to identify webpages with no text content. All pages have a boilerplate paragraph, so pages with a &lt;code&gt;text&lt;/code&gt; vector of length greater than one will be content pages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_content &amp;lt;- function(url) {
  # Function: Scrape the title, author, date, and text from a provided
  # content link. Return as a tibble/data.frame
  cat(&amp;quot;Scraping:&amp;quot;, url, &amp;quot;\n&amp;quot;)
  html &amp;lt;- read_html(url) # load html from selected url
  
  # Title
  title &amp;lt;- 
    html %&amp;gt;% # pass html
    html_node(&amp;quot;h1.articulo-titulo&amp;quot;) %&amp;gt;% # isolate title
    html_text(trim = TRUE) # extract title and trim whitespace
  
  # Author
  author &amp;lt;- 
    html %&amp;gt;% # pass html
    html_node(&amp;quot;span.autor-nombre&amp;quot;) %&amp;gt;% # isolate author
    html_text(trim = TRUE) # extract author and trim whitespace
  
  # Date
  date &amp;lt;- 
    html %&amp;gt;% # pass html
    html_nodes(&amp;quot;div.articulo-datos time&amp;quot;) %&amp;gt;% # isolate date
    html_attr(&amp;quot;datetime&amp;quot;) # extract date
  
  # Text
  text &amp;lt;- 
    html %&amp;gt;% # pass html
    html_nodes(&amp;quot;div.articulo-cuerpo p&amp;quot;) %&amp;gt;% # isolate text by paragraph
    html_text(trim = TRUE) # extract paragraphs and trim whitespace
  
  # Check to see if the article is text based
  # - only one paragraph suggests a non-text article (cartoon/ video/ album)
  if (length(text) &amp;gt; 1) { 
    # Create tibble/data.frame
    return(tibble(url, title, author, date, text, paragraph = (1:length(text))))
  } else {
    message(&amp;quot;Non-text based article. Link skipped.&amp;quot;)
    return(NULL)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Write the tabular data to disk.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’ve added code we’ve used in the previous data acquisition methods in this post to create a target directory before writing the file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_content &amp;lt;- function(content, target_file) {
  # Function: Write the tibble content to disk. Create the directory if
  # it does not already exist.
  target_dir &amp;lt;- dirname(target_file) # identify target file directory structure
  dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create directory
  write_csv(content, target_file) # write csv file to target location
  cat(&amp;quot;Content written to disk!\n&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These function each perform a task in our workflow and can be joined together to do our web scrape. To make this workflow maximally efficient I’ve wrapped them, and a conditional statement to avoid re-downloading a resource, in a function named &lt;code&gt;download_elpais_tag()&lt;/code&gt;. I’ve also added the &lt;code&gt;map()&lt;/code&gt; function to our workflow at a couple key points. &lt;code&gt;map()&lt;/code&gt; takes an object an iterates over each element in that object. Since the &lt;code&gt;get_content_links()&lt;/code&gt; and the &lt;code&gt;get_content()&lt;/code&gt; functions work on an object with a single element, we need the functions to be iteratively applied to objects with multiple elements. After &lt;code&gt;map()&lt;/code&gt; does its work applying the function to the elements of the object the results need to be joined. For the results from &lt;code&gt;map(get_content_links)&lt;/code&gt; will be a vector, so &lt;code&gt;combine()&lt;/code&gt; is the appropriate function. For &lt;code&gt;map(get_content)&lt;/code&gt; a tibble data frame will be returned so we use &lt;code&gt;bind_rows()&lt;/code&gt; to join the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;download_elpais_tag &amp;lt;- function(tag_name, sample_size, target_file, force = FALSE) {
  # Function: Download articles from elpais.com based on tag name. Select
  # number of archive pages to consult, then scrape and write the content 
  # to disk. If the target file exists, do not download again.
  if(!file.exists(target_file) | force == TRUE) {
    cat(&amp;quot;Downloading data.\n&amp;quot;)
    get_archive_pages(tag_name, sample_size) %&amp;gt;% # select tag archive pages
      map(get_content_links) %&amp;gt;% # get content links from pages sampled
      combine() %&amp;gt;% # combine the results as a single vector
      map(get_content) %&amp;gt;% # get the content for each content link
      bind_rows() %&amp;gt;% # bind the results as a single tibble
      write_content(target_file) # write content to disk
  } else {
    cat(&amp;quot;Data already downloaded!\n&amp;quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding these functions, including the &lt;code&gt;download_elpais_tag()&lt;/code&gt; function to the &lt;code&gt;functions/acquire_functions.R&lt;/code&gt; script in our project management template and then sourcing this script from the &lt;code&gt;acquire_data.R&lt;/code&gt; script in the &lt;code&gt;code/&lt;/code&gt; directory will allow us to use the function like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scrape archives of the Spanish news site elpais.com by tag
# To search for valid tags: https://elpais.com/tag/listado/
download_elpais_tag(tag_name = &amp;quot;politica&amp;quot;, 
                    target_file = &amp;quot;data/original/elpais/political_articles.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;90554 pages available for the politica tag.
1 pages selected.
22 content links scraped from tag archives.
Scraping: https://elpais.com/deportes/2017/10/20/actualidad/1508510590_014924.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508506425_813840.html 
Scraping: https://elpais.com/internacional/2017/10/20/actualidad/1508503663_430515.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508507460_569874.html 
Scraping: https://elpais.com/cultura/2017/10/20/actualidad/1508488913_681643.html 
Scraping: https://elpais.com/internacional/2017/10/20/actualidad/1508506096_337991.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508503572_812343.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508488656_838766.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508489106_542799.html 
Scraping: https://elpais.com/ccaa/2017/10/19/valencia/1508445805_457854.html 
Scraping: https://elpais.com/elpais/2017/10/20/album/1508487891_134872.html 
Non-text based article. Link skipped.
Scraping: https://elpais.com/ccaa/2017/10/20/catalunya/1508492661_274873.html 
Scraping: https://elpais.com/elpais/2017/10/19/ciencia/1508412461_971020.html 
Scraping: https://elpais.com/ccaa/2017/10/20/andalucia/1508499080_565687.html 
Scraping: https://elpais.com/ccaa/2017/10/20/catalunya/1508495565_034721.html 
Scraping: https://elpais.com/cultura/2017/10/19/actualidad/1508403967_099974.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508496322_284364.html 
Scraping: https://elpais.com/economia/2017/10/19/actualidad/1508431364_731058.html 
Scraping: https://elpais.com/elpais/2017/10/20/album/1508491490_512616.html 
Non-text based article. Link skipped.
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508481079_647952.html 
Scraping: https://elpais.com/ccaa/2017/10/20/valencia/1508493387_961965.html 
Scraping: https://elpais.com/economia/2017/10/20/actualidad/1508492104_302263.html 
Content written to disk!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I applied the function to the tag &lt;code&gt;gastronomia&lt;/code&gt; (gastronomy) in the same fashion. The results are stored in the &lt;code&gt;data/original/&lt;/code&gt; directory. Our complete data structure for this post looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── elpais
    │   ├── gastronomy_articles.csv
    │   └── political_articles.csv
    ├── gutenberg
    │   ├── works_pq.csv
    │   └── works_pr.csv
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

8 directories, 10 files&lt;/code&gt;&lt;/pre&gt;
&lt;!-- then web scrape the State of the Union Addresses acquiring both raw text and meta-data.

Consider how to store the data: `.csv` or `.xml`?

--&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-text-from-other-formats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting text from other formats&lt;/h2&gt;
&lt;!-- TODO: include discussion on how to download Word and PDF files from the web. --&gt;
&lt;p&gt;As a final note it is worth pointing out that machine-readable data for analysis is often trapped in other formats such as Word or PDF files. R provides packages for working with these formats and can extract the text programmatically. See &lt;a href=&#34;https://github.com/ropensci/antiword#readme&#34;&gt;antiword&lt;/a&gt; for Word files and &lt;a href=&#34;https://ropensci.org/blog/2016/03/01/pdftools-and-jeroen&#34;&gt;pdftools&lt;/a&gt; for PDF files. In the case that a PDF is an image that needs OCR (Optical Character Recognition), you can experiment with the &lt;a href=&#34;https://ropensci.org/blog/blog/2016/11/16/tesseract&#34;&gt;tessseract&lt;/a&gt; package. It is important to be aware, however, that recovering plain text from these formats can often result in conversion artifacts; especially using OCR. Not to worry, we can still work with the data it just might mean more pre-processing before we get to doing our analysis.&lt;/p&gt;
&lt;!-- show how to extract text from Word documents `antiword`, from PDF files `pdftools`, and OCR `tesseract`. --&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we covered scraping language data from the web. The &lt;code&gt;rvest&lt;/code&gt; package provides a host of functions for downloading and parsing HTML. We first looked at a toy example to get a basic understanding of how HTML works and then moved to applying this knowledge to a practical example. To maintain a reproducible workflow, the code developed in this example was grouped into task-oriented functions which were in turn joined and wrapped into a function that provided convenient access to our workflow and avoided unnecessary downloads (in the case the data already exists on disk).&lt;/p&gt;
&lt;p&gt;Here we have built on previously introduced R coding concepts and demonstrated various others. Web scraping often requires more knowledge of and familiarity with R as well as other web technologies. Rest assured, however, practice will increase confidence in your abilities. I encourage you to practice on your own with other websites. You will encounter problems. Consult the R documentation in RStudio or online and lean on the R community on the web at sites such as &lt;a href=&#34;https://stackoverflow.com&#34;&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At this point you have both a bird’s eye view of the data available on the web and strategies on how to access a great majority of it. It is now time to turn to the next step in our data analysis project: data curation. In the next posts I will cover how to wrangle your raw data into a tidy dataset. This will include working with and incorporating meta-data as well as augmenting a dataset with linguistic annotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-rvest&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2016. &lt;em&gt;Rvest: Easily Harvest (Scrape) Web Pages&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rvest&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rvest&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;———. 2017. &lt;em&gt;Tidyverse: Easily Install and Load ’Tidyverse’ Packages&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;tibble&lt;/code&gt; objects are &lt;code&gt;data.frame&lt;/code&gt; objects with some added extra bells and whistles that we won’t get into here.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (2/3): package interfaces</title>
      <link>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;!-- TODO:
--&gt;
&lt;div id=&#34;package-interfaces&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package interfaces&lt;/h2&gt;
&lt;p&gt;A convenient alternative method for acquiring data in R is through package interfaces to web services. These interfaces are built using R code to make connections with resources on the web through &lt;strong&gt;Automatic Programming Interfaces&lt;/strong&gt; (APIs). Websites such as Project Gutenberg, Twitter, Facebook, and many others provide APIs to allow access to their data under certain conditions, some more limiting for data collection than others. Programmers (like you!) in the R community take up the task of wrapping calls to an API with R code to make accessing that data from R possible. For example, &lt;a href=&#34;https://CRAN.R-project.org/package=gutenbergr&#34;&gt;gutenbergr&lt;/a&gt; provides access to Project Gutenberg, &lt;a href=&#34;https://CRAN.R-project.org/package=rtweet&#34;&gt;rtweet&lt;/a&gt; to Twitter, and &lt;a href=&#34;https://CRAN.R-project.org/package=Rfacebook&#34;&gt;Rfacebook&lt;/a&gt; to Facebook.&lt;/p&gt;
&lt;p&gt;Using R package interfaces, however, often requires some more knowledge about R objects and functions. Let’s take a look at how to access data from Project Gutenberg through the &lt;code&gt;gutenbergr&lt;/code&gt; package. Along the way we will touch upon various functions and concepts that are key to working with the R data types vectors and data frames including filtering and writing tabular data to disk in plain-text format.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The following code is available on GitHub &lt;code&gt;recipes-acquiring_data&lt;/code&gt; and is built on the &lt;code&gt;recipes-project_template&lt;/code&gt; I have discussed in detail &lt;a href=&#34;https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/&#34;&gt;here&lt;/a&gt; and made accessible &lt;a href=&#34;https://github.com/francojc/recipes-project_template.git&#34;&gt;here&lt;/a&gt;. I encourage you to follow along by downloading the &lt;code&gt;recipes-project_template&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; from the Terminal or create a new RStudio R Project and select the “Version Control” option.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;To get started let’s install and load the package. The most simple method for downloading an R package in RStudio is to select the ‘Packages’ tab in the Files pane and click the ‘Install’ icon. To ensure that our code is reproducible, however, it is better to approach the installation of packages programmatically. If the package is not part of the R base library, we will not assume that the user will have the package on their system. The code to install and load the &lt;code&gt;gutenbergr&lt;/code&gt; package is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;gutenbergr&amp;quot;) # install `gutenbergr` package
library(gutenbergr) # load the `gutenbergr` package&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach works just fine, but luck has it that there is an R package for installing and loading packages! The &lt;a href=&#34;https://CRAN.R-project.org/package=pacman&#34;&gt;pacman&lt;/a&gt; package includes a set of functions for managing packages. A very useful one is &lt;code&gt;p_load()&lt;/code&gt; which will look for a package on a system, load it if it is found, and install and then load it if it is not found. This helps potentially avoid using unnecessary bandwidth to install packages that may already exist on a user’s system. But, to use &lt;code&gt;pacman&lt;/code&gt; we need to include the code to install and load it with the functions &lt;code&gt;install.packages()&lt;/code&gt; and &lt;code&gt;library()&lt;/code&gt;. I’ve included some code that will mimic the behavior of &lt;code&gt;p_load()&lt;/code&gt; for installing &lt;code&gt;pacman&lt;/code&gt; itself, but as you can see it is not elegant, luckily it’s only used once as we add it to the SETUP section of our master file, &lt;code&gt;_pipeline.R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load `pacman`. If not installed, install then load.
if (!require(&amp;quot;pacman&amp;quot;, character.only = TRUE)) {
  install.packages(&amp;quot;pacman&amp;quot;)
  library(&amp;quot;pacman&amp;quot;, character.only = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have &lt;code&gt;pacman&lt;/code&gt; installed and loaded into our R session, let’s use the &lt;code&gt;p_load()&lt;/code&gt; function to make sure to install/ load the two packages we will need for the upcoming tasks. If you are following along with the &lt;code&gt;recipes-project_template&lt;/code&gt;, add this code within the SETUP section of the &lt;code&gt;acquire_data.R&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Script-specific options or packages
pacman::p_load(tidyverse, gutenbergr)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Note that the arguments &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;gutenbergr&lt;/code&gt; are comma-separated but not quoted.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Project Gutenberg provides access to thousands of texts in the public domain. The &lt;code&gt;gutenbergr&lt;/code&gt; package contains a set of tables, or &lt;strong&gt;data frames&lt;/strong&gt; in R speak, that index the meta-data for these texts broken down by text (&lt;code&gt;gutenberg_metadata&lt;/code&gt;), author (&lt;code&gt;gutenberg_authors&lt;/code&gt;), and subject (&lt;code&gt;gutenberg_subjects&lt;/code&gt;). I’ll use the &lt;code&gt;glimpse()&lt;/code&gt; function loaded in the &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;tidyverse&lt;/a&gt; package&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; to summarize the structure of these data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_metadata) # summarize text meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 51,997
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ...
## $ title               &amp;lt;chr&amp;gt; NA, &amp;quot;The Declaration of Independence of th...
## $ author              &amp;lt;chr&amp;gt; NA, &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;United States&amp;quot;, ...
## $ gutenberg_author_id &amp;lt;int&amp;gt; NA, 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, ...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; NA, &amp;quot;United States Law/American Revolution...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_authors) # summarize authors meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 16,236
## Variables: 7
## $ gutenberg_author_id &amp;lt;int&amp;gt; 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 1...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;United States&amp;quot;, &amp;quot;Lincoln, Abraham&amp;quot;, &amp;quot;Henr...
## $ alias               &amp;lt;chr&amp;gt; NA, NA, NA, NA, &amp;quot;Dodgson, Charles Lutwidge...
## $ birthdate           &amp;lt;int&amp;gt; NA, 1809, 1736, NA, 1832, NA, 1819, 1860, ...
## $ deathdate           &amp;lt;int&amp;gt; NA, 1865, 1799, NA, 1898, NA, 1891, 1937, ...
## $ wikipedia           &amp;lt;chr&amp;gt; NA, &amp;quot;http://en.wikipedia.org/wiki/Abraham_...
## $ aliases             &amp;lt;chr&amp;gt; NA, &amp;quot;United States President (1861-1865)/L...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_subjects) # summarize subjects meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 140,173
## Variables: 3
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5...
## $ subject_type &amp;lt;chr&amp;gt; &amp;quot;lcc&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcs...
## $ subject      &amp;lt;chr&amp;gt; &amp;quot;E201&amp;quot;, &amp;quot;United States. Declaration of Independen...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The &lt;code&gt;gutenberg_metadata&lt;/code&gt;, &lt;code&gt;gutenberg_authors&lt;/code&gt;, and &lt;code&gt;gutenberg_subjects&lt;/code&gt; are periodically updated. To check to see when each data frame was last updated run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attr(gutenberg_metadata, &amp;quot;date_updated&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2016-05-05&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;To download the text itself we use the &lt;code&gt;gutenberg_download()&lt;/code&gt; function which takes one required argument, &lt;code&gt;gutenberg_id&lt;/code&gt;. The &lt;code&gt;gutenberg_download()&lt;/code&gt; function is what is known as ‘vectorized’, that is, it can take a single value or multiple values for the argument &lt;code&gt;gutenberg_id&lt;/code&gt;. Vectorization refers to the process of applying a function to each of the elements stored in a &lt;strong&gt;vector&lt;/strong&gt; –a primary object type in R. A vector is a grouping of values of one of various types including character (&lt;code&gt;chr&lt;/code&gt;), integer (&lt;code&gt;int&lt;/code&gt;), and logical (&lt;code&gt;lgl&lt;/code&gt;) and a data frame is a grouping of vectors. The &lt;code&gt;gutenberg_download()&lt;/code&gt; function takes an integer vector which can be manually added or selected from the &lt;code&gt;gutenberg_metadata&lt;/code&gt; or &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frames using the &lt;code&gt;$&lt;/code&gt; operator (e.g. &lt;code&gt;gutenberg_metadata$gutenberg_id&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let’s first add them manually here as a toy example by generating a vector of integers from 1 to 5 assigned to the variable name &lt;code&gt;ids&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids &amp;lt;- 1:5 # integer vector of values 1 to 5
ids&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the works from Project Gutenberg corresponding to the &lt;code&gt;gutenberg_id&lt;/code&gt;s 1 to 5, we pass the &lt;code&gt;ids&lt;/code&gt; object to the &lt;code&gt;gutenberg_download()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;works_sample &amp;lt;- gutenberg_download(gutenberg_id = ids) # download works with `gutenberg_id` 1-5
glimpse(works_sample) # summarize `works` dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,939
## Variables: 2
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;December, 1971  [Etext #1]&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;The Projec...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two attributes are returned: &lt;code&gt;gutenberg_id&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;. The &lt;code&gt;text&lt;/code&gt; column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded. There are many more attributes available from the Project Gutenberg API that can be accessed by passing a character vector of the attribute names to the argument &lt;code&gt;meta_fields&lt;/code&gt;. The column names of the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame contains the available attributes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(gutenberg_metadata) # print the column names of the `gutenberg_metadata` data frame&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;gutenberg_id&amp;quot;        &amp;quot;title&amp;quot;               &amp;quot;author&amp;quot;             
## [4] &amp;quot;gutenberg_author_id&amp;quot; &amp;quot;language&amp;quot;            &amp;quot;gutenberg_bookshelf&amp;quot;
## [7] &amp;quot;rights&amp;quot;              &amp;quot;has_text&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s augment our previous download with the title and author of each of the works. To create a character vector we use the &lt;code&gt;c()&lt;/code&gt; function, then, quote and delimit the individual elements of the vector with a comma.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download works with `gutenberg_id` 1-5 including `title` and `author` as attributes
works_sample &amp;lt;- gutenberg_download(gutenberg_id = ids, 
                            meta_fields = c(&amp;quot;title&amp;quot;,
                                            &amp;quot;author&amp;quot;))
glimpse(works_sample) # summarize dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,939
## Variables: 4
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;December, 1971  [Etext #1]&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;The Projec...
## $ title        &amp;lt;chr&amp;gt; &amp;quot;The Declaration of Independence of the United St...
## $ author       &amp;lt;chr&amp;gt; &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;Jeffer...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in a more practical scenario we would like to select the values of &lt;code&gt;gutenberg_id&lt;/code&gt; by some principled query such as works from a specific author, language, or subject. To do this we first query either the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame or the &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frame. Let’s say we want to download a random sample of 10 works from English Literature (Library of Congress Classification, “PR”). Using the &lt;code&gt;filter()&lt;/code&gt; function (part of the &lt;code&gt;tidyverse&lt;/code&gt; package set) we first extract all the Gutenberg ids from &lt;code&gt;gutenberg_subjects&lt;/code&gt; where &lt;code&gt;subject_type == &amp;quot;lcc&amp;quot;&lt;/code&gt; and &lt;code&gt;subject == &amp;quot;PR&amp;quot;&lt;/code&gt; assigning the result to &lt;code&gt;ids&lt;/code&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids &amp;lt;- 
  filter(gutenberg_subjects, subject_type == &amp;quot;lcc&amp;quot;, subject == &amp;quot;PR&amp;quot;)
glimpse(ids)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 7,100
## Variables: 3
## $ gutenberg_id &amp;lt;int&amp;gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 5...
## $ subject_type &amp;lt;chr&amp;gt; &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, ...
## $ subject      &amp;lt;chr&amp;gt; &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;The operators &lt;code&gt;=&lt;/code&gt; and &lt;code&gt;==&lt;/code&gt; are not equivalents. &lt;code&gt;==&lt;/code&gt; is used for logical evaluation and &lt;code&gt;=&lt;/code&gt; is an alternate notation for variable assignment (&lt;code&gt;&amp;lt;-&lt;/code&gt;).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frame does not contain information as to whether a &lt;code&gt;gutenberg_id&lt;/code&gt; is associated with a plain-text version. To limit our query to only those English Literature works with text, we filter the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame by the ids we have selected in &lt;code&gt;ids&lt;/code&gt; and the attribute &lt;code&gt;has_text&lt;/code&gt; in the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids_has_text &amp;lt;- 
  filter(gutenberg_metadata, gutenberg_id %in% ids$gutenberg_id, has_text == TRUE)
glimpse(ids_has_text)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 6,724
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43...
## $ title               &amp;lt;chr&amp;gt; &amp;quot;Alice&amp;#39;s Adventures in Wonderland&amp;quot;, &amp;quot;Throu...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;Carroll, Lewis&amp;quot;, &amp;quot;Carroll, Lewis&amp;quot;, &amp;quot;Carro...
## $ gutenberg_author_id &amp;lt;int&amp;gt; 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 3...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; &amp;quot;Children&amp;#39;s Literature&amp;quot;, &amp;quot;Children&amp;#39;s Liter...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;A couple R programming notes on the code phrase &lt;code&gt;gutenberg_id %in% ids$gutenberg_id&lt;/code&gt;. First, the &lt;code&gt;$&lt;/code&gt; symbol in &lt;code&gt;ids$gutenberg_id&lt;/code&gt; is the programmatic way to target a particular column in an R data frame. In this example we select the &lt;code&gt;ids&lt;/code&gt; data frame and the column &lt;code&gt;gutenberg_id&lt;/code&gt;, which is a integer vector. The &lt;code&gt;gutenberg_id&lt;/code&gt; variable that precedes the &lt;code&gt;%in%&lt;/code&gt; operator does not need an explicit reference to a data frame because the primary argument of the &lt;code&gt;filter()&lt;/code&gt; function is this data frame (&lt;code&gt;gutenberg_metadata&lt;/code&gt;). Second, the &lt;code&gt;%in%&lt;/code&gt; operator logically evaluates whether the vector elements in &lt;code&gt;gutenberg_metadata$gutenberg_ids&lt;/code&gt; are also found in the vector &lt;code&gt;ids$gutenberg_id&lt;/code&gt; returning &lt;code&gt;TRUE&lt;/code&gt; and &lt;code&gt;FALSE&lt;/code&gt; accordingly. This effectively filters those ids which are not in both vectors.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;As we can see the number of works with text is fewer than the number of works listed, 7100 versus 6724. Now we can safely do our random selection of 10 works, with the function &lt;code&gt;sample_n()&lt;/code&gt; and be confident that the ids we select will contain text when we take the next step by downloading the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123) # make the sampling reproducible
ids_sample &amp;lt;- sample_n(ids_has_text, 10) # sample 10 works
glimpse(ids_sample) # summarize the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 10
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 7688, 33533, 12160, 37761, 40406, 1050, 18...
## $ title               &amp;lt;chr&amp;gt; &amp;quot;Lucretia — Volume 04&amp;quot;, &amp;quot;The Convict&amp;#39;s Far...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;Lytton, Edward Bulwer Lytton, Baron&amp;quot;, &amp;quot;Pa...
## $ gutenberg_author_id &amp;lt;int&amp;gt; 761, 35765, 1865, 1256, 25821, 467, 1062, ...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, &amp;quot;One Act Plays&amp;quot;, NA, N...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can now pass our ids (&lt;code&gt;ids_sample$gutenberg_id&lt;/code&gt;) as the argument of &lt;code&gt;gutenberg_download()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;works_pr &amp;lt;- gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, meta_fields = c(&amp;quot;author&amp;quot;, &amp;quot;title&amp;quot;))
glimpse(works_pr) # summarize the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 79,200
## Variables: 4
## $ gutenberg_id &amp;lt;int&amp;gt; 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;THE DARK LADY OF THE SONNETS&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;By Bernard S...
## $ author       &amp;lt;chr&amp;gt; &amp;quot;Shaw, Bernard&amp;quot;, &amp;quot;Shaw, Bernard&amp;quot;, &amp;quot;Shaw, Bernard&amp;quot;...
## $ title        &amp;lt;chr&amp;gt; &amp;quot;The Dark Lady of the Sonnets&amp;quot;, &amp;quot;The Dark Lady of...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have data and could move on to processing this data in preparation for analysis. However, we are aiming for a reproducible workflow and this code does not conform to our principle of modularity: each subsequent step in our analysis will depend on running this code first. Furthermore, running this code as it is creates issues with bandwidth, as in our previous examples from direct downloads. To address modularity we will write the data to disk in &lt;strong&gt;plain-text format&lt;/strong&gt;. In this way each subsequent step in our analysis can access the data locally. To address bandwidth concerns, we will devise a method for checking to see if the data is already downloaded and skip the download, if possible, to avoid accessing the Project Gutenberg server unnecessarily.&lt;/p&gt;
&lt;p&gt;To write our data frame to disk we will export it into a standard plain-text format for two-dimensional data: a CSV file (comma-separated value). The CSV structure for this data will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## gutenberg_id,text,author,title
## 1050,THE DARK LADY OF THE SONNETS,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,By Bernard Shaw,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line contains the names of the columns and subsequent lines the observations. Data points that contain commas themselves (e.g. “Shaw, Bernard”) are quoted to avoid misinterpreting these commas a deliminators in our data. To write this data to disk we will use the &lt;code&gt;write_csv()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(works_pr, path = &amp;quot;data/original/gutenberg_works_pr.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To avoid downloading data that already resides on disk, let’s implement a similar strategy to the one used in the previous post for &lt;a href=&#34;https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/&#34;&gt;direct downloads&lt;/a&gt;. I’ve incorporated the code for sampling and downloading data for a particular subject from Project Gutenberg with a control statement to check if the data file already exists into a function I named &lt;code&gt;get_gutenberg_subject()&lt;/code&gt;. Take a look at this function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_gutenberg_subject &amp;lt;- function(subject, target_file, sample_size = 10) {
  # Function: to download texts from Project Gutenberg with 
  # a specific LCC subject and write the data to disk.
  
  # Check to see if the data already exists
  if(!file.exists(target_file)) { # if data does not exist, download and write
    target_dir &amp;lt;- dirname(x) # generate target directory for the .csv file
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&amp;quot;Downloading data... \n&amp;quot;) # print status message
    # Select all records with a particular LCC subject
    ids &amp;lt;- 
      filter(gutenberg_subjects, 
             subject_type == &amp;quot;lcc&amp;quot;, subject == subject) # select subject
    # Select only those records with plain text available
    set.seed(123) # make the sampling reproducible
    ids_sample &amp;lt;- 
      filter(gutenberg_metadata, 
             gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames 
             has_text == TRUE) %&amp;gt;% # select those ids that have text
      sample_n(sample_size) # sample N works (default N = 10)
    # Download sample with associated `author` and `title` metadata
    works_sample &amp;lt;- 
      gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, 
                         meta_fields = c(&amp;quot;author&amp;quot;, &amp;quot;title&amp;quot;))
    # Write the dataset to disk in .csv format
    write_csv(works_sample, path = target_file)
    cat(&amp;quot;Data downloaded! \n&amp;quot;) # print status message
  } else { # if data exists, don&amp;#39;t download it again
    cat(&amp;quot;Data already exists \n&amp;quot;) # print status message
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding this function to our function script &lt;code&gt;functions/acquire_functions.R&lt;/code&gt;, we can now use this function in our &lt;code&gt;code/acquire_data.R&lt;/code&gt; script to download multiple subjects and store them in on disk in their own file.&lt;/p&gt;
&lt;p&gt;Let’s download American Literature now (LCC code “PQ”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download Project Gutenberg text for subject &amp;#39;PQ&amp;#39; (American Literature)
# and then write this dataset to disk in .csv format
get_gutenberg_subject(subject = &amp;quot;PQ&amp;quot;, target_file = &amp;quot;data/original/gutenberg/works_pq.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Applying this function to both the English and American Literature datasets, our data directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── gutenberg
    │   ├── works_pq.csv
    │   └── works_pr.csv
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

7 directories, 8 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And as before in the previous post, it is a good idea to log the results of our work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log the directory structure of the Project Gutenberg data
system(command = &amp;quot;tree data/original/gutenberg &amp;gt;&amp;gt; log/data_original_gutenberg.log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- then work with a more complex package interface. Introduce the `fulltext` package (or [crminer](https://github.com/ropensci/crminer) . Show how to search a specific publication, download the full text (in XML) format. Then extract the `doi`, `title`, and `abstract`, convert it to a data.frame and store it as a `.csv` file. 

TODO: - work with `fulltext` and `crminer` exploration/ package tutorials
      - determine the R skills needed to complete this activity

Search for Plos ONE publications in Linguistics?
--&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post I provided an overview to acquiring data from web service APIs through R packages. We took at closer look at the &lt;code&gt;gutenbergr&lt;/code&gt; package which provides programmatic access to works available on Project Gutenberg. Working with package interfaces requires more knowledge of R including loading/ installing packages, working with vectors and data frames, and exporting data from an R session. We touched on these programming concepts and also outlined a method to create a reproducible workflow.&lt;/p&gt;
&lt;p&gt;Our last step in this mini series on acquiring data for language research with R, we will explore methods for acquire language data from the browsable web. I will discuss using the &lt;code&gt;rvest&lt;/code&gt; package for downloading and isolating text elements from HTML pages and show how to organize and write the data to disk.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-gutenbergr&#34;&gt;
&lt;p&gt;Robinson, David. 2017. &lt;em&gt;Gutenbergr: Download and Process Public Domain Works from Project Gutenberg&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=gutenbergr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=gutenbergr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2017. &lt;em&gt;Tidyverse: Easily Install and Load ’Tidyverse’ Packages&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;tidyverse&lt;/code&gt; is not a typical package. It is a set of packages: &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;readr&lt;/code&gt;, &lt;code&gt;purrr&lt;/code&gt;, and &lt;code&gt;tibble&lt;/code&gt;. These packages are all installed/ loaded with &lt;code&gt;tidyverse&lt;/code&gt; and form the backbone for the type of work you will typically do in most analyses.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://www.loc.gov/catdir/cpso/lcco/&#34;&gt;Library of Congress Classification&lt;/a&gt; documentation for a complete list of subject codes.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
