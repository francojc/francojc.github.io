<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data on francojc ⟲</title>
    <link>https://francojc.github.io/tags/data/</link>
    <description>Recent content in Data on francojc ⟲</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jerid Francom</copyright>
    <lastBuildDate>Wed, 04 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/data/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Data for language research -types and sources</title>
      <link>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</guid>
      <description>&lt;p&gt;In this Recipe you will learn about the types of data available for language research and where to find data. The goal, then, is to introduce you to the landscape of language data available and provide a general overview of the characteristics of language data from a variety of sources providing you with resources to begin your own quantitative investigations.&lt;/p&gt;
&lt;div id=&#34;data-for-language-research&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data for language research&lt;/h2&gt;
&lt;p&gt;Language research can include data from a variety of sources, linguistic and non-linguistic, that record observations about the world. A typical type of data used in quantitative language research is a &lt;strong&gt;corpus&lt;/strong&gt;. In short, a corpus is set of machine-readable texts that have been compiled with an eye towards linguistic research. All corpora are not created equal, however, in content and/or format. A corpus may aim to represent a wide swath of language behavior or very specific aspects. It can be language specific (English or French), target a particular modality (spoken or written), or approximate domains of language use (medicine, business, etc.). A corpus that aims to represent a language (including modalities, registers, and sub-domains), for example, is known as a &lt;em&gt;generalized corpus&lt;/em&gt;. Corpora that aim to capture a snapshot of a particular modality or sub-domain of language use are known as &lt;em&gt;specialized corpora&lt;/em&gt;. Each corpus will have an underlying target population and the sampling process will reflect the authors’ best attempt (given the conditions at the point the corpus was compiled) at representing the stated target population. Whether a corpus is generalized or specialized can become difficult to nail down between the extremes. As such, it is key to be clear about the scope of a particular corpus to be able to ascertain its potential applications and gauge the extent to which these applications entail the research goals of your particular project.&lt;/p&gt;
&lt;p&gt;A corpus will often include various types of non-linguistic attributes, or &lt;em&gt;meta-data&lt;/em&gt;, as well. Ideally this will include information regarding the source(s) of the data, dates when it was acquired or published, and other author or speaker information. It may also include any number of other attributes that were identified as potentially important in order to appropriately document the target population. Again, it is key to match the available meta-data with the goals of your research. In some cases a corpus may be ideal in some aspects but not contain all the key information to address your research question. This may mean you will need to compile your own corpus if there are fundamental attributes missing. Before you consider compiling your own corpus, however, it is worth investigating the possibility of augmenting an available corpus to bring it inline with your particular goals. This may include adding new language sources, harnessing software for linguistic annotation (part-of-speech, syntactic structure, named entities, etc.), or linking available corpus meta-data to other resources, linguistic or non-linguistic.&lt;/p&gt;
&lt;p&gt;Corpora come in various formats, the main three being: running text, structured documents, and databases. The format of a corpus is often influenced by characteristics of the data but may also reflect an author’s individual preferences as well. It is typical for corpora with few meta-data characteristics to take the form of running text. In corpora with more meta-data a header may be appended to the top of each running text document or the meta-data may be contained in a separate file with appropriate coding to coordinate meta-data attributes with each text in the corpus. When meta-data increases in complexity it is common to structure each corpus document more explicitly with a markup language such as XML (Extensible Markup Language) or organize relationships between language and meta-data attributes in a database. Although there has been a push towards standardization of corpus formats, most available resources display some degree of idiosyncrasy. Being able to parse the structure of a corpus is a skill that will develop with time. With more experience working with corpora you will become more adept at identifying how the data is stored and whether its content and format will serve the needs of your analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sources-of-language-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources of language data&lt;/h2&gt;
&lt;p&gt;The most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes and language used in natural settings that can be coerced by the investigator into a corpus. Many organizations exist around the globe that provide access to corpora in browsable catalogs, or &lt;strong&gt;repositories&lt;/strong&gt;. There are repositories dedicated to language research, in general, such as the &lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Language Data Consortium&lt;/a&gt; or for specific language domains, such as the language acquisition repository &lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication. A standardized resource many times will be easier to interpret and evaluate for its appropriateness for a particular research project.&lt;/p&gt;
&lt;p&gt;In the table below I’ve compiled a list of some corpus repositories to help you get started.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-repositories&#34;&gt;Table 1: &lt;/span&gt;A list of some language corpora repositories.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus.byu.edu/&#34;&gt;BYU corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A repository of corpora that includes billions of words of data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://corporafromtheweb.org/&#34;&gt;COW (COrpora from the Web)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of linguistically processed gigatoken web corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://wortschatz.uni-leipzig.de/en/download/&#34;&gt;Leipzig Corpora Collection&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpora in different languages using the same format and comparable sources.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Linguistic Data Consortium&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.resourcebook.eu/searchll.php#&#34;&gt;LRE Map&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.nltk.org/nltk_data/&#34;&gt;NLTK language data&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of corpora and language datasets included with the Python package NLTK.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/&#34;&gt;OPUS - an open source parallel corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of translated texts from the web.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus1.mpi.nl/ds/asv/?4&#34;&gt;The Language Archive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Various corpora and language datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://ota.ox.ac.uk/&#34;&gt;The Oxford Text Archive (OTA)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of thousands of texts in more than 25 different languages.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Repositories are by no means the only source of corpora on the web. Researchers from around the world provide access to corpora and other data sources on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. Finding these resources is a matter of doing a web search with the word ‘corpus’ and a list of desired attributes, including language, modality, register, etc. As part of a general movement towards reproducible more corpora are available on the web than ever before. Therefore data sharing platforms supporting reproducible research, such as &lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;, &lt;a href=&#34;http://www.re3data.org/&#34;&gt;Re3data&lt;/a&gt;, etc., are a good place to look as well, if searching repositories and targeted web searches do not yield results.&lt;/p&gt;
&lt;p&gt;In the table below you will find a list of corpus resources and datasets.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-corpora&#34;&gt;Table 2: &lt;/span&gt;Corpora and language datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://storage.googleapis.com/books/ngrams/books/datasetsv2.html&#34;&gt;Google Ngram Viewer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Google web corpus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~enron/&#34;&gt;Enron Email Dataset&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Enron email data from about 150 users, mostly senior management.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://cesa.arizona.edu/&#34;&gt;Corpus of Spanish in Southern Arizona&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Spanish varieties spoken in Arizona.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/OpenSubtitles_v2.php&#34;&gt;OpenSubtitles2011&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of documents from &lt;a href=&#34;http://www.opensubtitles.org/&#34; class=&#34;uri&#34;&gt;http://www.opensubtitles.org/&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.statmt.org/europarl/&#34;&gt;Europarl Parallel Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A parallel corpus based on the proceedings of the European Parliament&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.lllf.uam.es/~fmarcos/informes/corpus/coarginl.html&#34;&gt;Corpus Argentino&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpus of Argentine Spanish&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.ruscorpora.ru/en/&#34;&gt;Russian National Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus of modern Russian language incorporating over 300 million words.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html&#34;&gt;Cornell Movie-Dialogs Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is important to note that there can be access and use restrictions for data from particular sources. Compiling, hosting, and maintaining corpus resources can be costly. To gain full access to data, some repositories and homepages of larger corpora require a fee to offset these costs. In other cases, resources may require individual license agreements to ensure that the data is not being used in ways it was not intended or to ensure potentially sensitive participant information will be treated appropriately. You can take a look at a &lt;a href=&#34;https://www.corpusdata.org/restrictions.asp&#34;&gt;license agreement for the BYU Corpora&lt;/a&gt; as an example. If you are a member of an academic institution and aim to conduct research for scholarly purposes licensing is often easily obtained. Fees, on the other hand, may present a more challenging obstacle. If you are an affiliate of an academic institution it is worth checking with your library to see if there are funds for acquiring licensing for you as an individual, a research group or lab or, for the institution.&lt;/p&gt;
&lt;p&gt;If your corpus search ends in a dead-end, either because a suitable resource does not appear to exist or an existing resource is unattainable given licensing restrictions or fees, it may be time to compile your own corpus. Turning to machine readable texts on the internet is usually the logical first step to access language for a new corpus. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on social media, news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by hand instead of automating the task. Our approach here, however, will be to automate the process as much as possible whether that means leveraging R package interfaces to language data, converting hundreds of pdf documents to plain text, or scraping content from web documents.&lt;/p&gt;
&lt;p&gt;The table below lists some R packages that serve to interface language data directly through R.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-apis&#34;&gt;Table 3: &lt;/span&gt;R Package interfaces to language corpora and datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/crminer&#34;&gt;crminer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface focusing on getting the user full text via the Crossref search API.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/arxiv_tutorial.html&#34;&gt;aRxiv&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/internetarchive_tutorial.html&#34;&gt;internetarchive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/dvn&#34;&gt;dvn&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to access to the Dataverse Network APIs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/gutenbergr_tutorial.html&#34;&gt;gutenbergr&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to download and process public domain works from the Project Gutenberg collection.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/fulltext_tutorial.html&#34;&gt;fulltext&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query open access journals, such as PLOS.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/hrbrmstr/newsflash&#34;&gt;newsflash&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive and GDELT Television Explorer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/oai&#34;&gt;oai&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query any OAI-PMH repository, including Zenodo.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/rfigshare&#34;&gt;rfigshare&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the data sharing platform FigShare.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Data for language research is not limited to (primary) text sources. Other sources may include processed data from previous research; word lists, linguistic features, etc.. Alone or in combination with text sources this data can be a rich and viable source of data for a research project.&lt;/p&gt;
&lt;p&gt;Below I’ve included some processed language resources.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-experimental&#34;&gt;Table 4: &lt;/span&gt;Language data from previous research and meta-studies.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/lingtypology&#34;&gt;lingtypology&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://elexicon.wustl.edu/WordStart.asp&#34;&gt;English Lexicon Project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://icon.shef.ac.uk/Moby/&#34;&gt;The Moby lexicon project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Language wordlists and resources from the Moby project.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The list of data available for language research is constantly growing. I’ve document very few of the wide variety of resources. Below I’ve included attempts by others to provide a summary of the corpus data and language resources available.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-listing&#34;&gt;Table 5: &lt;/span&gt;Lists of corpus resources.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html&#34;&gt;Learner corpora around the world&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A listing of learner corpora around the world&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/&#34;&gt;Where can you find language data on the web?&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of various corpora and language datasets.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://nlp.stanford.edu/links/statnlp.html#Corpora&#34;&gt;Stanford NLP corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of corpora and language resources aimed at the NLP community.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we have covered some of the basics on data for language research including types of data and sources to get you started on the path of identifying a viable source for your data analysis project. In the next post we will begin working directly with R code to access and acquire data through R. Along the way I will introduce fundamental programming concepts of the language you will use throughout your project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- Topics:

- natural and naturalistic
modality, register (formal/ informal), 
  - spoken (Callhome -LDC, ...)
  - written (literature -gutenbergr, periodicals -LDC, blogs, social media -streamR, ...)
  - other (tv/film closed captions -newsflash, Brysbaert/ ACTIV-ES, translation -OPEL?, ...)
- elicited data
  - essays (-BELC, )
  - interviews (Santa Barbara Corpus)
  - surveys (US Census -acs, Harvard Dialect Survey, Language attitude)
  - experimental findings
    - response times (MRC lexicon), eye-gaze, acceptability ratings, etc. 
    
- 


- Types of data
  - Uncurated
  - Curated 
- Sources of data
    - Repositories
    - Sources
    - Web
- Data formats
  - Plain text
    - .txt
    - .csv/ .tsv
    - .xml/ .json
  - Other files
    - .doc(x)
    - .pdf
  - Databases
- How to acquire data
    - Package interface (`gutenbergr`)
    - API interface (`streamR`)
    - Download, read (multiple files, w/ lapply and scan?
    - Web scraping (`rvest`)
--&gt;
&lt;!-- ## Types of data --&gt;
&lt;!-- In the last post we discussed what data is and the importance of data sampling and organization is for subsequent data analysis. We touched briefly on an example in which we were working with files which our language data was in running text format. Running text is one of the types of data that you will encounter when you look to obtain data to conduct research into the topic you are interested in knowing something more about. In this post we leared that text in this format needed to be organized into a format that was more conducive for statistical analysis. The aim, then, was to **curate** the **uncurated** data. Athough data is often talked about in terms of being curated or uncurated, it is important to understand that this is less a dicotomy and more of a continuum. Since each analysis has specific goals the primary data is always in need of some amount of curation to prepare the data in the particular ways necessary to faciliate the particular goals of the particular analysis.   --&gt;
&lt;!-- ## Sources of data --&gt;
&lt;!-- Having said that it is convenient to talk about curation in binary terms as it facilitates a discussion of the sources of data out there and to what degree the researcher needs to manipulate the data from particular sources to be able to conduct an analysis. When looking at the landscape of language data available, sources that have been prepared to some degree to facilitate analysis are often found in *repositories*. There are countless repositories that can be accessed on the web.  --&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-mcenery:2012&#34;&gt;
&lt;p&gt;McEnery, Tony, and Andrew Hardie. 2012. &lt;em&gt;Corpus Linguistics: Method, Theory and Practice&lt;/em&gt;. Cambridge University Press. doi:&lt;a href=&#34;https://doi.org/10.1017/CBO9780511981395&#34;&gt;10.1017/CBO9780511981395&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to statistical thinking</title>
      <link>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</guid>
      <description>&lt;p&gt;Before we begin working on the specifics of our data project, it is important to have a clear understanding of some of the basic concepts that need to be in place to guide our work. In this post I will cover some of these topics including the importance of identifying a research question, how different statistical approaches relate to different types of research, and understanding data from a sampling and organizational standpoint. I will also provide some examples of linking research questions with variables in a toy dataset as we begin to discuss how to approach data analysis, primarily through visualization techniques.&lt;/p&gt;
&lt;div id=&#34;research-aims&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research aims&lt;/h2&gt;
&lt;!-- TODO: pepper in examples of the types of questions and methods one might find --&gt;
&lt;p&gt;Before jumping into the code, every researcher must come to a project with a clear idea about the purpose of the analysis. This means doing your homework in order to understand what it is exactly that you want to achieve; that is, you need to identify a &lt;strong&gt;research question&lt;/strong&gt;. The first step is become versed in the previous literature on the topic. What has been written? What are the main findings? Secondly, it is important to become familiar with the standard methods for approaching the topic of interest. How has the topic been approached methodologically? What are the types, sources, and quality of data employed? What have been the statistical approaches employed? What particular statistical tests have been chosen? Getting an overview not only of the domain-specific findings in the literature but also the methodological choices will help you identify promising plan for carrying out your research.&lt;/p&gt;
&lt;!-- All other steps in an analysis will be guided by this question and hinge on the choices you make conceptually to carry out a data analysis plan.  --&gt;
&lt;!-- * Identify a research question --&gt;
&lt;!--   - research previous literature --&gt;
&lt;!--   - get to know the nature of the phenomenon --&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-statistical-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing a statistical approach&lt;/h2&gt;
&lt;p&gt;With a research question in hand and a sense of how similar studies have approached the topic methodologically, it’s time to make a more refined decision about how the data is to be analyzed. This decision will dictate all other methodological choices from data collection to interpreting results.&lt;/p&gt;
&lt;p&gt;There are three main statistical approaches:&lt;/p&gt;
&lt;!-- * Identify goals of your analysis --&gt;
&lt;div id=&#34;inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;!-- TODO: find a way to introduce the concept &#39;model&#39;. this could make it easier to speak about statistical approaches, here and later in the post --&gt;
&lt;!-- TODO: pepper in examples of each type of statistical approach to ground these concepts a bit --&gt;
&lt;p&gt;Also commonly known as hypothesis testing or confirmation, statistical inference aims to establish whether there is a reliable and generalizable relationship given patterns in the data. The approach makes the starting assumption that there is no relationship, or that the null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) is true. A relationship is only reliable, or &lt;em&gt;significant&lt;/em&gt;, if the chance that the null hypothesis is false is less than some predetermined threshold; in which case we accept the alternative hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;). The standard threshold used in the Social Sciences, Linguistic included, is the famous p-value &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt;. Without digging into the deeper meaning of a p-value, in a nutshell a p-value is a confidence measure to suggest that the relationship you are investigating is robust and reliable given the data. In an inference approach all the data is used and is used &lt;em&gt;only&lt;/em&gt; once. This is not the case for the other two statistical approaches we will cover, Exploration and Prediction. For this reason it is vital to identify your statistical approach from the beginning. In the case of inference tests, failing to make a clear hypothesis often leads to p-hacking; a practice of running multiple tests and/or parameters on the same data (i.e. reusing the data) until evidence for the alternative hypothesis appears.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploration&lt;/h3&gt;
&lt;p&gt;One of two statistical learning approaches, this statistical method is used to uncover potential relationships in the data and gain new insight in an area where predictions and hypotheses cannot be clearly made. In statistical learning, exploration is a type of &lt;strong&gt;unsupervised learning&lt;/strong&gt;. Supervision here, and for Prediction, refers to the presence or absence of an outcome variable. By choosing exploration as our approach we make no assumptions (or hypotheses) about the relationships between any of the particular variables in the data. Rather we hope to investigate the extent to which we can induce meaningful patterns wherever they may lie. Findings from exploratory analyses can provide valuable insight for future study but they cannot be safely used to generalize to the larger population, which is why exploratory analyses are often known as hypothesis generating analyses (rather than hypothesis confirming). Given our generalizing power is curtailed, the data &lt;em&gt;can&lt;/em&gt; be reused multiple times trying out various tests. While it is not strictly required, data for exploratory analysis is often partitioned into two sets, training and validation, at roughly an 80%/20% split. The training set is used for refining statistical measures and the test set is used to evaluate the refined measures. Although the evaluation results still cannot be used to generalize, the insight can be taken as stronger evidence that there is a potential relationship, or set of relationships, worthy of further study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;The other statistical learning approach, Prediction, aims to uncover relationships in our data as they pertain to a particular outcome variable. This approach is known as &lt;strong&gt;supervised learning&lt;/strong&gt;. Similar to Exploration in many ways, this approach also makes no assumptions about the potential relationships between variables in our data and the data can be used multiple times to refine our statistical tests in order to tease out the most effective method for our goals. Where an exploratory analysis aims to uncover meaningful patterns of any sort, prediction, however, is more focused in that the main aim is to ascertain the extent to which the variables in the data pattern, individually or together, in such a way to make reliable associations to a particular outcome variable in unseen data. To evaluate the robustness of a prediction model the data is partitioned into training and validation sets. Depending on the application and the amount of available data, a third ‘development’ set is sometimes created as a pseudo test set to facilitate the testing of multiple approaches before the final evaluation. The proportions vary, but it a good rule of thumb is to reserve 60% of the data for training, 20% for development, and 20% for validation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;understanding-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Understanding data&lt;/h2&gt;
&lt;p&gt;Knowing the statistical approach to take then frames the next conceptual steps: &lt;strong&gt;data sampling&lt;/strong&gt; and &lt;strong&gt;organization of data&lt;/strong&gt;. But what is data anyway? Abstractly it is some set of empirical observations about the world. There are innumerable types of observations, as you can imagine, which can be used to describe objects and events. Our scientific aim is to systematically attempt to relate these observations and deduce the nature of their relationships to gain a better understanding of how our world works.&lt;/p&gt;
&lt;p&gt;Language research aims to understand a subset of these observations, namely those that concern linguistic behavior. The psycholinguist may observe the reaction times in a lexical decision task, eye-gaze in a visual world paradigm, or electro-magnetic brain activation in an ERP study. A sociolinguist may conduct interviews with members of a community, solicit language attitude responses to a language attitude survey, or ethnographically record face-to-face encounters. A syntactician may solicit acceptability ratings, calculate the frequency of a syntactic structure in a corpus, or document the permutations of subject-verb-object order in the world’s languages. As language is a defining characteristic of our species, language-related observations feature many other disciplines as well such as Anthropology, History, Neurology, Mathematics, and Biology. Linguistic inquiry, then, is not isolated to linguistic form, but rather the connection between linguistic form and other non-linguistic objects and events in the world at large –wherever that may take us.&lt;/p&gt;
&lt;!-- * Emperical observations --&gt;
&lt;!--   - Data is empirical observations about the world --&gt;
&lt;!--   - Innumberable types of observations: linguistic and non-linguistic --&gt;
&lt;!--   - Examples (acceptability ratings, reaction times, words in a corpus, lengths of words, age, sex, occupation, (non)-native speaker, etc.) --&gt;
&lt;div id=&#34;sampling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling&lt;/h3&gt;
&lt;p&gt;One major limitation inherent to most data sampling, and a primary reason why statistics are so important to doing and interpreting science, is the fact that our vantage point to the observing the world is restricted. We can only work with the data at our disposal, a &lt;strong&gt;sample&lt;/strong&gt;, even when it is clear that there is a much larger existing world, or &lt;strong&gt;population&lt;/strong&gt;. Ideally we would have access to the entire population of interest, but in most cases this is either not physically possible to obtain (or even store) the data or it is conceptually impossible to ever observe the entire population. As an example, say we wanted to catalog all the words in the English language. From a logistics point of view, where would we start? Any given dictionary only catalogs a subset of the words in a language –many words that are used in English-speaking communities, especially those from spoken language, will not appear. A corpus may capture linguistic diversity that does not appear in a dictionary, but it too will fall short of our lofty goal. But for argumentation sake, let’s imagine we could somehow capture all the words. What happens to our population in a day, a week, or a month from now? It quickly becomes a sample because new words are created all the time and some words are lost. Our population of words in the English language, then, is a moving target.&lt;/p&gt;
&lt;p&gt;This transitory property of populations is well-known and methods for obtaining reliable, or externally valid, samples is an area of study in its own right. In short, we aim for a sample to be balanced and representative of the idealized population. &lt;em&gt;Representativeness&lt;/em&gt; is the extent to which a sample reflects the total diversity in the population. &lt;em&gt;Balance&lt;/em&gt; is concerned with modeling the proportions of that diversity. An ideal sample combines both.&lt;/p&gt;
&lt;p&gt;The first strategy most often applied to obtaining a valid sample is to increase &lt;em&gt;sample size&lt;/em&gt;. This is an intuitive technique whose logic appeals to the notion that more is better. More is better, clearly. But more data alone does not always ensure an externally valid sample. For example, say we want to know something about the frequencies of words in written Spanish. Our target population is, then, words written in Spanish. It occurs to us that we can access a lot of written Spanish online via &lt;a href=&#34;http://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt;. We download works from many authors over a span of many years. After doing some calculations our sample contains around 100 million words. That’s a lot of words, and surely more indicative of the population than say 100 thousand words. But we have potentially overlooked something very important: all of the data in our sample comes from literature, specifically literature in the public domain. In other words, our sample is not random.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;random sample&lt;/em&gt; will help increase the potential diversity in any sample. In our sample this means drawing data from a number of written sources of Spanish at random. This strategy will increase our chances to capture written Spanish from other genres and registers. Now a 100 million word sample randomly selected from genres and registers of written Spanish is bound to be more representative of the population, but we run into another conceptual snag. Our sample is large and randomly selected from the population, but does it reflect the proportion each subgroup (genres and registers) contributes to the idealized population?&lt;/p&gt;
&lt;p&gt;There is no absolute way of knowing if the proportions of each subgroup are balanced, or even what all the subgroups may be for that matter, but in most cases we can make an educated guess on both these fronts that will allow us to increase the validity of our sample. For example, the literary genre ‘self-help’ intuitively constitutes a smaller portion of our target population than say ‘news’. Ideally we would want to reflect this understanding in our sample. Applying this logic is known as &lt;em&gt;stratified sampling&lt;/em&gt;. A large, stratified random sample is always at least as valid as an equally sized large random sample with the added benefit that we are safeguarded from large skews that a large random sample may potentially produce. Now it is important to keep in mind that stratified sampling has its limitations as well. The difficulties posed in obtaining a valid sample from the macro view (i.e. the total population is never observable) are present at the micro view as well (i.e. sub- and sub-substrata are equally illusive). Again, there are no absolutes in sampling. The key is to keep the aim of the research question clear during the sampling process and strive for sizable, randomly stratified samples to minimize sampling error to the extent that it is feasible –and then work from there.&lt;/p&gt;
&lt;p&gt;This lack of certainty in sampling may seem troublesome. Sampling uncertainty, however, does not mean we cannot gain insight into the essence of the objects and events in the world we aim to understand. It just means we need to be aware of any given sample’s limitations, document these limitations, and always approach statistical findings based on this data with caution; suspending generalizations of the absolute nature. This is why science, contrary to popular belief, does not ‘prove’ anything. Rather science aims to collect evidence for or against a hypotheses. Since the data is always changing there are no absolute conclusions. As the evidence grows, so does the case for a particular view of how the world works. It is this systematic approach which makes science so powerful.&lt;/p&gt;
&lt;!-- * Populations and samples --&gt;
&lt;!--   - Ideally we would have access to the entire record of those observation that are of potential interest to us in our analysis --&gt;
&lt;!--   - Reality is we, more often than not, cannot feasibly acquire the observations of an entire population. --&gt;
&lt;!--   - The best we can do is shoot for a sample of that population which aims to model the population as appropriately possible given the particular research question to be addressed --&gt;
&lt;!--   - In the end we will always strive for a balanced and representative sample. A balanced sample contains the predicted major features indicative of the population. A representative sample contains these features in a way that reflect the proportion of these features.  --&gt;
&lt;!--   - Attaining the most ideal sample given the potential limitations is key to gaining robust and generalizable insight into the nature of the phenomenon we are investigating.  --&gt;
&lt;/div&gt;
&lt;div id=&#34;organization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Organization&lt;/h3&gt;
&lt;p&gt;Identifying and capturing a data sample moves us one step closer to performing our data analysis but the format of the raw or original data is often not in a format conducive for visualization nor statistical tests. The hypothetical written Spanish data we identified to sample in the previous section would most likely take the form of documents of running text with potentially some meta-data about the text (author, title of the work, date published, genre, etc.) in the header of the file and/or the name of each file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Title: Cuando los robots tomen el mando y hagan la guerra
Date: 3 AGO 2015 - 00:00 CEST
Genre: News
Source: El País
Tags: Científicos, Isaac Asimov, Robótica, Gente, Tecnología, Informática, Ciencia, Sociedad, Industria

La primera reflexión abarcadora sobre la coexistencia entre los robots y los humanos no fue obra de un científico de la computación ni de un filósofo ético, sino de un novelista. Isaac Asimov formuló las tres “leyes de la robótica” que deberían incorporarse en la programación de cualquier autómata lo bastante avanzado como para suponer un peligro: “No dañar a los humanos, obedecerles salvo conflicto con lo anterior y autoprotegerse salvo conflicto con todo lo anterior”. Las tres leyes de Asimov configuran una propuesta sólida y autoconsistente, y cuentan con apoyo entre la comunidad de la inteligencia artificial, que reconoce, por ejemplo, que cualquier sistema autónomo funcional debe ser capaz de autoprotegerse.
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As raw data this format is fine, but to gain insight from this data, we will need to explicitly organize the attributes of our data that are key to our analysis. Our data should be in tabular, or &lt;a href=&#34;www.jstatsoft.org/v59/i10/paper&#34;&gt;‘tidy’ format&lt;/a&gt; where each row is an observation, or &lt;strong&gt;case&lt;/strong&gt; and each column, or &lt;strong&gt;variable&lt;/strong&gt; is a list of attributes of the observation. Each cell, then, is a particular attribute of a particular observation, or &lt;strong&gt;data point&lt;/strong&gt;. Say our objective is to perform an exploratory analysis to evaluate the potential similarities and differences in word frequencies between genres. For this particular analysis we will want to extract and organize the title of each document (&lt;code&gt;doc_id&lt;/code&gt;), the genre it is from (&lt;code&gt;genre&lt;/code&gt;), and each word (&lt;code&gt;word&lt;/code&gt;) as a single, row, or observation, in our tidy dataset.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:spanish-works&#34;&gt;Table 1: &lt;/span&gt;Tidy dataset&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;doc_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genre&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;es&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;son&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;peligro&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;guerra&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;por&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;han&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;á&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;considerado&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;celoso&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;guide&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;esperan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;teme&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;el&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This tidy organization may seem somewhat redundant; a single value for &lt;code&gt;doc_id&lt;/code&gt; is repeated for each value of &lt;code&gt;word&lt;/code&gt; and a single value of &lt;code&gt;genre&lt;/code&gt; is repeated for each value of &lt;code&gt;doc_id&lt;/code&gt;. However tidy data, although visually redundant, is an explicit description of the relationship between our variables. Each row corresponds to all of the necessary attributes to describe a particular observation. In this data, the occurrence of a word is associated with the file it appears in and the genre that file is associated with.&lt;/p&gt;
&lt;!-- TODO: consider the format that is required for doing a cluster analysis. Is my discussion leading in that direction, or is it complicated by potentially needing to create a term-document matrix? Look at the `tidytext` package vignette. Might need to change the discussion to focus on an inference analysis: assume a hypothesis that a specific register is more lexically diverse than another. Include genre as a factor? --&gt;
&lt;p&gt;Our objective in this toy example is to explore the relationship between word frequencies and genres, yet at this point there is no explicit variable for the frequencies of words. The information we need, however, is in the data and since we have an organized, tidy dataset, calculating &lt;code&gt;word_freq&lt;/code&gt; is a matter of tabulating the occurrences of each word. This can be done easily with R, as we will see in detail in future posts, but for our discussion on data organization let’s skip the details and jump to the new dataset with a column for &lt;code&gt;word_freq&lt;/code&gt;.&lt;/p&gt;
&lt;!-- example tabular data: doc_id, genre, word. Adding the word_freq column --&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:spanish-works-freq&#34;&gt;Table 2: &lt;/span&gt;Tidy dataset with &lt;code&gt;word_freq&lt;/code&gt; variable.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;doc_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genre&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;word_freq&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;la&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;559&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;el&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;376&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;en&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;309&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;la&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;y&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;los&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;to&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;314&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;217&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;_m&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;172&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Other measures and/or attributes can be added as necessary to this tabular format and in some cases we may convert our tidy tabular dataset to other data formats that may be required for some particular statistic approaches but at all times the relationship between the variables should be maintained in line with our research purpose. We will touch on examples of other types of data formats when we dive into particular statistical approaches that require them later in the series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;informational-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Informational value&lt;/h3&gt;
&lt;p&gt;Let’s turn now to the informational nature of our variables as it will set up how we implement our data analysis. Taking our variable &lt;code&gt;word_freq&lt;/code&gt; as an example, it is important to point out there are many ways to define ‘frequency’. Some frequency measures are more appropriate than others given the statistical approach we intend to apply to our data. Our current dataset contains raw frequency scores, that is the frequency is measured in observed counts for each word in each file of our data. We could, for example, instead bin our frequency scores under the labels “high” and “low” frequency converting frequency from counts to labels. In this case we change the &lt;strong&gt;informational value&lt;/strong&gt; of &lt;code&gt;word_freq&lt;/code&gt;. Some variables in our dataset, on the other hand, cannot be converted. Take for example, &lt;code&gt;genre&lt;/code&gt;. The values for &lt;code&gt;genre&lt;/code&gt; label the genre of the file from which the word was observed. We could of course summarize the genres under meta-genres, but we maintain labeled data; the same informational value as before.&lt;/p&gt;
&lt;p&gt;Understanding the informational value of variables in key to organizing and preparing your data for analysis as it has implications for what insight we can gain from the data and what visualization techniques and statistical measures we can use to interrogate the data. There are four potential informational values for all data: nominal, ordinal, interval, and ratio.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Nominal variables&lt;/em&gt; contain attributes which are labels denoting the membership in a class in which there is no relationship between the labels. Examples of nominal data include part-of-speech labels, the sex of a participant, the genre of a text, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Ordinal variables&lt;/em&gt; also contain labels of classes, but in contrast to nominal variables, there is a relationship between the classes, namely one in which there is a precedence relationship or rank. Our frequency conversion from scores to high- and low-frequency bins is a type of ordinal data –there is an explicit ordering of these two categories. Grouping participants in a study as “young”, “middle-aged”, and “old” would also be ordinal values; again, each value can be interpreted in relationship to the other values.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Interval variables&lt;/em&gt; are like ordinal variables in which there is an explicit precedence relationship, but in addition the values describe precise intervals between each value. So take our earlier operationalization of age as “young”, “middle-aged”, and “old”. As an ordinal variable no assumption is made that the differences in age between young and middle-aged are the same as between middle-aged and old –only that one class is ordered before or after another. If our criterion to code our values of age, however, were based regular intervals between age groups, not some non-regular assignment, then our values of age would be interval-valued.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Ratio variables&lt;/em&gt; have all the properties of interval variables but also include a non-arbitrary definition of zero. Frequency counts are ratio variables as it is clear that there is a potential value for 0 and any value greater can be interpreted in reference to this anchor. A word with a frequency of 100 is two times as large as a word with frequency 50. By the same token, a participant that is 20 years old is half the age of a 40 year old participant.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These informational types are often described in macro terms grouping nominal and ordinal variables as &lt;strong&gt;categorical variables&lt;/strong&gt; and interval and ratio variables as &lt;strong&gt;continous variables&lt;/strong&gt;. All continuous variables can be converted to categorical variables, but the reverse is not true. In most cases it is preferred to cast your data as continuous, if the nature of the variable permits it, as the recasting of continuous data to categorical data results in a loss of information –which will result in a loss of statistical power and may lead to results that obscure meaningful patterns in the data.&lt;/p&gt;
&lt;!-- The structure of raw, or original, data sampled varies  --&gt;
&lt;!-- * Informational value of variables --&gt;
&lt;!--   - In preparation for analysis, the research must be cognicent of the informational status of the variables of the data --&gt;
&lt;!--   - Variables are the observational features, attributes, and measures --&gt;
&lt;!--   - Ultimately visualizing and performing statistical operations on your data to analyze your data will depend on these variables and the information they contain. --&gt;
&lt;!--   - There are four informational types of variables: --&gt;
&lt;!--     * Nominal --&gt;
&lt;!--     * Ordinal --&gt;
&lt;!--     * Ratio --&gt;
&lt;!--     * Interval --&gt;
&lt;!--   - For convenience, we often talk about two major classes --&gt;
&lt;!--     * Categorical (nominal/ ordinal) --&gt;
&lt;!--     * Continuous (ratio/ interval) --&gt;
&lt;!--   - (Examples of real data sets) --&gt;
&lt;!-- * Operationalizing variables --&gt;
&lt;!--   - While some variables and their informational value are self-evident (such as sex, or age), some need to be interpreted and explicitly formulated --&gt;
&lt;!--   - Measurements --&gt;
&lt;!--     * Some variables contain information that needs to be calculated (frequencies, dispersion, etc.) or transformed (log, normalization (z-scores, etc.)) to be meaningful for analysis --&gt;
&lt;/div&gt;
&lt;div id=&#34;dependent-and-independent-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dependent and independent variables&lt;/h3&gt;
&lt;p&gt;The last step before we move to visualization and statistical tests is to identify our &lt;strong&gt;dependent variable&lt;/strong&gt; and/ or &lt;strong&gt;independent variables&lt;/strong&gt;. A dependent variable is the outcome variable that is used in inference and prediction analyses that reflects the observations of the behavior we want to gain understanding about. The identification of a dependent variable should be guided by your research question; it is the measure of the phenomenon in question. An independent variable is a predictor variable, or a variable which we assume will be related to the values of the dependent variable in some systematic way. There is typically only one dependent variable in an analysis, but there can be multiple independent variables. In an exploratory analysis, however, all the variables are independent variables as this approach assumes no particular relationship between the variables; the goal in this approach, remember, is to uncover patterns that may suggest a relationship between particular set of variables.&lt;/p&gt;
&lt;!-- * Dependent and independent variables --&gt;
&lt;!--   - Another key distiction to be made with our variables concerns the linking of our variables with the research question and statistical approach we intend to use to explore this question and it&#39;s predictions.  --&gt;
&lt;!--   - A dependent variable is the variable that is considered the outcome measurement to be understood.  --&gt;
&lt;!--   - Independent variables are those variables that we aim to use to understand the nature and variability of the dependent variable --&gt;
&lt;!--   - In hypothesis testing or prediction a dependent variable must be identified (in prediction this variable is also called the class or outcome variable) --&gt;
&lt;!--   - For exploration, where the goal is to discover patterns, the dependent variable is unknown and is the feature that we aim to discover --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data analysis&lt;/h2&gt;
&lt;p&gt;The primary goal of a data analysis is to reduce the observed data to a human-interpretable summary that best approximates the nature of the phenomenon we are investigating. With well-sampled data in a tidy dataset in hand where observations and variables are explicitly related, identified for their informational value, and the dependent and/or independent variables are clear, we can now proceed to visualizing and applying the appropriate statistical tests to the data to come to some more concrete, actionable insight.&lt;/p&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization&lt;/h3&gt;
&lt;p&gt;It is always key to gain insight into the behavior of the data visually before jumping in to the statistical analysis. Using our research aim as our guide, we will choose the most appropriate visualization to use given the number and informational value of our target variables. To get a sense of how this looks, let’s work with an example dataset and pose different questions to the data with an eye towards seeing how various combinations of variables are visualized.&lt;/p&gt;
&lt;p&gt;The dataset we will use here is from the &lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank repository&lt;/a&gt; which provides data from various language learning contexts.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The specific data we will use is the ‘narratives’ section of the &lt;a href=&#34;http://talkbank.org/access/SLABank/English/BELC.html&#34;&gt;BELC (Barcelona English Language Corpus)&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Muñoz 2006)&lt;/span&gt;. It is a corpus of writing samples from second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of “Me: my past, present and future”. Data was collected for many (but not all) participants up to four times over the course of seven years. The entire dataset includes 123 observations from 54 participants. Below I’ve included the first 10 observations from the dataset which reflects some data cleaning I’ve done so we start with a tidy dataset.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:view-belc-dataset&#34;&gt;Table 3: &lt;/span&gt;BELC dataset for demonstration.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;participant_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;learner_group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tokens&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The variables &lt;code&gt;participant_id&lt;/code&gt;, &lt;code&gt;sex&lt;/code&gt;, and &lt;code&gt;age&lt;/code&gt; should be self-explanatory. &lt;code&gt;learner_group&lt;/code&gt; contains the values 1-4 which record the stage for each participant formally learning English. The number of words written in each sample is listed for each participant at each stage in the variable &lt;code&gt;tokens&lt;/code&gt;. We should also note the informational value of these variables. &lt;code&gt;participant_id&lt;/code&gt;, &lt;code&gt;sex&lt;/code&gt;, and &lt;code&gt;learner_group&lt;/code&gt; are categorical variables; both &lt;code&gt;participant_id&lt;/code&gt; and &lt;code&gt;sex&lt;/code&gt; are nominal and &lt;code&gt;learner_group&lt;/code&gt; is ordinal. &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; are continuous variables; both of the ratio type as they are scaled in relation to a non-arbitrary value for zero.&lt;/p&gt;
&lt;p&gt;With general understanding of the data, let’s run through various data analysis scenarios and their corresponding visualizations grouping them by the information value of the dependent variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Categorical dependent variable&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Starting basic, let’s say we are interested in investigating the difference in the number of &lt;code&gt;males&lt;/code&gt; and &lt;code&gt;females&lt;/code&gt; in our study. This is not a particularly interesting question, but it allow us to illustrate a scenario in which we have a single dependent variable, &lt;code&gt;sex&lt;/code&gt;, which is categorical. When summarizing categorical data we produce counts of each of the levels of that variable. We can visualize this summary in one of two ways, textually and graphically. A text summary would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sex
female   male 
    67     56 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A graphic display does not necessarily facilitate a better understanding, in such a simple case, but let’s graphically visualize this scenario anyway. The type of plot we want to use is a ‘bar plot’, which simply plots the dependent variable on the x-axis and the counts on the y-axis.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-1.png&#34; alt=&#34;Bar plot of the categorical variable `sex`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Bar plot of the categorical variable &lt;code&gt;sex&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Inspecting these visualizations it is clear that there is a numeric difference between the number of writing samples in the data written by women. At this point, however, we only have a trend. To decide whether this is a reliable contrast is the purpose of our statistical tests, but we’ll leave the details of statistical testing for this scenario, and those that follow, for subsequent posts.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One categorical independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A more common scenario is one in which we have a categorical dependent variable and a categorical independent variable. With our data we can investigate the relationship between &lt;code&gt;sex&lt;/code&gt; and the &lt;code&gt;learner_group&lt;/code&gt;. Are there more males than females in a particular learner group? In this case both variables are categorical and the dimensions are such that we can textually represent them and gain some insight.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;        learner_group
sex       1  2  3  4
  female 20 24 13 10
  male   15 23 13  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s more difficult to see the pattern here than in the basic single dependent variable scenario for two reasons: 1) as the number of independent variables and/or the levels within an independent variable increase, our ability to interpret the results decreases. 2) the relationship between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt; does not take into account that there are more female samples than males, and therefore the raw counts here can be misleading.&lt;/p&gt;
&lt;p&gt;A graphic representation of this contrast will be a bit easier to interpret; although it is important to be aware that more variables and levels always leads to interpretability problems. The bar plot below reflects the raw counts from the cross-tabulation of the variables &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-graph-1.png&#34; alt=&#34;Bar plot of the variable `sex` and `learner_group`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Bar plot of the variable &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Adjusting the bar plot to account for the proportions of males to females in each group provides a clearer picture of the relationship between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-graph-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-graph-2-1.png&#34; alt=&#34;Bar plot of the variable `sex` and `learner_group` proportionally scaled.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Bar plot of the variable &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt; proportionally scaled.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From this visualization it appears that there are more females in the first and last learner groups.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Two categorical independent variables&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s look at a more complex case in which we have two categorical independent variables. Now the dataset, as is, does not have a third categorical variable for us to explore but we can recast the continuous &lt;code&gt;tokens&lt;/code&gt; variable as a categorical variable if we bin the scores into groups. I’ve binned &lt;code&gt;tokens&lt;/code&gt; into three score groups with equal ranges in a new variable called &lt;code&gt;token_bins&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:token-bins&#34;&gt;Table 4: &lt;/span&gt;&lt;code&gt;belc&lt;/code&gt; dataset with categorical &lt;code&gt;token_bins&lt;/code&gt; variable added.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;participant_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;learner_group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tokens&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;token_bins&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Adding a second categorical independent variable ups the complexity of our analysis and as a result our visualization strategy will change. As text our data will include individual two-way cross-tabulations for each of the levels for the third variable. In this case it is often best to use the variable with the fewest levels as the third variable.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;, , sex = female

             token_bins
learner_group low mid high
            1  18   2    0
            2  18   6    0
            3   9   4    0
            4   7   2    1

, , sex = male

             token_bins
learner_group low mid high
            1  13   2    0
            2  15   7    1
            3   8   5    0
            4   2   3    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To graphically visualize three categorical variables we turn to a mosaic plot.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-2-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-2-graph-1.png&#34; alt=&#34;Mosaic plot contrasting the categorical variables `sex`, `learner_group`, and `token_bins`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Mosaic plot contrasting the categorical variables &lt;code&gt;sex&lt;/code&gt;, &lt;code&gt;learner_group&lt;/code&gt;, and &lt;code&gt;token_bins&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From these visualizations we can see there is a general trend for the tokens from writing samples to increase in higher learner groups. There are some apparent divergent scores from this trend to be cautious of, however.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continuous dependent variable&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Working with a single continuous dependent variable means that the only practical way to summarize the data is graphically –as textual visualization will be very verbose and by and large uninterpretable. Plotting a single continuous variable often takes the form of a histogram which summarizes the frequency of the values of the dependent variable. So from our dataset, we may want to know what the distribution of &lt;code&gt;token&lt;/code&gt; scores looks like. That is, are they normally distributed (‘bell-shaped’), or skewed to the left or right (more values in the low or high range), or some other type of distribution (i.e. bi-modal, etc.)?&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-graph-1.png&#34; alt=&#34;Histogram (with and without a density line) for the continuous variable `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Histogram (with and without a density line) for the continuous variable &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The plot on the left is a standard histogram and the plot on the right is the same histogram with a density line added to highlight the distribution. From these plots we see that token counts are slightly left skewed. The longer tail to the right for higher token scores shows some evidence of outliers –that is, scores that are uncharacteristic of the general data distribution. For many analyses plotting a histogram is a key first step to identifying they type of statistical test to use on the data as certain test have assumptions about how the data should be distributed for their results to be reliable. For example, a class of tests called &lt;em&gt;parametric&lt;/em&gt; assume that continuous data is normally distributed (&lt;em&gt;non-parametric&lt;/em&gt; tests do not make this assumption). Having plotted the data we can see that it is probably not normally distributed. All is not lost, however. There are methods for &lt;em&gt;transforming&lt;/em&gt; the data that can often times take mildly skewed data and coerce it into a normal distribution.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One categorical independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another common data analysis scenario is one in which we have a continuous dependent variable and one categorical independent variable. Say we wanted to know the average number of &lt;code&gt;tokens&lt;/code&gt; used by men and by women. We would use the &lt;code&gt;tokens&lt;/code&gt; variable as our dependent variable and and &lt;code&gt;sex&lt;/code&gt; as the independent variable. We can visualize these means textually, as we are calculating the mean for each level of &lt;code&gt;sex&lt;/code&gt;,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  female     male 
57.14925 60.80357 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or graphically with a box plot, in which we get a host of information about the distribution.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-cat-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-cat-graph-1.png&#34; alt=&#34;Box plot summarizing the contrast between `sex` and `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Box plot summarizing the contrast between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you take a look at the results in the box plot you will see that the medians (the &lt;strong&gt;bold horizontal lines&lt;/strong&gt;) are not that different. Note that the mean and the median measure different things. And the mean is more sensitive to outliers –and the plot shows that there are some outliers in the male and female data. When we statistically analyze the data these types of outliers contribute to unexplained variation, or ‘noise’ at it is often called. Noise has the effect of reducing our confidence that the differences between populations are real, and not likely due to chance. We see much more on this later in the series.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One continuous independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The behavior of two continuous variables, one dependent and the other independent is represented using a scatter plot. The value for each variable for each observation is plotted as a coordinate pair. From this mapping we evaluate the relationship, or correlation, between the variables. Sticking with &lt;code&gt;tokens&lt;/code&gt; as our measure, let’s explore the extent to which &lt;code&gt;age&lt;/code&gt; conditions the number of tokens used by a participant.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From a visual inspection it appears that there is a slight effect for &lt;code&gt;age&lt;/code&gt; on &lt;code&gt;tokens&lt;/code&gt;, namely that the older the participant the more tokens they tend to use.&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; It is often helpful, and/ or necessary to add a trend line to the plot to help see the relationship more clearly. Note that the ribbon (in grey) surrounding the trend line is the ‘standard error’, or SE. The SE is a confidence interval suggesting that the trend line could have been drawn within any part of this space. You will note that a larger ribbon width corresponds with more variability. This is clearly the case for the tokens used by participants at age 14.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-2-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` with a trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; with a trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Another note on the trend line. The trend line drawn here is a non-linear, which is why you see the line is bendy. Often times when we are doing hypothesis testing we will be making the assumption that the relationship between two or more variables is linear, not non-linear. We can add a linear trend line to get a better understanding of the linear relationship between our variables.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-3-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` with a linear trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; with a linear trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interpreting the linear representation we see there is an apparent trend for increasing values for &lt;code&gt;tokens&lt;/code&gt; as &lt;code&gt;age&lt;/code&gt; increases.&lt;/p&gt;
&lt;p&gt;Say our aim was to understand the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; as a potential function of &lt;code&gt;sex&lt;/code&gt;. We can incorporate &lt;code&gt;age&lt;/code&gt; as a categorical variable. The result provides us a scatter plot with two trend lines, one for each level of &lt;code&gt;sex&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-4-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` by `sex` with a linear trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; by &lt;code&gt;sex&lt;/code&gt; with a linear trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From this visualization we see there is an apparent difference between males and females. Namely, males appear to increase their token production more than females of the same age. The SE ribbons here, however, are telling. Since they overlap we should be very cautious in interpreting the difference the trend line shows. Overlapping SE ribbons suggest the trend line could have been drawn in this space and therefore there is a likely probability that the visual difference will not result in a statistical difference. Again, this is another example of why visualization is such an integral first step in data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-tests&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Statistical tests&lt;/h3&gt;
&lt;p&gt;In the previous section various visualization strategies were illustrated through the lens of typical data analysis scenarios. To gain confidence that the trends in the data that we observe are reliable we submit the data to statistical tests. There are numerous tests available, too many to discuss at this point. But it is important to understand that much like our visualization choices, the test we choose depends on the number of variables we are investigating and the information values of these variables. However, particular statistical tests also potentially require a number of test specific assumptions (such as whether the data is normally distributed, for example). We will cover these on a case by case basis later on in the series.&lt;/p&gt;
&lt;!-- * Goal is to reduce the data into a summary of the data that best approximates the nature of the phenomenon that we are investigating.  --&gt;
&lt;!--   - To do this we start with visualization to gain an intuitive understanding of the distribution of the data and variables of interest. This exploratory process should be guided by your research question and statistical aims (hypothesis in hypothesis testing). It is fundamental to build up this graphic and mental picture of our data before moving to statistically evaluate the relationships of interest. --&gt;
&lt;!--   - (Examples) --&gt;
&lt;!--   - With an intuition of the data, awareness of the informational values of our variables, and having identified dependent and independent variables, we proceed to statistical analysis to develop a model that achieves these goals. Ultimately we want to know if the nature of the connections between our variables are reliable. That&#39;s what statistics will do for us. --&gt;
&lt;!--   - (Examples) --&gt;
&lt;!-- Keep these examples brief, as we will deal with data analysis head on later in the series.  --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we covered some foundational topics for any data analysis project. Guiding the entire process is a clear research question. From there we can proceed to acquire data that is relevant and reliable on the phenomenon we hope to understand better, choose a statistical approach which matches our analysis goals, and organize our data into a format conducive for achieving these goals. Only at this point can we confidently move to analyzing our data, first beginning with appropriate visualization techniques to get a feel for the trends and then moving to performing the relevant statistical tests to provide confidence that the trends captured in our visualizations are in fact reliable.&lt;/p&gt;
&lt;p&gt;There is still much left to discuss, in particular what statistical tests to apply to a given data analysis scenario and the assumptions behind statistical tests. We will address these topics later in the series when we have established a stronger and practical understanding of the preceding project steps and increase our proficiency programming in R. The next steps in this journey will be to learn about data types and sources and the specifics of acquiring data for language research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- OTHER APPROACHES TO THE TOPIC

GRIES, Statistics for Linguists with R
CHAPTER 1

* Design and logic of quantitative studies
  - Linguistics is an empirical science; thus understanding empirical methods and statistics is key to undertanding scientific and linguistic argumentation
  - Choosing a topic -consult the literature and get a feel for the area
* Reseach aims 
  - Hypotheses
  - Exploratory analysis
  - (Prediction -not covered but would logically fall here)
* Variables and measures
  - Operationalizing variables
  - Informational value (level of measurement)
  - Dependent and independent variables
    * Measure of dependent variable
    * Number of independent variables
* Data
  - Populations and samples (aiming for a balanced, representative sample)
  - Format for analysis (tidy, basically)


WICKHAM, R for Data Science
EXPLORE

* Data science workflow (overview)
* Data visualization (ggplot2, grammar of graphics fundamentals)
* Exploratory data analysis (EDA)
  - Variation
  - Covariation
  - Patterns and models (modelling concepts)

MODEL

* Hypothesis generating (exploration) vs. hypothesis confirmation (testing/ inference)
  - Observations may be used only once for inference, but multiple times for exploration
  - Modelling: creating a low-dimension summary of the dataset
  - Choosing a model (model selection)

JOHNSON, Quantitative Methods in Linguistics

Chapter 1:

* Goals of quantitative analyses
  - Data reduction
  - Inference (confirm a hypothesized relationship)
  - Exploration (discover relationships)
  - Explore processes (? not sure I like this; I would add Prediction (leverage/ harness relationships for predicting events) )
* Observations
  - What is an observation?
  - Informational values of observations (variables)
* Measures
  - Frequency distributions
  - Shapes of distributions
    * Uniform, skewed (right or left), bimodal, normal, J-shaped, U-shaped
  - Importance of the normal distribution
    * What is the normal distribution and why is it important?
      * Assumption for many statistical tests
    * How do we know if the data is normal?
    * Transforming data to fit the normal distribution
  - Measures of central tendency
    * Mean (arithmatic), median, and mode
  - Measure of dispersion
    * Variance
    * Standard deviation

Chapter 2:

* Sampling
  - What makes a good sample?
* Data
  - Observations
  - Good samples are critical to a sound analysis (your findings cannot be more reliable than your data!)
* Hypothesis testing
  - Central Limit Theorem
  - Null and alternative hypotheses
  - Type I and Type II error
  - Correlation
    
BAAYEN, Analyzing linguistic data: A practical introduction to statistics using R

* Not (obviously) a good source for intro statistical thinking

STANTON, An Introduction to Data Science

Data Science: Many Skills
* 

GRIES, Quantitative Corpus Linguistics with R

Chapter 5: Some statistics for Corpus Linguistics

* Intro to statistical thinking
  - Variables and their roles in an analysis
  - Variables and their informational value
  - Hypotheses: formulation and operationalization
  - Data analysis
* Categorical dependent variables
  - 1 DV, no IV
  - 1 DV, 1 IV (categorical)
  - 1 DV, 2+ IV (categorical)
* Interval/ ratio-scaled dependent variables
  - 1 DV, no IV
  - 1 DV, 1 IV (categorical)
  - 1 DV, 1 IV (interval/ratio)
  - 1 DV, 2+ IV (mixed)

--&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Gries2013a&#34;&gt;
&lt;p&gt;Gries, ST. 2013. &lt;em&gt;Statistics for Linguistics with R. A Practical Introduction&lt;/em&gt;. 2nd revise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Johnson:2008&#34;&gt;
&lt;p&gt;Johnson, K. 2008. &lt;em&gt;Quantitative methods in linguistics&lt;/em&gt;. Blackwell Pub.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Munoz2006&#34;&gt;
&lt;p&gt;Muñoz, Carme, ed. 2006. &lt;em&gt;Age and the rate of foreign language learning&lt;/em&gt;. Clevedon: Multilingual Matters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wickham2017&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Garrett Grolemund. 2017. &lt;em&gt;R for Data Science&lt;/em&gt;. First edit. O’Reilly Media. &lt;a href=&#34;http://r4ds.had.co.nz/&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The &lt;a href=&#34;http://talkbank.org/&#34;&gt;HomeBank&lt;/a&gt; section of TalkBank is restricted to members only. Membership is free but you will need to contact the repository and request membership.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that I’ve added a ‘jitter’ to the data points in this scatter plot to avoid overplotting.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
