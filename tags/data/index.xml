<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data on francojc ⟲</title>
    <link>https://francojc.github.io/tags/data/</link>
    <description>Recent content in Data on francojc ⟲</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jerid Francom</copyright>
    <lastBuildDate>Mon, 23 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/data/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Acquiring data for language research (2/3): package interfaces</title>
      <link>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;!-- TODO:
--&gt;
&lt;div id=&#34;package-interfaces&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package interfaces&lt;/h2&gt;
&lt;p&gt;A convenient alternative method for acquiring data in R is through package interfaces to web services. These interfaces are built using R code to make connections with resources on the web through &lt;strong&gt;Automatic Programming Interfaces&lt;/strong&gt; (APIs). Websites such as Project Gutenberg, Twitter, Facebook, and many others provide APIs to allow access to their data under certain conditions, some more limiting for data collection than others. Programmers (like you!) in the R community take up the task of wrapping calls to an API with R code to make accessing that data from R possible. For example, &lt;a href=&#34;https://CRAN.R-project.org/package=gutenbergr&#34;&gt;gutenbergr&lt;/a&gt; provides access to Project Gutenberg, &lt;a href=&#34;https://CRAN.R-project.org/package=rtweet&#34;&gt;rtweet&lt;/a&gt; to Twitter, and &lt;a href=&#34;https://CRAN.R-project.org/package=Rfacebook&#34;&gt;Rfacebook&lt;/a&gt; to Facebook.&lt;/p&gt;
&lt;p&gt;Using R package interfaces, however, often requires some more knowledge about R objects and functions. Let’s take a look at how to access data from Project Gutenberg through the &lt;code&gt;gutenbergr&lt;/code&gt; package. Along the way we will touch upon various functions and concepts that are key to working with the R data types vectors and data frames including filtering and writing tabular data to disk in plain-text format.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The following code is available on GitHub &lt;code&gt;recipes-acquiring_data&lt;/code&gt; and is built on the &lt;code&gt;recipes-project_template&lt;/code&gt; I have discussed in detail &lt;a href=&#34;https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/&#34;&gt;here&lt;/a&gt; and made accessible &lt;a href=&#34;https://github.com/francojc/recipes-project_template.git&#34;&gt;here&lt;/a&gt;. I encourage you to follow along by downloading the &lt;code&gt;recipes-project_template&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; from the Terminal or create a new RStudio R Project and select the “Version Control” option.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;To get started let’s install and load the package. The most simple method for downloading an R package in RStudio is to select the ‘Packages’ tab in the Files pane and click the ‘Install’ icon. To ensure that our code is reproducible, however, it is better to approach the installation of packages programmatically. If the package is not part of the R base library, we will not assume that the user will have the package on their system. The code to install and load the &lt;code&gt;gutenbergr&lt;/code&gt; package is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;gutenbergr&amp;quot;) # install `gutenbergr` package
library(gutenbergr) # load the `gutenbergr` package&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach works just fine, but luck has it that there is an R package for installing and loading packages! The &lt;a href=&#34;https://CRAN.R-project.org/package=pacman&#34;&gt;pacman&lt;/a&gt; package includes a set of functions for managing packages. A very useful one is &lt;code&gt;p_load()&lt;/code&gt; which will look for a package on a system, load it if it is found, and install and then load it if it is not found. This helps potentially avoid using unnecessary bandwidth to install packages that may already exist on a user’s system. But, to use &lt;code&gt;pacman&lt;/code&gt; we need to include the code to install and load it with the functions &lt;code&gt;install.packages()&lt;/code&gt; and &lt;code&gt;library()&lt;/code&gt;. I’ve included some code that will mimic the behavior of &lt;code&gt;p_load()&lt;/code&gt; for installing &lt;code&gt;pacman&lt;/code&gt; itself, but as you can see it is not elegant, luckily it’s only used once as we add it to the SETUP section of our master file, &lt;code&gt;_pipeline.R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load `pacman`. If not installed, install then load.
if (!require(&amp;quot;pacman&amp;quot;, character.only = TRUE)) {
  install.packages(&amp;quot;pacman&amp;quot;)
  library(&amp;quot;pacman&amp;quot;, character.only = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have &lt;code&gt;pacman&lt;/code&gt; installed and loaded into our R session, let’s use the &lt;code&gt;p_load()&lt;/code&gt; function to make sure to install/ load the two packages we will need for the upcoming tasks. If you are following along with the &lt;code&gt;recipes-project_template&lt;/code&gt;, add this code within the SETUP section of the &lt;code&gt;acquire_data.R&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Script-specific options or packages
pacman::p_load(tidyverse, gutenbergr)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Note that the arguments &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;gutenbergr&lt;/code&gt; are comma-separated but not quoted.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Project Gutenberg provides access to thousands of texts in the public domain. The &lt;code&gt;gutenbergr&lt;/code&gt; package contains a set of tables, or &lt;strong&gt;data frames&lt;/strong&gt; in R speak, that index the meta-data for these texts broken down by text (&lt;code&gt;gutenberg_metadata&lt;/code&gt;), author (&lt;code&gt;gutenberg_authors&lt;/code&gt;), and subject (&lt;code&gt;gutenberg_subjects&lt;/code&gt;). I’ll use the &lt;code&gt;glimpse()&lt;/code&gt; function loaded in the &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;tidyverse&lt;/a&gt; package&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; to summarize the structure of these data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_metadata) # summarize text meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 51,997
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ...
## $ title               &amp;lt;chr&amp;gt; NA, &amp;quot;The Declaration of Independence of th...
## $ author              &amp;lt;chr&amp;gt; NA, &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;United States&amp;quot;, ...
## $ gutenberg_author_id &amp;lt;int&amp;gt; NA, 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, ...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; NA, &amp;quot;United States Law/American Revolution...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_authors) # summarize authors meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 16,236
## Variables: 7
## $ gutenberg_author_id &amp;lt;int&amp;gt; 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 1...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;United States&amp;quot;, &amp;quot;Lincoln, Abraham&amp;quot;, &amp;quot;Henr...
## $ alias               &amp;lt;chr&amp;gt; NA, NA, NA, NA, &amp;quot;Dodgson, Charles Lutwidge...
## $ birthdate           &amp;lt;int&amp;gt; NA, 1809, 1736, NA, 1832, NA, 1819, 1860, ...
## $ deathdate           &amp;lt;int&amp;gt; NA, 1865, 1799, NA, 1898, NA, 1891, 1937, ...
## $ wikipedia           &amp;lt;chr&amp;gt; NA, &amp;quot;http://en.wikipedia.org/wiki/Abraham_...
## $ aliases             &amp;lt;chr&amp;gt; NA, &amp;quot;United States President (1861-1865)/L...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_subjects) # summarize subjects meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 140,173
## Variables: 3
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5...
## $ subject_type &amp;lt;chr&amp;gt; &amp;quot;lcc&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcs...
## $ subject      &amp;lt;chr&amp;gt; &amp;quot;E201&amp;quot;, &amp;quot;United States. Declaration of Independen...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The &lt;code&gt;gutenberg_metadata&lt;/code&gt;, &lt;code&gt;gutenberg_authors&lt;/code&gt;, and &lt;code&gt;gutenberg_subjects&lt;/code&gt; are periodically updated. To check to see when each data frame was last updated run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attr(gutenberg_metadata, &amp;quot;date_updated&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2016-05-05&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;To download the text itself we use the &lt;code&gt;gutenberg_download()&lt;/code&gt; function which takes one required argument, &lt;code&gt;gutenberg_id&lt;/code&gt;. The &lt;code&gt;gutenberg_download()&lt;/code&gt; function is what is known as ‘vectorized’, that is, it can take a single value or multiple values for the argument &lt;code&gt;gutenberg_id&lt;/code&gt;. Vectorization refers to the process of applying a function to each of the elements stored in a &lt;strong&gt;vector&lt;/strong&gt; –a primary object type in R. A vector is a grouping of values of one of various types including character (&lt;code&gt;chr&lt;/code&gt;), integer (&lt;code&gt;int&lt;/code&gt;), and logical (&lt;code&gt;lgl&lt;/code&gt;) and a data frame is a grouping of vectors. The &lt;code&gt;gutenberg_download()&lt;/code&gt; function takes an integer vector which can be manually added or selected from the &lt;code&gt;gutenberg_metadata&lt;/code&gt; or &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frames using the &lt;code&gt;$&lt;/code&gt; operator (e.g. &lt;code&gt;gutenberg_metadata$gutenberg_id&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let’s first add them manually here as a toy example by generating a vector of integers from 1 to 5 assigned to the variable name &lt;code&gt;ids&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids &amp;lt;- 1:5 # integer vector of values 1 to 5
ids&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the works from Project Gutenberg corresponding to the &lt;code&gt;gutenberg_id&lt;/code&gt;s 1 to 5, we pass the &lt;code&gt;ids&lt;/code&gt; object to the &lt;code&gt;gutenberg_download()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;works_sample &amp;lt;- gutenberg_download(gutenberg_id = ids) # download works with `gutenberg_id` 1-5
glimpse(works_sample) # summarize `works` dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,939
## Variables: 2
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;December, 1971  [Etext #1]&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;The Projec...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two attributes are returned: &lt;code&gt;gutenberg_id&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;. The &lt;code&gt;text&lt;/code&gt; column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded. There are many more attributes available from the Project Gutenberg API that can be accessed by passing a character vector of the attribute names to the argument &lt;code&gt;meta_fields&lt;/code&gt;. The column names of the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame contains the available attributes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(gutenberg_metadata) # print the column names of the `gutenberg_metadata` data frame&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;gutenberg_id&amp;quot;        &amp;quot;title&amp;quot;               &amp;quot;author&amp;quot;             
## [4] &amp;quot;gutenberg_author_id&amp;quot; &amp;quot;language&amp;quot;            &amp;quot;gutenberg_bookshelf&amp;quot;
## [7] &amp;quot;rights&amp;quot;              &amp;quot;has_text&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s augment our previous download with the title and author of each of the works. To create a character vector we use the &lt;code&gt;c()&lt;/code&gt; function, then, quote and delimit the individual elements of the vector with a comma.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download works with `gutenberg_id` 1-5 including `title` and `author` as attributes
works_sample &amp;lt;- gutenberg_download(gutenberg_id = ids, 
                            meta_fields = c(&amp;quot;title&amp;quot;,
                                            &amp;quot;author&amp;quot;))
glimpse(works_sample) # summarize dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,939
## Variables: 4
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;December, 1971  [Etext #1]&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;The Projec...
## $ title        &amp;lt;chr&amp;gt; &amp;quot;The Declaration of Independence of the United St...
## $ author       &amp;lt;chr&amp;gt; &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;Jeffer...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in a more practical scenario we would like to select the values of &lt;code&gt;gutenberg_id&lt;/code&gt; by some principled query such as works from a specific author, language, or subject. To do this we first query either the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame or the &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frame. Let’s say we want to download a random sample of 10 works from English Literature (Library of Congress Classification, “PR”). Using the &lt;code&gt;filter()&lt;/code&gt; function (part of the &lt;code&gt;tidyverse&lt;/code&gt; package set) we first extract all the Gutenberg ids from &lt;code&gt;gutenberg_subjects&lt;/code&gt; where &lt;code&gt;subject_type == &amp;quot;lcc&amp;quot;&lt;/code&gt; and &lt;code&gt;subject == &amp;quot;PR&amp;quot;&lt;/code&gt; assigning the result to &lt;code&gt;ids&lt;/code&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids &amp;lt;- 
  filter(gutenberg_subjects, subject_type == &amp;quot;lcc&amp;quot;, subject == &amp;quot;PR&amp;quot;)
glimpse(ids)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 7,100
## Variables: 3
## $ gutenberg_id &amp;lt;int&amp;gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 5...
## $ subject_type &amp;lt;chr&amp;gt; &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, ...
## $ subject      &amp;lt;chr&amp;gt; &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;The operators &lt;code&gt;=&lt;/code&gt; and &lt;code&gt;==&lt;/code&gt; are not equivalents. &lt;code&gt;==&lt;/code&gt; is used for logical evaluation and &lt;code&gt;=&lt;/code&gt; is an alternate notation for variable assignment (&lt;code&gt;&amp;lt;-&lt;/code&gt;).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frame does not contain information as to whether a &lt;code&gt;gutenberg_id&lt;/code&gt; is associated with a plain-text version. To limit our query to only those English Literature works with text, we filter the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame by the ids we have selected in &lt;code&gt;ids&lt;/code&gt; and the attribute &lt;code&gt;has_text&lt;/code&gt; in the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids_has_text &amp;lt;- 
  filter(gutenberg_metadata, gutenberg_id %in% ids$gutenberg_id, has_text == TRUE)
glimpse(ids_has_text)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 6,724
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43...
## $ title               &amp;lt;chr&amp;gt; &amp;quot;Alice&amp;#39;s Adventures in Wonderland&amp;quot;, &amp;quot;Throu...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;Carroll, Lewis&amp;quot;, &amp;quot;Carroll, Lewis&amp;quot;, &amp;quot;Carro...
## $ gutenberg_author_id &amp;lt;int&amp;gt; 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 3...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; &amp;quot;Children&amp;#39;s Literature&amp;quot;, &amp;quot;Children&amp;#39;s Liter...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;A couple R programming notes on the code phrase &lt;code&gt;gutenberg_id %in% ids$gutenberg_id&lt;/code&gt;. First, the &lt;code&gt;$&lt;/code&gt; symbol in &lt;code&gt;ids$gutenberg_id&lt;/code&gt; is the programmatic way to target a particular column in an R data frame. In this example we select the &lt;code&gt;ids&lt;/code&gt; data frame and the column &lt;code&gt;gutenberg_id&lt;/code&gt;, which is a integer vector. The &lt;code&gt;gutenberg_id&lt;/code&gt; variable that precedes the &lt;code&gt;%in%&lt;/code&gt; operator does not need an explicit reference to a data frame because the primary argument of the &lt;code&gt;filter()&lt;/code&gt; function is this data frame (&lt;code&gt;gutenberg_metadata&lt;/code&gt;). Second, the &lt;code&gt;%in%&lt;/code&gt; operator logically evaluates whether the vector elements in &lt;code&gt;gutenberg_metadata$gutenberg_ids&lt;/code&gt; are also found in the vector &lt;code&gt;ids$gutenberg_id&lt;/code&gt; returning &lt;code&gt;TRUE&lt;/code&gt; and &lt;code&gt;FALSE&lt;/code&gt; accordingly. This effectively filters those ids which are not in both vectors.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;As we can see the number of works with text is fewer than the number of works listed, 7100 versus 6724. Now we can safely do our random selection of 10 works, with the function &lt;code&gt;sample_n()&lt;/code&gt; and be confident that the ids we select will contain text when we take the next step by downloading the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123) # make the sampling reproducible
ids_sample &amp;lt;- sample_n(ids_has_text, 10) # sample 10 works
glimpse(ids_sample) # summarize the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 10
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 7688, 33533, 12160, 37761, 40406, 1050, 18...
## $ title               &amp;lt;chr&amp;gt; &amp;quot;Lucretia — Volume 04&amp;quot;, &amp;quot;The Convict&amp;#39;s Far...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;Lytton, Edward Bulwer Lytton, Baron&amp;quot;, &amp;quot;Pa...
## $ gutenberg_author_id &amp;lt;int&amp;gt; 761, 35765, 1865, 1256, 25821, 467, 1062, ...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, &amp;quot;One Act Plays&amp;quot;, NA, N...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can now pass our ids (&lt;code&gt;ids_sample$gutenberg_id&lt;/code&gt;) as the argument of &lt;code&gt;gutenberg_download()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;works_pr &amp;lt;- gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, meta_fields = c(&amp;quot;author&amp;quot;, &amp;quot;title&amp;quot;))
glimpse(works_pr) # summarize the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 79,200
## Variables: 4
## $ gutenberg_id &amp;lt;int&amp;gt; 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;THE DARK LADY OF THE SONNETS&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;By Bernard S...
## $ author       &amp;lt;chr&amp;gt; &amp;quot;Shaw, Bernard&amp;quot;, &amp;quot;Shaw, Bernard&amp;quot;, &amp;quot;Shaw, Bernard&amp;quot;...
## $ title        &amp;lt;chr&amp;gt; &amp;quot;The Dark Lady of the Sonnets&amp;quot;, &amp;quot;The Dark Lady of...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have data and could move on to processing this data in preparation for analysis. However, we are aiming for a reproducible workflow and this code does not conform to our principle of modularity: each subsequent step in our analysis will depend on running this code first. Furthermore, running this code as it is creates issues with bandwidth, as in our previous examples from direct downloads. To address modularity we will write the data to disk in &lt;strong&gt;plain-text format&lt;/strong&gt;. In this way each subsequent step in our analysis can access the data locally. To address bandwidth concerns, we will devise a method for checking to see if the data is already downloaded and skip the download, if possible, to avoid accessing the Project Gutenberg server unnecessarily.&lt;/p&gt;
&lt;p&gt;To write our data frame to disk we will export it into a standard plain-text format for two-dimensional data: a CSV file (comma-separated value). The CSV structure for this data will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## gutenberg_id,text,author,title
## 1050,THE DARK LADY OF THE SONNETS,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,By Bernard Shaw,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line contains the names of the columns and subsequent lines the observations. Data points that contain commas themselves (e.g. “Shaw, Bernard”) are quoted to avoid misinterpreting these commas a deliminators in our data. To write this data to disk we will use the &lt;code&gt;write_csv()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(works_pr, path = &amp;quot;data/original/gutenberg_works_pr.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To avoid downloading data that already resides on disk, let’s implement a similar strategy to the one used in the previous post for &lt;a href=&#34;https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/&#34;&gt;direct downloads&lt;/a&gt;. I’ve incorporated the code for sampling and downloading data for a particular subject from Project Gutenberg with a control statement to check if the data file already exists into a function I named &lt;code&gt;get_gutenberg_subject()&lt;/code&gt;. Take a look at this function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_gutenberg_subject &amp;lt;- function(subject, target_file, sample_size = 10) {
  # Function: to download texts from Project Gutenberg with 
  # a specific LCC subject and write the data to disk.
  
  # Check to see if the data already exists
  if(!file.exists(target_file)) { # if data does not exist, download and write
    target_dir &amp;lt;- dirname(x) # generate target directory for the .csv file
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&amp;quot;Downloading data... \n&amp;quot;) # print status message
    # Select all records with a particular LCC subject
    ids &amp;lt;- 
      filter(gutenberg_subjects, 
             subject_type == &amp;quot;lcc&amp;quot;, subject == subject) # select subject
    # Select only those records with plain text available
    set.seed(123) # make the sampling reproducible
    ids_sample &amp;lt;- 
      filter(gutenberg_metadata, 
             gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames 
             has_text == TRUE) %&amp;gt;% # select those ids that have text
      sample_n(sample_size) # sample N works (default N = 10)
    # Download sample with associated `author` and `title` metadata
    works_sample &amp;lt;- 
      gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, 
                         meta_fields = c(&amp;quot;author&amp;quot;, &amp;quot;title&amp;quot;))
    # Write the dataset to disk in .csv format
    write_csv(works_sample, path = target_file)
    cat(&amp;quot;Data downloaded! \n&amp;quot;) # print status message
  } else { # if data exists, don&amp;#39;t download it again
    cat(&amp;quot;Data already exists \n&amp;quot;) # print status message
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding this function to our function script &lt;code&gt;functions/acquire_functions.R&lt;/code&gt;, we can now use this function in our &lt;code&gt;code/acquire_data.R&lt;/code&gt; script to download multiple subjects and store them in on disk in their own file.&lt;/p&gt;
&lt;p&gt;Let’s download American Literature now (LCC code “PQ”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download Project Gutenberg text for subject &amp;#39;PQ&amp;#39; (American Literature)
# and then write this dataset to disk in .csv format
get_gutenberg_subject(subject = &amp;quot;PQ&amp;quot;, target_file = &amp;quot;data/original/gutenberg/works_pq.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Applying this function to both the English and American Literature datasets, our data directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── gutenberg
    │   ├── works_pq.csv
    │   └── works_pr.csv
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

7 directories, 8 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And as before in the previous post, it is a good idea to log the results of our work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log the directory structure of the Project Gutenberg data
system(command = &amp;quot;tree data/original/gutenberg &amp;gt;&amp;gt; log/data_original_gutenberg.log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- then work with a more complex package interface. Introduce the `fulltext` package (or [crminer](https://github.com/ropensci/crminer) . Show how to search a specific publication, download the full text (in XML) format. Then extract the `doi`, `title`, and `abstract`, convert it to a data.frame and store it as a `.csv` file. 

TODO: - work with `fulltext` and `crminer` exploration/ package tutorials
      - determine the R skills needed to complete this activity

Search for Plos ONE publications in Linguistics?
--&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post I provided an overview to acquiring data from web service APIs through R packages. We took at closer look at the &lt;code&gt;gutenbergr&lt;/code&gt; package which provides programmatic access to works available on Project Gutenberg. Working with package interfaces requires more knowledge of R including loading/ installing packages, working with vectors and data frames, and exporting data from an R session. We touched on these programming concepts and also outlined a method to create a reproducible workflow.&lt;/p&gt;
&lt;p&gt;Our last step in this mini series on acquiring data for language research with R, we will explore methods for acquire language data from the browsable web. I will discuss using the &lt;code&gt;rvest&lt;/code&gt; package for downloading and isolating text elements from HTML pages and show how to organize and write the data to disk.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-gutenbergr&#34;&gt;
&lt;p&gt;Robinson, David. 2017. &lt;em&gt;Gutenbergr: Download and Process Public Domain Works from Project Gutenberg&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=gutenbergr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=gutenbergr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2017. &lt;em&gt;Tidyverse: Easily Install and Load ’Tidyverse’ Packages&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;tidyverse&lt;/code&gt; is not a typical package. It is a set of packages: &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;readr&lt;/code&gt;, &lt;code&gt;purrr&lt;/code&gt;, and &lt;code&gt;tibble&lt;/code&gt;. These packages are all installed/ loaded with &lt;code&gt;tidyverse&lt;/code&gt; and form the backbone for the type of work you will typically do in most analyses.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://www.loc.gov/catdir/cpso/lcco/&#34;&gt;Library of Congress Classification&lt;/a&gt; documentation for a complete list of subject codes.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (1/3): direct downloads</title>
      <link>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There are three main ways to acquire corpus data using R that I will introduce you to: &lt;strong&gt;direct download&lt;/strong&gt;, &lt;strong&gt;package interfaces&lt;/strong&gt;, and &lt;strong&gt;web scraping&lt;/strong&gt;. In this post we will start by directly downloading a corpus as it is the most straightforward process for the novice R programmer and incurs the least number of steps. Along the way I will introduce some key R coding concepts including control statements and custom functions.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The following code is available on GitHub &lt;code&gt;recipes-acquiring_data&lt;/code&gt; and is built on the &lt;code&gt;recipes-project_template&lt;/code&gt; I have discussed in detail &lt;a href=&#34;https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/&#34;&gt;here&lt;/a&gt; and made accessible &lt;a href=&#34;https://github.com/francojc/recipes-project_template.git&#34;&gt;here&lt;/a&gt;. I encourage you to follow along by downloading the &lt;code&gt;recipes-project_template&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; from the Terminal or create a new RStudio R Project and select the “Version Control” option.&lt;/p&gt;

&lt;/div&gt;

&lt;!-- TODO: add {#anchor} below --&gt;
&lt;div id=&#34;direct-downloads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Direct downloads&lt;/h2&gt;
&lt;p&gt;Published corpus data found in repositories or individual sources are usually the easiest to start working with as it is generally a matter of identifying a resource to download and then downloading it with R. OK, there’s a little more involved, but that’s the basic idea.&lt;/p&gt;
&lt;p&gt;Let’s take a look at how this works starting with the a sample from the Switchboard Corpus, a corpus of 2,400 telephone conversations by 543 speakers. First we navigate to the site with a browser and download the file that we are looking for. In this case I found the Switchboard Corpus on the &lt;a href=&#34;http://www.nltk.org/nltk_data/&#34;&gt;NLTK data repository site&lt;/a&gt;. More often than not this file will be some type of compressed archive file with an extension such as &lt;code&gt;.zip&lt;/code&gt; or &lt;code&gt;.tz&lt;/code&gt;, which is the case here. Archive files make downloading multiple files easy by grouping files and directories into one file. In R we can used the &lt;code&gt;download.file()&lt;/code&gt; function from the base R library&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. There are a number of &lt;strong&gt;arguments&lt;/strong&gt; that a function may require or provide optionally. The &lt;code&gt;download.file()&lt;/code&gt; function minimally requires two: &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;destfile&lt;/code&gt;. That is the file to download and the location where it is to be saved to disk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download .zip file and write to disk
download.file(url = &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;, destfile = &amp;quot;data/original/switchboard.zip&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once an archive file is downloaded, however, the file needs to be ‘decompressed’ to reveal the file structure. The file we downloaded is located on our disk at &lt;code&gt;data/original/switchboard.zip&lt;/code&gt;. To decompress this file we use the &lt;code&gt;unzip()&lt;/code&gt; function with the arguments &lt;code&gt;zipfile&lt;/code&gt; pointing to the &lt;code&gt;.zip&lt;/code&gt; file and &lt;code&gt;exdir&lt;/code&gt; specifying the directory where we want the files to be extracted to.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;I encourage you to use the &lt;code&gt;TAB&lt;/code&gt; key to expand the list of options of a function to avoid having to remember the arguments of a function and also to avoid typos. After typing the name of the function and opening &lt;code&gt;(&lt;/code&gt; hit &lt;code&gt;TAB&lt;/code&gt; to view and select the argument(s) you want. Furthermore, the &lt;code&gt;TAB&lt;/code&gt; key can also help you expand paths to files and directories. Note that the expansion will default to the current working directory.&lt;/p&gt;

&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Decompress .zip file and extract to our target directory
unzip(zipfile = &amp;quot;data/original/switchboard.zip&amp;quot;, exdir = &amp;quot;data/original/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The directory structure of &lt;code&gt;data/&lt;/code&gt; now should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/
├── derived
└── original
    ├── switchboard
    │   ├── README
    │   ├── discourse
    │   ├── disfluency
    │   ├── tagged
    │   ├── timed-transcript
    │   └── transcript
    └── switchboard.zip

3 directories, 7 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have acquired the data programmatically and with this code as part of our workflow anyone could run this code and reproduce the same results. The code as it is, however, is not ideally efficient. Firstly the &lt;code&gt;switchboard.zip&lt;/code&gt; file is not strictly needed after we decompress it and it occupies disk space if we keep it. And second, each time we run this code the file will be downloaded from the remote serve leading to unnecessary data transfer and server traffic. Let’s tackle each of these issues in turn.&lt;/p&gt;
&lt;p&gt;To avoid writing the &lt;code&gt;switchboard.zip&lt;/code&gt; file to disk (long-term) we can use the &lt;code&gt;tempfile()&lt;/code&gt; function to open a temporary holding space for the file. This space can then be used to store the file, unzip it, and then the temporary file will be destroyed. We assign the temporary space to an R object we will name &lt;code&gt;temp&lt;/code&gt; with the &lt;code&gt;tempfile()&lt;/code&gt; function. This object can now be used as the value of the argument &lt;code&gt;destfile&lt;/code&gt; in the &lt;code&gt;download.file()&lt;/code&gt; function. Let’s also assign the web address to another object &lt;code&gt;url&lt;/code&gt; which we will use as the value of the &lt;code&gt;url&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a temporary file space for our .zip file
temp &amp;lt;- tempfile()
# Assign our web address to `url`
url &amp;lt;- &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;
# Download .zip file and write to disk
download.file(url, temp)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;In the previous code I’ve used the values stored in the objects &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;temp&lt;/code&gt; in the &lt;code&gt;download.file()&lt;/code&gt; function without specifying the argument names –only providing the names of the objects. R will assume that values of a function map to the ordering of the arguments. If your values do not map to ordering of the arguments you are required to specify the argument name and the value. To view the ordering of objects hit &lt;code&gt;TAB&lt;/code&gt; after entering the function name or consult the function documentation by prefixing the function name with &lt;code&gt;?&lt;/code&gt; and hitting &lt;code&gt;ENTER&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;At this point our downloaded file is stored temporarily on disk and can be accessed and decompressed to our target directory using &lt;code&gt;temp&lt;/code&gt; as the value for the argument &lt;code&gt;zipfile&lt;/code&gt; from the &lt;code&gt;unzip()&lt;/code&gt; function. I’ve assigned our target directory path to &lt;code&gt;target_dir&lt;/code&gt; and used it as the value for the argument &lt;code&gt;exdir&lt;/code&gt; to prepare us for the next tweak on our approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Assign our target directory to `target_dir`
target_dir &amp;lt;- &amp;quot;data/original/&amp;quot;
# Decompress .zip file and extract to our target directory
unzip(zipfile = temp, exdir = target_dir)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/
├── derived
└── original
    └── switchboard
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

3 directories, 6 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second issue I raised concerns the fact that running this code as part of our project will repeat the download each time. Since we would like to be good citizens and avoid unnecessary traffic on the web it would be nice if our code checked to see if we already have the data on disk and if it exists, then skip the download, if not then download it. To achieve this we need to introduce two new functions &lt;code&gt;if()&lt;/code&gt; and &lt;code&gt;dir.exists()&lt;/code&gt;. &lt;code&gt;dir.exists()&lt;/code&gt; takes a path to a directory as an argument and returns the logical value, &lt;code&gt;TRUE&lt;/code&gt;, if that directory exists, and &lt;code&gt;FALSE&lt;/code&gt; if it does not. &lt;code&gt;if()&lt;/code&gt; evaluates logical statements and processes subsequent code based on the logical value it is passed as an argument. Let’s look at a toy example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num &amp;lt;- 1
if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1 is 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I assigned &lt;code&gt;num&lt;/code&gt; to the value &lt;code&gt;1&lt;/code&gt; and created a logical evaluation &lt;code&gt;num ==&lt;/code&gt; whose result is passed as the argument to &lt;code&gt;if()&lt;/code&gt;. If the statement returns &lt;code&gt;TRUE&lt;/code&gt; then the code withing the first set of curly braces &lt;code&gt;{...}&lt;/code&gt; is run. If &lt;code&gt;num == 1&lt;/code&gt; is false, like in the code below, the code withing the braces following the &lt;code&gt;else&lt;/code&gt; will be run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num &amp;lt;- 2
if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;if()&lt;/code&gt; is one of various functions that are called &lt;strong&gt;control statements&lt;/strong&gt;. Theses functions provide a lot of power to make dynamic choices as code is run.&lt;/p&gt;
&lt;p&gt;Before we get back to our key objective to avoid downloading resources that we already have on disk, let me introduce another strategy to making code more powerful and ultimately more efficient and as well as more legible –the &lt;strong&gt;custom function&lt;/strong&gt;. Custom functions are functions that the user writes to create a set of procedures that can be run in similar contexts. I’ve created a custom function named &lt;code&gt;eval_num()&lt;/code&gt; below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num &amp;lt;- function(num) {
  if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look at what’s going on here. The function &lt;code&gt;function()&lt;/code&gt; creates a function in which the user decides what arguments are necessary for the code to perform its task. In this case the only necessary argument is the object to store a numeric value to be evaluated. I’ve called it &lt;code&gt;num&lt;/code&gt; because it reflects the name of the object in our toy example, but there is nothing special about this name. It’s only important that the object names be consistently used. I’ve included our previous code (except for the hard-coded assignment of &lt;code&gt;num&lt;/code&gt;) inside the curly braces and assigned the entire code chunk to &lt;code&gt;eval_num&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now use the function &lt;code&gt;eval_num()&lt;/code&gt; to perform the task of evaluating whether a value of &lt;code&gt;num&lt;/code&gt; is or is not equal to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1 is 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve put these coding strategies together with our previous code in a function I named &lt;code&gt;get_zip_data()&lt;/code&gt;. There is a lot going on here. Take a look first and see if you can follow the logic involved given what you now know.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_zip_data &amp;lt;- function(url, target_dir) {
  # Function: to download and decompress a .zip file to a target directory
  
  # Check to see if the data already exists
  if(!dir.exists(target_dir)) { # if data does not exist, download/ decompress
    cat(&amp;quot;Creating target data directory \n&amp;quot;) # print status message
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&amp;quot;Downloading data... \n&amp;quot;) # print status message
    temp &amp;lt;- tempfile() # create a temporary space for the file to be written to
    download.file(url = url, destfile = temp) # download the data to the temp file
    unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE) # decompress the temp file in the target directory
    cat(&amp;quot;Data downloaded! \n&amp;quot;) # print status message
  } else { # if data exists, don&amp;#39;t download it again
    cat(&amp;quot;Data already exists \n&amp;quot;) # print status message
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK. You should have recognized the general steps in this function: the argument &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;target_dir&lt;/code&gt; specify where to get the data and where to write the decompressed files, the &lt;code&gt;if()&lt;/code&gt; statement evaluates whether the data already exists, if not (&lt;code&gt;!dir.exists(target_dir)&lt;/code&gt;) then the data is downloaded and decompressed, if it does exist (&lt;code&gt;else&lt;/code&gt;) then it is not downloaded.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The prefixed &lt;code&gt;!&lt;/code&gt; in the logical expression &lt;code&gt;dir.exists(target_dir)&lt;/code&gt; returns the opposite logical value. This is needed in this case so when the target directory exists, the expression will return &lt;code&gt;FALSE&lt;/code&gt;, not &lt;code&gt;TRUE&lt;/code&gt;, and therefore not proceed in downloading the resource.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;There are a couple key tweaks I’ve added that provide some additional functionality. For one I’ve included the function &lt;code&gt;dir.create()&lt;/code&gt; to create the target directory where the data will be written. I’ve also added an additional argument to the &lt;code&gt;unzip()&lt;/code&gt; function, &lt;code&gt;junkpaths = TRUE&lt;/code&gt;. Together these additions allow the user to create an arbitrary directory path where the files, and only the files, will be extracted to on our disk. This will discard the containing directory of the &lt;code&gt;.zip&lt;/code&gt; file which can be helpful when we want to add multiple &lt;code&gt;.zip&lt;/code&gt; files to the same target directory.&lt;/p&gt;
&lt;p&gt;A practical scenario where this applies is when we want to download data from a corpus that is contained in multiple &lt;code&gt;.zip&lt;/code&gt; files but still maintain these files in a single primary data directory. Take for example the &lt;a href=&#34;http://www.linguistics.ucsb.edu/research/santa-barbara-corpus&#34;&gt;Santa Barbara Corpus&lt;/a&gt;. This corpus resource includes a series of interviews in which there is one &lt;code&gt;.zip&lt;/code&gt; file, &lt;code&gt;SBCorpus.zip&lt;/code&gt; which contains the &lt;a href=&#34;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip&#34;&gt;transcribed interviews&lt;/a&gt; and another &lt;code&gt;.zip&lt;/code&gt; file, &lt;code&gt;metadata.zip&lt;/code&gt; which organizes the &lt;a href=&#34;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip&#34;&gt;meta-data&lt;/a&gt; associated with each speaker. Applying our initial strategy to download and decompress the data will lead to the following directory structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── SBCorpus
    │   ├── TRN
    │   └── __MACOSX
    │       └── TRN
    └── metadata
        └── __MACOSX

8 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By applying our new custom function &lt;code&gt;get_zip_data()&lt;/code&gt; to the transcriptions and then the meta-data we can better organize the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download corpus transcriptions
get_zip_data(url = &amp;quot;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip&amp;quot;, target_dir = &amp;quot;data/original/sbc/transcriptions/&amp;quot;)

# Download corpus meta-data
get_zip_data(url = &amp;quot;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip&amp;quot;, target_dir = &amp;quot;data/original/sbc/meta-data/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now our &lt;code&gt;data/&lt;/code&gt; directory is better organized; both the transcriptions and the meta-data are housed under &lt;code&gt;data/original/sbc/&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    └── sbc
        ├── meta-data
        └── transcriptions

5 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we add data from other sources we can keep them logical separate and allow our data collection to scale without creating unnecessary complexity. Let’s add the Switchboard Corpus sample using our &lt;code&gt;get_zip_data()&lt;/code&gt; function to see this in action.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download corpus
get_zip_data(url = &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;, target_dir = &amp;quot;data/original/scs/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our corpora our housed in their own directories and the files are clearly associated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs

6 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have what we need to continue to the next step in our data analysis project. But before we go, we should do some housekeeping to document and organize this process to make our work reproducible. We will take advantage of the &lt;code&gt;project-template&lt;/code&gt; directory structure, seen below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── README.md
├── _pipeline.R
├── code
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data
│   ├── derived
│   └── original
├── figures
├── functions
├── log
├── recipes-acquire-data.Rproj
└── report
    ├── article.Rmd
    ├── bibliography.bib
    ├── slides.Rmd
    └── web.Rmd

8 directories, 13 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First it is good practice to separate custom functions from our processing scripts. We can create a file in our &lt;code&gt;functions/&lt;/code&gt; directory named &lt;code&gt;acquire_functions.R&lt;/code&gt; and add our custom function &lt;code&gt;get_zip_data()&lt;/code&gt; there. We then use the &lt;code&gt;source()&lt;/code&gt; function to read that function into our current script to make it available to use as needed. It is good practice to source your functions in the SETUP section of your script.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load custom functions for this project
source(file = &amp;quot;functions/acquire_functions.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second it is advisable to log the structure of the data in plain text files. You can create a directory tree (as those seen in this post) with the bash command &lt;code&gt;tree&lt;/code&gt; on the command line. R provides a function &lt;code&gt;system()&lt;/code&gt; which will interface the command line. Adding the following code to the LOG section of your &lt;code&gt;acquire_data.R&lt;/code&gt; R script will generate the directory structure for each of the corpora that we have downloaded in this post in the files &lt;code&gt;data_original_sbc.log&lt;/code&gt; and &lt;code&gt;data_original_scs.log&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log the directory structure of the Santa Barbara Corpus
system(command = &amp;quot;tree data/original/sbc &amp;gt;&amp;gt; log/data_original_sbc.log&amp;quot;)
# Log the directory structure of the Switchboard Corpus sample
system(command = &amp;quot;tree data/original/scs &amp;gt;&amp;gt; log/data_original_scs.log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our project directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── README.md
├── _pipeline.R
├── code
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data
│   ├── derived
│   └── original
├── figures
├── functions
│   └── acquire_functions.R
├── log
│   ├── data_original_sbc.log
│   └── data_original_scs.log
├── recipes-acquire-data.Rproj
└── report
    ├── article.Rmd
    ├── bibliography.bib
    ├── slides.Rmd
    └── web.Rmd

8 directories, 15 files&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we’ve covered how to access, download, and organize data contained in .zip files; the most common format for language data found on repositories and individual sites. This included an introduction to a few key R programming concepts and strategies including using functions, writing custom functions, and controlling program flow with control statements. Our approach was to gather data while also keeping in mind the reproducibility of the code. To this end I introduced programming strategies for avoiding unnecessary web traffic (downloads), scalable directory creation, and data documentation.&lt;/p&gt;
&lt;p&gt;In the next post in this three part mini-series I will cover acquiring data from web services such as Project Gutenberg, Twitter, and Facebook through R packages. Using package interfaces will require additional knowledge of R objects. I will discuss vector types and data frames and show how to manipulate these objects in practical situations like filtering data and writing data to disk in plain-text files.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Remember base R packages are installed by default with R and are loaded and accessible by default in each R session.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data for language research -types and sources</title>
      <link>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</guid>
      <description>&lt;p&gt;In this Recipe you will learn about the types of data available for language research and where to find data. The goal, then, is to introduce you to the landscape of language data available and provide a general overview of the characteristics of language data from a variety of sources providing you with resources to begin your own quantitative investigations.&lt;/p&gt;
&lt;div id=&#34;data-for-language-research&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data for language research&lt;/h2&gt;
&lt;p&gt;Language research can include data from a variety of sources, linguistic and non-linguistic, that record observations about the world. A typical type of data used in quantitative language research is a &lt;strong&gt;corpus&lt;/strong&gt;. In short, a corpus is set of machine-readable texts that have been compiled with an eye towards linguistic research. All corpora are not created equal, however, in content and/or format. A corpus may aim to represent a wide swath of language behavior or very specific aspects. It can be language specific (English or French), target a particular modality (spoken or written), or approximate domains of language use (medicine, business, etc.). A corpus that aims to represent a language (including modalities, registers, and sub-domains), for example, is known as a &lt;em&gt;generalized corpus&lt;/em&gt;. Corpora that aim to capture a snapshot of a particular modality or sub-domain of language use are known as &lt;em&gt;specialized corpora&lt;/em&gt;. Each corpus will have an underlying target population and the sampling process will reflect the authors’ best attempt (given the conditions at the point the corpus was compiled) at representing the stated target population. Whether a corpus is generalized or specialized can become difficult to nail down between the extremes. As such, it is key to be clear about the scope of a particular corpus to be able to ascertain its potential applications and gauge the extent to which these applications entail the research goals of your particular project.&lt;/p&gt;
&lt;p&gt;A corpus will often include various types of non-linguistic attributes, or &lt;em&gt;meta-data&lt;/em&gt;, as well. Ideally this will include information regarding the source(s) of the data, dates when it was acquired or published, and other author or speaker information. It may also include any number of other attributes that were identified as potentially important in order to appropriately document the target population. Again, it is key to match the available meta-data with the goals of your research. In some cases a corpus may be ideal in some aspects but not contain all the key information to address your research question. This may mean you will need to compile your own corpus if there are fundamental attributes missing. Before you consider compiling your own corpus, however, it is worth investigating the possibility of augmenting an available corpus to bring it inline with your particular goals. This may include adding new language sources, harnessing software for linguistic annotation (part-of-speech, syntactic structure, named entities, etc.), or linking available corpus meta-data to other resources, linguistic or non-linguistic.&lt;/p&gt;
&lt;p&gt;Corpora come in various formats, the main three being: running text, structured documents, and databases. The format of a corpus is often influenced by characteristics of the data but may also reflect an author’s individual preferences as well. It is typical for corpora with few meta-data characteristics to take the form of running text. In corpora with more meta-data a header may be appended to the top of each running text document or the meta-data may be contained in a separate file with appropriate coding to coordinate meta-data attributes with each text in the corpus. When meta-data increases in complexity it is common to structure each corpus document more explicitly with a markup language such as XML (Extensible Markup Language) or organize relationships between language and meta-data attributes in a database. Although there has been a push towards standardization of corpus formats, most available resources display some degree of idiosyncrasy. Being able to parse the structure of a corpus is a skill that will develop with time. With more experience working with corpora you will become more adept at identifying how the data is stored and whether its content and format will serve the needs of your analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sources-of-language-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources of language data&lt;/h2&gt;
&lt;p&gt;The most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes and language used in natural settings that can be coerced by the investigator into a corpus. Many organizations exist around the globe that provide access to corpora in browsable catalogs, or &lt;strong&gt;repositories&lt;/strong&gt;. There are repositories dedicated to language research, in general, such as the &lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Language Data Consortium&lt;/a&gt; or for specific language domains, such as the language acquisition repository &lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication. A standardized resource many times will be easier to interpret and evaluate for its appropriateness for a particular research project.&lt;/p&gt;
&lt;p&gt;In the table below I’ve compiled a list of some corpus repositories to help you get started.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-repositories&#34;&gt;Table 1: &lt;/span&gt;A list of some language corpora repositories.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus.byu.edu/&#34;&gt;BYU corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A repository of corpora that includes billions of words of data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://corporafromtheweb.org/&#34;&gt;COW (COrpora from the Web)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of linguistically processed gigatoken web corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://wortschatz.uni-leipzig.de/en/download/&#34;&gt;Leipzig Corpora Collection&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpora in different languages using the same format and comparable sources.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Linguistic Data Consortium&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.resourcebook.eu/searchll.php#&#34;&gt;LRE Map&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.nltk.org/nltk_data/&#34;&gt;NLTK language data&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of corpora and language datasets included with the Python package NLTK.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/&#34;&gt;OPUS - an open source parallel corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of translated texts from the web.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus1.mpi.nl/ds/asv/?4&#34;&gt;The Language Archive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Various corpora and language datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://ota.ox.ac.uk/&#34;&gt;The Oxford Text Archive (OTA)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of thousands of texts in more than 25 different languages.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Repositories are by no means the only source of corpora on the web. Researchers from around the world provide access to corpora and other data sources on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. Finding these resources is a matter of doing a web search with the word ‘corpus’ and a list of desired attributes, including language, modality, register, etc. As part of a general movement towards reproducible more corpora are available on the web than ever before. Therefore data sharing platforms supporting reproducible research, such as &lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;, &lt;a href=&#34;http://www.re3data.org/&#34;&gt;Re3data&lt;/a&gt;, etc., are a good place to look as well, if searching repositories and targeted web searches do not yield results.&lt;/p&gt;
&lt;p&gt;In the table below you will find a list of corpus resources and datasets.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-corpora&#34;&gt;Table 2: &lt;/span&gt;Corpora and language datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://storage.googleapis.com/books/ngrams/books/datasetsv2.html&#34;&gt;Google Ngram Viewer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Google web corpus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~enron/&#34;&gt;Enron Email Dataset&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Enron email data from about 150 users, mostly senior management.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://cesa.arizona.edu/&#34;&gt;Corpus of Spanish in Southern Arizona&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Spanish varieties spoken in Arizona.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/OpenSubtitles_v2.php&#34;&gt;OpenSubtitles2011&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of documents from &lt;a href=&#34;http://www.opensubtitles.org/&#34; class=&#34;uri&#34;&gt;http://www.opensubtitles.org/&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.statmt.org/europarl/&#34;&gt;Europarl Parallel Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A parallel corpus based on the proceedings of the European Parliament&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.lllf.uam.es/~fmarcos/informes/corpus/coarginl.html&#34;&gt;Corpus Argentino&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpus of Argentine Spanish&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.ruscorpora.ru/en/&#34;&gt;Russian National Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus of modern Russian language incorporating over 300 million words.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html&#34;&gt;Cornell Movie-Dialogs Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is important to note that there can be access and use restrictions for data from particular sources. Compiling, hosting, and maintaining corpus resources can be costly. To gain full access to data, some repositories and homepages of larger corpora require a fee to offset these costs. In other cases, resources may require individual license agreements to ensure that the data is not being used in ways it was not intended or to ensure potentially sensitive participant information will be treated appropriately. You can take a look at a &lt;a href=&#34;https://www.corpusdata.org/restrictions.asp&#34;&gt;license agreement for the BYU Corpora&lt;/a&gt; as an example. If you are a member of an academic institution and aim to conduct research for scholarly purposes licensing is often easily obtained. Fees, on the other hand, may present a more challenging obstacle. If you are an affiliate of an academic institution it is worth checking with your library to see if there are funds for acquiring licensing for you as an individual, a research group or lab or, for the institution.&lt;/p&gt;
&lt;p&gt;If your corpus search ends in a dead-end, either because a suitable resource does not appear to exist or an existing resource is unattainable given licensing restrictions or fees, it may be time to compile your own corpus. Turning to machine readable texts on the internet is usually the logical first step to access language for a new corpus. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on social media, news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by hand instead of automating the task. Our approach here, however, will be to automate the process as much as possible whether that means leveraging R package interfaces to language data, converting hundreds of pdf documents to plain text, or scraping content from web documents.&lt;/p&gt;
&lt;p&gt;The table below lists some R packages that serve to interface language data directly through R.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-apis&#34;&gt;Table 3: &lt;/span&gt;R Package interfaces to language corpora and datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/crminer&#34;&gt;crminer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface focusing on getting the user full text via the Crossref search API.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/arxiv_tutorial.html&#34;&gt;aRxiv&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/internetarchive_tutorial.html&#34;&gt;internetarchive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/dvn&#34;&gt;dvn&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to access to the Dataverse Network APIs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/gutenbergr_tutorial.html&#34;&gt;gutenbergr&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to download and process public domain works from the Project Gutenberg collection.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/fulltext_tutorial.html&#34;&gt;fulltext&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query open access journals, such as PLOS.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/hrbrmstr/newsflash&#34;&gt;newsflash&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive and GDELT Television Explorer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/oai&#34;&gt;oai&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query any OAI-PMH repository, including Zenodo.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/rfigshare&#34;&gt;rfigshare&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the data sharing platform FigShare.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Data for language research is not limited to (primary) text sources. Other sources may include processed data from previous research; word lists, linguistic features, etc.. Alone or in combination with text sources this data can be a rich and viable source of data for a research project.&lt;/p&gt;
&lt;p&gt;Below I’ve included some processed language resources.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-experimental&#34;&gt;Table 4: &lt;/span&gt;Language data from previous research and meta-studies.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/lingtypology&#34;&gt;lingtypology&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://elexicon.wustl.edu/WordStart.asp&#34;&gt;English Lexicon Project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://icon.shef.ac.uk/Moby/&#34;&gt;The Moby lexicon project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Language wordlists and resources from the Moby project.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The list of data available for language research is constantly growing. I’ve document very few of the wide variety of resources. Below I’ve included attempts by others to provide a summary of the corpus data and language resources available.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-listing&#34;&gt;Table 5: &lt;/span&gt;Lists of corpus resources.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html&#34;&gt;Learner corpora around the world&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A listing of learner corpora around the world&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/&#34;&gt;Where can you find language data on the web?&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of various corpora and language datasets.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://nlp.stanford.edu/links/statnlp.html#Corpora&#34;&gt;Stanford NLP corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of corpora and language resources aimed at the NLP community.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we have covered some of the basics on data for language research including types of data and sources to get you started on the path of identifying a viable source for your data analysis project. In the next post we will begin working directly with R code to access and acquire data through R. Along the way I will introduce fundamental programming concepts of the language you will use throughout your project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- Topics:

- natural and naturalistic
modality, register (formal/ informal), 
  - spoken (Callhome -LDC, ...)
  - written (literature -gutenbergr, periodicals -LDC, blogs, social media -streamR, ...)
  - other (tv/film closed captions -newsflash, Brysbaert/ ACTIV-ES, translation -OPEL?, ...)
- elicited data
  - essays (-BELC, )
  - interviews (Santa Barbara Corpus)
  - surveys (US Census -acs, Harvard Dialect Survey, Language attitude)
  - experimental findings
    - response times (MRC lexicon), eye-gaze, acceptability ratings, etc. 
    
- 


- Types of data
  - Uncurated
  - Curated 
- Sources of data
    - Repositories
    - Sources
    - Web
- Data formats
  - Plain text
    - .txt
    - .csv/ .tsv
    - .xml/ .json
  - Other files
    - .doc(x)
    - .pdf
  - Databases
- How to acquire data
    - Package interface (`gutenbergr`)
    - API interface (`streamR`)
    - Download, read (multiple files, w/ lapply and scan?
    - Web scraping (`rvest`)
--&gt;
&lt;!-- ## Types of data --&gt;
&lt;!-- In the last post we discussed what data is and the importance of data sampling and organization is for subsequent data analysis. We touched briefly on an example in which we were working with files which our language data was in running text format. Running text is one of the types of data that you will encounter when you look to obtain data to conduct research into the topic you are interested in knowing something more about. In this post we leared that text in this format needed to be organized into a format that was more conducive for statistical analysis. The aim, then, was to **curate** the **uncurated** data. Athough data is often talked about in terms of being curated or uncurated, it is important to understand that this is less a dicotomy and more of a continuum. Since each analysis has specific goals the primary data is always in need of some amount of curation to prepare the data in the particular ways necessary to faciliate the particular goals of the particular analysis.   --&gt;
&lt;!-- ## Sources of data --&gt;
&lt;!-- Having said that it is convenient to talk about curation in binary terms as it facilitates a discussion of the sources of data out there and to what degree the researcher needs to manipulate the data from particular sources to be able to conduct an analysis. When looking at the landscape of language data available, sources that have been prepared to some degree to facilitate analysis are often found in *repositories*. There are countless repositories that can be accessed on the web.  --&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-mcenery:2012&#34;&gt;
&lt;p&gt;McEnery, Tony, and Andrew Hardie. 2012. &lt;em&gt;Corpus Linguistics: Method, Theory and Practice&lt;/em&gt;. Cambridge University Press. doi:&lt;a href=&#34;https://doi.org/10.1017/CBO9780511981395&#34;&gt;10.1017/CBO9780511981395&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to statistical thinking</title>
      <link>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</guid>
      <description>&lt;p&gt;Before we begin working on the specifics of our data project, it is important to have a clear understanding of some of the basic concepts that need to be in place to guide our work. In this post I will cover some of these topics including the importance of identifying a research question, how different statistical approaches relate to different types of research, and understanding data from a sampling and organizational standpoint. I will also provide some examples of linking research questions with variables in a toy dataset as we begin to discuss how to approach data analysis, primarily through visualization techniques.&lt;/p&gt;
&lt;div id=&#34;research-aims&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research aims&lt;/h2&gt;
&lt;!-- TODO: pepper in examples of the types of questions and methods one might find --&gt;
&lt;p&gt;Before jumping into the code, every researcher must come to a project with a clear idea about the purpose of the analysis. This means doing your homework in order to understand what it is exactly that you want to achieve; that is, you need to identify a &lt;strong&gt;research question&lt;/strong&gt;. The first step is become versed in the previous literature on the topic. What has been written? What are the main findings? Secondly, it is important to become familiar with the standard methods for approaching the topic of interest. How has the topic been approached methodologically? What are the types, sources, and quality of data employed? What have been the statistical approaches employed? What particular statistical tests have been chosen? Getting an overview not only of the domain-specific findings in the literature but also the methodological choices will help you identify promising plan for carrying out your research.&lt;/p&gt;
&lt;!-- All other steps in an analysis will be guided by this question and hinge on the choices you make conceptually to carry out a data analysis plan.  --&gt;
&lt;!-- * Identify a research question --&gt;
&lt;!--   - research previous literature --&gt;
&lt;!--   - get to know the nature of the phenomenon --&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-statistical-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing a statistical approach&lt;/h2&gt;
&lt;p&gt;With a research question in hand and a sense of how similar studies have approached the topic methodologically, it’s time to make a more refined decision about how the data is to be analyzed. This decision will dictate all other methodological choices from data collection to interpreting results.&lt;/p&gt;
&lt;p&gt;There are three main statistical approaches:&lt;/p&gt;
&lt;!-- * Identify goals of your analysis --&gt;
&lt;div id=&#34;inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;!-- TODO: find a way to introduce the concept &#39;model&#39;. this could make it easier to speak about statistical approaches, here and later in the post --&gt;
&lt;!-- TODO: pepper in examples of each type of statistical approach to ground these concepts a bit --&gt;
&lt;p&gt;Also commonly known as hypothesis testing or confirmation, statistical inference aims to establish whether there is a reliable and generalizable relationship given patterns in the data. The approach makes the starting assumption that there is no relationship, or that the null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) is true. A relationship is only reliable, or &lt;em&gt;significant&lt;/em&gt;, if the chance that the null hypothesis is false is less than some predetermined threshold; in which case we accept the alternative hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;). The standard threshold used in the Social Sciences, Linguistic included, is the famous p-value &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt;. Without digging into the deeper meaning of a p-value, in a nutshell a p-value is a confidence measure to suggest that the relationship you are investigating is robust and reliable given the data. In an inference approach all the data is used and is used &lt;em&gt;only&lt;/em&gt; once. This is not the case for the other two statistical approaches we will cover, Exploration and Prediction. For this reason it is vital to identify your statistical approach from the beginning. In the case of inference tests, failing to make a clear hypothesis often leads to p-hacking; a practice of running multiple tests and/or parameters on the same data (i.e. reusing the data) until evidence for the alternative hypothesis appears.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploration&lt;/h3&gt;
&lt;p&gt;One of two statistical learning approaches, this statistical method is used to uncover potential relationships in the data and gain new insight in an area where predictions and hypotheses cannot be clearly made. In statistical learning, exploration is a type of &lt;strong&gt;unsupervised learning&lt;/strong&gt;. Supervision here, and for Prediction, refers to the presence or absence of an outcome variable. By choosing exploration as our approach we make no assumptions (or hypotheses) about the relationships between any of the particular variables in the data. Rather we hope to investigate the extent to which we can induce meaningful patterns wherever they may lie. Findings from exploratory analyses can provide valuable insight for future study but they cannot be safely used to generalize to the larger population, which is why exploratory analyses are often known as hypothesis generating analyses (rather than hypothesis confirming). Given our generalizing power is curtailed, the data &lt;em&gt;can&lt;/em&gt; be reused multiple times trying out various tests. While it is not strictly required, data for exploratory analysis is often partitioned into two sets, training and validation, at roughly an 80%/20% split. The training set is used for refining statistical measures and the test set is used to evaluate the refined measures. Although the evaluation results still cannot be used to generalize, the insight can be taken as stronger evidence that there is a potential relationship, or set of relationships, worthy of further study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;The other statistical learning approach, Prediction, aims to uncover relationships in our data as they pertain to a particular outcome variable. This approach is known as &lt;strong&gt;supervised learning&lt;/strong&gt;. Similar to Exploration in many ways, this approach also makes no assumptions about the potential relationships between variables in our data and the data can be used multiple times to refine our statistical tests in order to tease out the most effective method for our goals. Where an exploratory analysis aims to uncover meaningful patterns of any sort, prediction, however, is more focused in that the main aim is to ascertain the extent to which the variables in the data pattern, individually or together, in such a way to make reliable associations to a particular outcome variable in unseen data. To evaluate the robustness of a prediction model the data is partitioned into training and validation sets. Depending on the application and the amount of available data, a third ‘development’ set is sometimes created as a pseudo test set to facilitate the testing of multiple approaches before the final evaluation. The proportions vary, but it a good rule of thumb is to reserve 60% of the data for training, 20% for development, and 20% for validation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;understanding-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Understanding data&lt;/h2&gt;
&lt;p&gt;Knowing the statistical approach to take then frames the next conceptual steps: &lt;strong&gt;data sampling&lt;/strong&gt; and &lt;strong&gt;organization of data&lt;/strong&gt;. But what is data anyway? Abstractly it is some set of empirical observations about the world. There are innumerable types of observations, as you can imagine, which can be used to describe objects and events. Our scientific aim is to systematically attempt to relate these observations and deduce the nature of their relationships to gain a better understanding of how our world works.&lt;/p&gt;
&lt;p&gt;Language research aims to understand a subset of these observations, namely those that concern linguistic behavior. The psycholinguist may observe the reaction times in a lexical decision task, eye-gaze in a visual world paradigm, or electro-magnetic brain activation in an ERP study. A sociolinguist may conduct interviews with members of a community, solicit language attitude responses to a language attitude survey, or ethnographically record face-to-face encounters. A syntactician may solicit acceptability ratings, calculate the frequency of a syntactic structure in a corpus, or document the permutations of subject-verb-object order in the world’s languages. As language is a defining characteristic of our species, language-related observations feature many other disciplines as well such as Anthropology, History, Neurology, Mathematics, and Biology. Linguistic inquiry, then, is not isolated to linguistic form, but rather the connection between linguistic form and other non-linguistic objects and events in the world at large –wherever that may take us.&lt;/p&gt;
&lt;!-- * Emperical observations --&gt;
&lt;!--   - Data is empirical observations about the world --&gt;
&lt;!--   - Innumberable types of observations: linguistic and non-linguistic --&gt;
&lt;!--   - Examples (acceptability ratings, reaction times, words in a corpus, lengths of words, age, sex, occupation, (non)-native speaker, etc.) --&gt;
&lt;div id=&#34;sampling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling&lt;/h3&gt;
&lt;p&gt;One major limitation inherent to most data sampling, and a primary reason why statistics are so important to doing and interpreting science, is the fact that our vantage point to the observing the world is restricted. We can only work with the data at our disposal, a &lt;strong&gt;sample&lt;/strong&gt;, even when it is clear that there is a much larger existing world, or &lt;strong&gt;population&lt;/strong&gt;. Ideally we would have access to the entire population of interest, but in most cases this is either not physically possible to obtain (or even store) the data or it is conceptually impossible to ever observe the entire population. As an example, say we wanted to catalog all the words in the English language. From a logistics point of view, where would we start? Any given dictionary only catalogs a subset of the words in a language –many words that are used in English-speaking communities, especially those from spoken language, will not appear. A corpus may capture linguistic diversity that does not appear in a dictionary, but it too will fall short of our lofty goal. But for argumentation sake, let’s imagine we could somehow capture all the words. What happens to our population in a day, a week, or a month from now? It quickly becomes a sample because new words are created all the time and some words are lost. Our population of words in the English language, then, is a moving target.&lt;/p&gt;
&lt;p&gt;This transitory property of populations is well-known and methods for obtaining reliable, or externally valid, samples is an area of study in its own right. In short, we aim for a sample to be balanced and representative of the idealized population. &lt;em&gt;Representativeness&lt;/em&gt; is the extent to which a sample reflects the total diversity in the population. &lt;em&gt;Balance&lt;/em&gt; is concerned with modeling the proportions of that diversity. An ideal sample combines both.&lt;/p&gt;
&lt;p&gt;The first strategy most often applied to obtaining a valid sample is to increase &lt;em&gt;sample size&lt;/em&gt;. This is an intuitive technique whose logic appeals to the notion that more is better. More is better, clearly. But more data alone does not always ensure an externally valid sample. For example, say we want to know something about the frequencies of words in written Spanish. Our target population is, then, words written in Spanish. It occurs to us that we can access a lot of written Spanish online via &lt;a href=&#34;http://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt;. We download works from many authors over a span of many years. After doing some calculations our sample contains around 100 million words. That’s a lot of words, and surely more indicative of the population than say 100 thousand words. But we have potentially overlooked something very important: all of the data in our sample comes from literature, specifically literature in the public domain. In other words, our sample is not random.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;random sample&lt;/em&gt; will help increase the potential diversity in any sample. In our sample this means drawing data from a number of written sources of Spanish at random. This strategy will increase our chances to capture written Spanish from other genres and registers. Now a 100 million word sample randomly selected from genres and registers of written Spanish is bound to be more representative of the population, but we run into another conceptual snag. Our sample is large and randomly selected from the population, but does it reflect the proportion each subgroup (genres and registers) contributes to the idealized population?&lt;/p&gt;
&lt;p&gt;There is no absolute way of knowing if the proportions of each subgroup are balanced, or even what all the subgroups may be for that matter, but in most cases we can make an educated guess on both these fronts that will allow us to increase the validity of our sample. For example, the literary genre ‘self-help’ intuitively constitutes a smaller portion of our target population than say ‘news’. Ideally we would want to reflect this understanding in our sample. Applying this logic is known as &lt;em&gt;stratified sampling&lt;/em&gt;. A large, stratified random sample is always at least as valid as an equally sized large random sample with the added benefit that we are safeguarded from large skews that a large random sample may potentially produce. Now it is important to keep in mind that stratified sampling has its limitations as well. The difficulties posed in obtaining a valid sample from the macro view (i.e. the total population is never observable) are present at the micro view as well (i.e. sub- and sub-substrata are equally illusive). Again, there are no absolutes in sampling. The key is to keep the aim of the research question clear during the sampling process and strive for sizable, randomly stratified samples to minimize sampling error to the extent that it is feasible –and then work from there.&lt;/p&gt;
&lt;p&gt;This lack of certainty in sampling may seem troublesome. Sampling uncertainty, however, does not mean we cannot gain insight into the essence of the objects and events in the world we aim to understand. It just means we need to be aware of any given sample’s limitations, document these limitations, and always approach statistical findings based on this data with caution; suspending generalizations of the absolute nature. This is why science, contrary to popular belief, does not ‘prove’ anything. Rather science aims to collect evidence for or against a hypotheses. Since the data is always changing there are no absolute conclusions. As the evidence grows, so does the case for a particular view of how the world works. It is this systematic approach which makes science so powerful.&lt;/p&gt;
&lt;!-- * Populations and samples --&gt;
&lt;!--   - Ideally we would have access to the entire record of those observation that are of potential interest to us in our analysis --&gt;
&lt;!--   - Reality is we, more often than not, cannot feasibly acquire the observations of an entire population. --&gt;
&lt;!--   - The best we can do is shoot for a sample of that population which aims to model the population as appropriately possible given the particular research question to be addressed --&gt;
&lt;!--   - In the end we will always strive for a balanced and representative sample. A balanced sample contains the predicted major features indicative of the population. A representative sample contains these features in a way that reflect the proportion of these features.  --&gt;
&lt;!--   - Attaining the most ideal sample given the potential limitations is key to gaining robust and generalizable insight into the nature of the phenomenon we are investigating.  --&gt;
&lt;/div&gt;
&lt;div id=&#34;organization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Organization&lt;/h3&gt;
&lt;p&gt;Identifying and capturing a data sample moves us one step closer to performing our data analysis but the format of the raw or original data is often not in a format conducive for visualization nor statistical tests. The hypothetical written Spanish data we identified to sample in the previous section would most likely take the form of documents of running text with potentially some meta-data about the text (author, title of the work, date published, genre, etc.) in the header of the file and/or the name of each file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Title: Cuando los robots tomen el mando y hagan la guerra
Date: 3 AGO 2015 - 00:00 CEST
Genre: News
Source: El País
Tags: Científicos, Isaac Asimov, Robótica, Gente, Tecnología, Informática, Ciencia, Sociedad, Industria

La primera reflexión abarcadora sobre la coexistencia entre los robots y los humanos no fue obra de un científico de la computación ni de un filósofo ético, sino de un novelista. Isaac Asimov formuló las tres “leyes de la robótica” que deberían incorporarse en la programación de cualquier autómata lo bastante avanzado como para suponer un peligro: “No dañar a los humanos, obedecerles salvo conflicto con lo anterior y autoprotegerse salvo conflicto con todo lo anterior”. Las tres leyes de Asimov configuran una propuesta sólida y autoconsistente, y cuentan con apoyo entre la comunidad de la inteligencia artificial, que reconoce, por ejemplo, que cualquier sistema autónomo funcional debe ser capaz de autoprotegerse.
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As raw data this format is fine, but to gain insight from this data, we will need to explicitly organize the attributes of our data that are key to our analysis. Our data should be in tabular, or &lt;a href=&#34;www.jstatsoft.org/v59/i10/paper&#34;&gt;‘tidy’ format&lt;/a&gt; where each row is an observation, or &lt;strong&gt;case&lt;/strong&gt; and each column, or &lt;strong&gt;variable&lt;/strong&gt; is a list of attributes of the observation. Each cell, then, is a particular attribute of a particular observation, or &lt;strong&gt;data point&lt;/strong&gt;. Say our objective is to perform an exploratory analysis to evaluate the potential similarities and differences in word frequencies between genres. For this particular analysis we will want to extract and organize the title of each document (&lt;code&gt;doc_id&lt;/code&gt;), the genre it is from (&lt;code&gt;genre&lt;/code&gt;), and each word (&lt;code&gt;word&lt;/code&gt;) as a single, row, or observation, in our tidy dataset.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:spanish-works&#34;&gt;Table 1: &lt;/span&gt;Tidy dataset&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;doc_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genre&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;es&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;son&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;peligro&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;guerra&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;lo&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;por&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;han&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;á&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;considerado&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;celoso&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;guide&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;esperan&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;teme&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;el&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This tidy organization may seem somewhat redundant; a single value for &lt;code&gt;doc_id&lt;/code&gt; is repeated for each value of &lt;code&gt;word&lt;/code&gt; and a single value of &lt;code&gt;genre&lt;/code&gt; is repeated for each value of &lt;code&gt;doc_id&lt;/code&gt;. However tidy data, although visually redundant, is an explicit description of the relationship between our variables. Each row corresponds to all of the necessary attributes to describe a particular observation. In this data, the occurrence of a word is associated with the file it appears in and the genre that file is associated with.&lt;/p&gt;
&lt;!-- TODO: consider the format that is required for doing a cluster analysis. Is my discussion leading in that direction, or is it complicated by potentially needing to create a term-document matrix? Look at the `tidytext` package vignette. Might need to change the discussion to focus on an inference analysis: assume a hypothesis that a specific register is more lexically diverse than another. Include genre as a factor? --&gt;
&lt;p&gt;Our objective in this toy example is to explore the relationship between word frequencies and genres, yet at this point there is no explicit variable for the frequencies of words. The information we need, however, is in the data and since we have an organized, tidy dataset, calculating &lt;code&gt;word_freq&lt;/code&gt; is a matter of tabulating the occurrences of each word. This can be done easily with R, as we will see in detail in future posts, but for our discussion on data organization let’s skip the details and jump to the new dataset with a column for &lt;code&gt;word_freq&lt;/code&gt;.&lt;/p&gt;
&lt;!-- example tabular data: doc_id, genre, word. Adding the word_freq column --&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:spanish-works-freq&#34;&gt;Table 2: &lt;/span&gt;Tidy dataset with &lt;code&gt;word_freq&lt;/code&gt; variable.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;doc_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genre&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;word_freq&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;la&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;559&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;el&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;376&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;en&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;309&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;la&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;y&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;los&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;to&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;314&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;217&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;_m&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;172&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Other measures and/or attributes can be added as necessary to this tabular format and in some cases we may convert our tidy tabular dataset to other data formats that may be required for some particular statistic approaches but at all times the relationship between the variables should be maintained in line with our research purpose. We will touch on examples of other types of data formats when we dive into particular statistical approaches that require them later in the series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;informational-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Informational value&lt;/h3&gt;
&lt;p&gt;Let’s turn now to the informational nature of our variables as it will set up how we implement our data analysis. Taking our variable &lt;code&gt;word_freq&lt;/code&gt; as an example, it is important to point out there are many ways to define ‘frequency’. Some frequency measures are more appropriate than others given the statistical approach we intend to apply to our data. Our current dataset contains raw frequency scores, that is the frequency is measured in observed counts for each word in each file of our data. We could, for example, instead bin our frequency scores under the labels “high” and “low” frequency converting frequency from counts to labels. In this case we change the &lt;strong&gt;informational value&lt;/strong&gt; of &lt;code&gt;word_freq&lt;/code&gt;. Some variables in our dataset, on the other hand, cannot be converted. Take for example, &lt;code&gt;genre&lt;/code&gt;. The values for &lt;code&gt;genre&lt;/code&gt; label the genre of the file from which the word was observed. We could of course summarize the genres under meta-genres, but we maintain labeled data; the same informational value as before.&lt;/p&gt;
&lt;p&gt;Understanding the informational value of variables in key to organizing and preparing your data for analysis as it has implications for what insight we can gain from the data and what visualization techniques and statistical measures we can use to interrogate the data. There are four potential informational values for all data: nominal, ordinal, interval, and ratio.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Nominal variables&lt;/em&gt; contain attributes which are labels denoting the membership in a class in which there is no relationship between the labels. Examples of nominal data include part-of-speech labels, the sex of a participant, the genre of a text, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Ordinal variables&lt;/em&gt; also contain labels of classes, but in contrast to nominal variables, there is a relationship between the classes, namely one in which there is a precedence relationship or rank. Our frequency conversion from scores to high- and low-frequency bins is a type of ordinal data –there is an explicit ordering of these two categories. Grouping participants in a study as “young”, “middle-aged”, and “old” would also be ordinal values; again, each value can be interpreted in relationship to the other values.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Interval variables&lt;/em&gt; are like ordinal variables in which there is an explicit precedence relationship, but in addition the values describe precise intervals between each value. So take our earlier operationalization of age as “young”, “middle-aged”, and “old”. As an ordinal variable no assumption is made that the differences in age between young and middle-aged are the same as between middle-aged and old –only that one class is ordered before or after another. If our criterion to code our values of age, however, were based regular intervals between age groups, not some non-regular assignment, then our values of age would be interval-valued.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Ratio variables&lt;/em&gt; have all the properties of interval variables but also include a non-arbitrary definition of zero. Frequency counts are ratio variables as it is clear that there is a potential value for 0 and any value greater can be interpreted in reference to this anchor. A word with a frequency of 100 is two times as large as a word with frequency 50. By the same token, a participant that is 20 years old is half the age of a 40 year old participant.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These informational types are often described in macro terms grouping nominal and ordinal variables as &lt;strong&gt;categorical variables&lt;/strong&gt; and interval and ratio variables as &lt;strong&gt;continous variables&lt;/strong&gt;. All continuous variables can be converted to categorical variables, but the reverse is not true. In most cases it is preferred to cast your data as continuous, if the nature of the variable permits it, as the recasting of continuous data to categorical data results in a loss of information –which will result in a loss of statistical power and may lead to results that obscure meaningful patterns in the data.&lt;/p&gt;
&lt;!-- The structure of raw, or original, data sampled varies  --&gt;
&lt;!-- * Informational value of variables --&gt;
&lt;!--   - In preparation for analysis, the research must be cognicent of the informational status of the variables of the data --&gt;
&lt;!--   - Variables are the observational features, attributes, and measures --&gt;
&lt;!--   - Ultimately visualizing and performing statistical operations on your data to analyze your data will depend on these variables and the information they contain. --&gt;
&lt;!--   - There are four informational types of variables: --&gt;
&lt;!--     * Nominal --&gt;
&lt;!--     * Ordinal --&gt;
&lt;!--     * Ratio --&gt;
&lt;!--     * Interval --&gt;
&lt;!--   - For convenience, we often talk about two major classes --&gt;
&lt;!--     * Categorical (nominal/ ordinal) --&gt;
&lt;!--     * Continuous (ratio/ interval) --&gt;
&lt;!--   - (Examples of real data sets) --&gt;
&lt;!-- * Operationalizing variables --&gt;
&lt;!--   - While some variables and their informational value are self-evident (such as sex, or age), some need to be interpreted and explicitly formulated --&gt;
&lt;!--   - Measurements --&gt;
&lt;!--     * Some variables contain information that needs to be calculated (frequencies, dispersion, etc.) or transformed (log, normalization (z-scores, etc.)) to be meaningful for analysis --&gt;
&lt;/div&gt;
&lt;div id=&#34;dependent-and-independent-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dependent and independent variables&lt;/h3&gt;
&lt;p&gt;The last step before we move to visualization and statistical tests is to identify our &lt;strong&gt;dependent variable&lt;/strong&gt; and/ or &lt;strong&gt;independent variables&lt;/strong&gt;. A dependent variable is the outcome variable that is used in inference and prediction analyses that reflects the observations of the behavior we want to gain understanding about. The identification of a dependent variable should be guided by your research question; it is the measure of the phenomenon in question. An independent variable is a predictor variable, or a variable which we assume will be related to the values of the dependent variable in some systematic way. There is typically only one dependent variable in an analysis, but there can be multiple independent variables. In an exploratory analysis, however, all the variables are independent variables as this approach assumes no particular relationship between the variables; the goal in this approach, remember, is to uncover patterns that may suggest a relationship between particular set of variables.&lt;/p&gt;
&lt;!-- * Dependent and independent variables --&gt;
&lt;!--   - Another key distiction to be made with our variables concerns the linking of our variables with the research question and statistical approach we intend to use to explore this question and it&#39;s predictions.  --&gt;
&lt;!--   - A dependent variable is the variable that is considered the outcome measurement to be understood.  --&gt;
&lt;!--   - Independent variables are those variables that we aim to use to understand the nature and variability of the dependent variable --&gt;
&lt;!--   - In hypothesis testing or prediction a dependent variable must be identified (in prediction this variable is also called the class or outcome variable) --&gt;
&lt;!--   - For exploration, where the goal is to discover patterns, the dependent variable is unknown and is the feature that we aim to discover --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data analysis&lt;/h2&gt;
&lt;p&gt;The primary goal of a data analysis is to reduce the observed data to a human-interpretable summary that best approximates the nature of the phenomenon we are investigating. With well-sampled data in a tidy dataset in hand where observations and variables are explicitly related, identified for their informational value, and the dependent and/or independent variables are clear, we can now proceed to visualizing and applying the appropriate statistical tests to the data to come to some more concrete, actionable insight.&lt;/p&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization&lt;/h3&gt;
&lt;p&gt;It is always key to gain insight into the behavior of the data visually before jumping in to the statistical analysis. Using our research aim as our guide, we will choose the most appropriate visualization to use given the number and informational value of our target variables. To get a sense of how this looks, let’s work with an example dataset and pose different questions to the data with an eye towards seeing how various combinations of variables are visualized.&lt;/p&gt;
&lt;p&gt;The dataset we will use here is from the &lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank repository&lt;/a&gt; which provides data from various language learning contexts.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The specific data we will use is the ‘narratives’ section of the &lt;a href=&#34;http://talkbank.org/access/SLABank/English/BELC.html&#34;&gt;BELC (Barcelona English Language Corpus)&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Muñoz 2006)&lt;/span&gt;. It is a corpus of writing samples from second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of “Me: my past, present and future”. Data was collected for many (but not all) participants up to four times over the course of seven years. The entire dataset includes 123 observations from 54 participants. Below I’ve included the first 10 observations from the dataset which reflects some data cleaning I’ve done so we start with a tidy dataset.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:view-belc-dataset&#34;&gt;Table 3: &lt;/span&gt;BELC dataset for demonstration.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;participant_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;learner_group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tokens&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The variables &lt;code&gt;participant_id&lt;/code&gt;, &lt;code&gt;sex&lt;/code&gt;, and &lt;code&gt;age&lt;/code&gt; should be self-explanatory. &lt;code&gt;learner_group&lt;/code&gt; contains the values 1-4 which record the stage for each participant formally learning English. The number of words written in each sample is listed for each participant at each stage in the variable &lt;code&gt;tokens&lt;/code&gt;. We should also note the informational value of these variables. &lt;code&gt;participant_id&lt;/code&gt;, &lt;code&gt;sex&lt;/code&gt;, and &lt;code&gt;learner_group&lt;/code&gt; are categorical variables; both &lt;code&gt;participant_id&lt;/code&gt; and &lt;code&gt;sex&lt;/code&gt; are nominal and &lt;code&gt;learner_group&lt;/code&gt; is ordinal. &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; are continuous variables; both of the ratio type as they are scaled in relation to a non-arbitrary value for zero.&lt;/p&gt;
&lt;p&gt;With general understanding of the data, let’s run through various data analysis scenarios and their corresponding visualizations grouping them by the information value of the dependent variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Categorical dependent variable&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Starting basic, let’s say we are interested in investigating the difference in the number of &lt;code&gt;males&lt;/code&gt; and &lt;code&gt;females&lt;/code&gt; in our study. This is not a particularly interesting question, but it allow us to illustrate a scenario in which we have a single dependent variable, &lt;code&gt;sex&lt;/code&gt;, which is categorical. When summarizing categorical data we produce counts of each of the levels of that variable. We can visualize this summary in one of two ways, textually and graphically. A text summary would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sex
female   male 
    67     56 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A graphic display does not necessarily facilitate a better understanding, in such a simple case, but let’s graphically visualize this scenario anyway. The type of plot we want to use is a ‘bar plot’, which simply plots the dependent variable on the x-axis and the counts on the y-axis.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-1.png&#34; alt=&#34;Bar plot of the categorical variable `sex`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Bar plot of the categorical variable &lt;code&gt;sex&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Inspecting these visualizations it is clear that there is a numeric difference between the number of writing samples in the data written by women. At this point, however, we only have a trend. To decide whether this is a reliable contrast is the purpose of our statistical tests, but we’ll leave the details of statistical testing for this scenario, and those that follow, for subsequent posts.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One categorical independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A more common scenario is one in which we have a categorical dependent variable and a categorical independent variable. With our data we can investigate the relationship between &lt;code&gt;sex&lt;/code&gt; and the &lt;code&gt;learner_group&lt;/code&gt;. Are there more males than females in a particular learner group? In this case both variables are categorical and the dimensions are such that we can textually represent them and gain some insight.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;        learner_group
sex       1  2  3  4
  female 20 24 13 10
  male   15 23 13  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s more difficult to see the pattern here than in the basic single dependent variable scenario for two reasons: 1) as the number of independent variables and/or the levels within an independent variable increase, our ability to interpret the results decreases. 2) the relationship between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt; does not take into account that there are more female samples than males, and therefore the raw counts here can be misleading.&lt;/p&gt;
&lt;p&gt;A graphic representation of this contrast will be a bit easier to interpret; although it is important to be aware that more variables and levels always leads to interpretability problems. The bar plot below reflects the raw counts from the cross-tabulation of the variables &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-graph-1.png&#34; alt=&#34;Bar plot of the variable `sex` and `learner_group`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Bar plot of the variable &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Adjusting the bar plot to account for the proportions of males to females in each group provides a clearer picture of the relationship between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-graph-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-graph-2-1.png&#34; alt=&#34;Bar plot of the variable `sex` and `learner_group` proportionally scaled.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Bar plot of the variable &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt; proportionally scaled.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From this visualization it appears that there are more females in the first and last learner groups.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Two categorical independent variables&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s look at a more complex case in which we have two categorical independent variables. Now the dataset, as is, does not have a third categorical variable for us to explore but we can recast the continuous &lt;code&gt;tokens&lt;/code&gt; variable as a categorical variable if we bin the scores into groups. I’ve binned &lt;code&gt;tokens&lt;/code&gt; into three score groups with equal ranges in a new variable called &lt;code&gt;token_bins&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:token-bins&#34;&gt;Table 4: &lt;/span&gt;&lt;code&gt;belc&lt;/code&gt; dataset with categorical &lt;code&gt;token_bins&lt;/code&gt; variable added.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;participant_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;learner_group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tokens&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;token_bins&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Adding a second categorical independent variable ups the complexity of our analysis and as a result our visualization strategy will change. As text our data will include individual two-way cross-tabulations for each of the levels for the third variable. In this case it is often best to use the variable with the fewest levels as the third variable.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;, , sex = female

             token_bins
learner_group low mid high
            1  18   2    0
            2  18   6    0
            3   9   4    0
            4   7   2    1

, , sex = male

             token_bins
learner_group low mid high
            1  13   2    0
            2  15   7    1
            3   8   5    0
            4   2   3    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To graphically visualize three categorical variables we turn to a mosaic plot.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-2-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-2-graph-1.png&#34; alt=&#34;Mosaic plot contrasting the categorical variables `sex`, `learner_group`, and `token_bins`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Mosaic plot contrasting the categorical variables &lt;code&gt;sex&lt;/code&gt;, &lt;code&gt;learner_group&lt;/code&gt;, and &lt;code&gt;token_bins&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From these visualizations we can see there is a general trend for the tokens from writing samples to increase in higher learner groups. There are some apparent divergent scores from this trend to be cautious of, however.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continuous dependent variable&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Working with a single continuous dependent variable means that the only practical way to summarize the data is graphically –as textual visualization will be very verbose and by and large uninterpretable. Plotting a single continuous variable often takes the form of a histogram which summarizes the frequency of the values of the dependent variable. So from our dataset, we may want to know what the distribution of &lt;code&gt;token&lt;/code&gt; scores looks like. That is, are they normally distributed (‘bell-shaped’), or skewed to the left or right (more values in the low or high range), or some other type of distribution (i.e. bi-modal, etc.)?&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-graph-1.png&#34; alt=&#34;Histogram (with and without a density line) for the continuous variable `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Histogram (with and without a density line) for the continuous variable &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The plot on the left is a standard histogram and the plot on the right is the same histogram with a density line added to highlight the distribution. From these plots we see that token counts are slightly left skewed. The longer tail to the right for higher token scores shows some evidence of outliers –that is, scores that are uncharacteristic of the general data distribution. For many analyses plotting a histogram is a key first step to identifying they type of statistical test to use on the data as certain test have assumptions about how the data should be distributed for their results to be reliable. For example, a class of tests called &lt;em&gt;parametric&lt;/em&gt; assume that continuous data is normally distributed (&lt;em&gt;non-parametric&lt;/em&gt; tests do not make this assumption). Having plotted the data we can see that it is probably not normally distributed. All is not lost, however. There are methods for &lt;em&gt;transforming&lt;/em&gt; the data that can often times take mildly skewed data and coerce it into a normal distribution.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One categorical independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another common data analysis scenario is one in which we have a continuous dependent variable and one categorical independent variable. Say we wanted to know the average number of &lt;code&gt;tokens&lt;/code&gt; used by men and by women. We would use the &lt;code&gt;tokens&lt;/code&gt; variable as our dependent variable and and &lt;code&gt;sex&lt;/code&gt; as the independent variable. We can visualize these means textually, as we are calculating the mean for each level of &lt;code&gt;sex&lt;/code&gt;,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  female     male 
57.14925 60.80357 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or graphically with a box plot, in which we get a host of information about the distribution.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-cat-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-cat-graph-1.png&#34; alt=&#34;Box plot summarizing the contrast between `sex` and `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Box plot summarizing the contrast between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you take a look at the results in the box plot you will see that the medians (the &lt;strong&gt;bold horizontal lines&lt;/strong&gt;) are not that different. Note that the mean and the median measure different things. And the mean is more sensitive to outliers –and the plot shows that there are some outliers in the male and female data. When we statistically analyze the data these types of outliers contribute to unexplained variation, or ‘noise’ at it is often called. Noise has the effect of reducing our confidence that the differences between populations are real, and not likely due to chance. We see much more on this later in the series.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One continuous independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The behavior of two continuous variables, one dependent and the other independent is represented using a scatter plot. The value for each variable for each observation is plotted as a coordinate pair. From this mapping we evaluate the relationship, or correlation, between the variables. Sticking with &lt;code&gt;tokens&lt;/code&gt; as our measure, let’s explore the extent to which &lt;code&gt;age&lt;/code&gt; conditions the number of tokens used by a participant.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From a visual inspection it appears that there is a slight effect for &lt;code&gt;age&lt;/code&gt; on &lt;code&gt;tokens&lt;/code&gt;, namely that the older the participant the more tokens they tend to use.&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; It is often helpful, and/ or necessary to add a trend line to the plot to help see the relationship more clearly. Note that the ribbon (in grey) surrounding the trend line is the ‘standard error’, or SE. The SE is a confidence interval suggesting that the trend line could have been drawn within any part of this space. You will note that a larger ribbon width corresponds with more variability. This is clearly the case for the tokens used by participants at age 14.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-2-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` with a trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; with a trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Another note on the trend line. The trend line drawn here is a non-linear, which is why you see the line is bendy. Often times when we are doing hypothesis testing we will be making the assumption that the relationship between two or more variables is linear, not non-linear. We can add a linear trend line to get a better understanding of the linear relationship between our variables.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-3-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` with a linear trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; with a linear trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interpreting the linear representation we see there is an apparent trend for increasing values for &lt;code&gt;tokens&lt;/code&gt; as &lt;code&gt;age&lt;/code&gt; increases.&lt;/p&gt;
&lt;p&gt;Say our aim was to understand the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; as a potential function of &lt;code&gt;sex&lt;/code&gt;. We can incorporate &lt;code&gt;age&lt;/code&gt; as a categorical variable. The result provides us a scatter plot with two trend lines, one for each level of &lt;code&gt;sex&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-4-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` by `sex` with a linear trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; by &lt;code&gt;sex&lt;/code&gt; with a linear trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From this visualization we see there is an apparent difference between males and females. Namely, males appear to increase their token production more than females of the same age. The SE ribbons here, however, are telling. Since they overlap we should be very cautious in interpreting the difference the trend line shows. Overlapping SE ribbons suggest the trend line could have been drawn in this space and therefore there is a likely probability that the visual difference will not result in a statistical difference. Again, this is another example of why visualization is such an integral first step in data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-tests&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Statistical tests&lt;/h3&gt;
&lt;p&gt;In the previous section various visualization strategies were illustrated through the lens of typical data analysis scenarios. To gain confidence that the trends in the data that we observe are reliable we submit the data to statistical tests. There are numerous tests available, too many to discuss at this point. But it is important to understand that much like our visualization choices, the test we choose depends on the number of variables we are investigating and the information values of these variables. However, particular statistical tests also potentially require a number of test specific assumptions (such as whether the data is normally distributed, for example). We will cover these on a case by case basis later on in the series.&lt;/p&gt;
&lt;!-- * Goal is to reduce the data into a summary of the data that best approximates the nature of the phenomenon that we are investigating.  --&gt;
&lt;!--   - To do this we start with visualization to gain an intuitive understanding of the distribution of the data and variables of interest. This exploratory process should be guided by your research question and statistical aims (hypothesis in hypothesis testing). It is fundamental to build up this graphic and mental picture of our data before moving to statistically evaluate the relationships of interest. --&gt;
&lt;!--   - (Examples) --&gt;
&lt;!--   - With an intuition of the data, awareness of the informational values of our variables, and having identified dependent and independent variables, we proceed to statistical analysis to develop a model that achieves these goals. Ultimately we want to know if the nature of the connections between our variables are reliable. That&#39;s what statistics will do for us. --&gt;
&lt;!--   - (Examples) --&gt;
&lt;!-- Keep these examples brief, as we will deal with data analysis head on later in the series.  --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we covered some foundational topics for any data analysis project. Guiding the entire process is a clear research question. From there we can proceed to acquire data that is relevant and reliable on the phenomenon we hope to understand better, choose a statistical approach which matches our analysis goals, and organize our data into a format conducive for achieving these goals. Only at this point can we confidently move to analyzing our data, first beginning with appropriate visualization techniques to get a feel for the trends and then moving to performing the relevant statistical tests to provide confidence that the trends captured in our visualizations are in fact reliable.&lt;/p&gt;
&lt;p&gt;There is still much left to discuss, in particular what statistical tests to apply to a given data analysis scenario and the assumptions behind statistical tests. We will address these topics later in the series when we have established a stronger and practical understanding of the preceding project steps and increase our proficiency programming in R. The next steps in this journey will be to learn about data types and sources and the specifics of acquiring data for language research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- OTHER APPROACHES TO THE TOPIC

GRIES, Statistics for Linguists with R
CHAPTER 1

* Design and logic of quantitative studies
  - Linguistics is an empirical science; thus understanding empirical methods and statistics is key to undertanding scientific and linguistic argumentation
  - Choosing a topic -consult the literature and get a feel for the area
* Reseach aims 
  - Hypotheses
  - Exploratory analysis
  - (Prediction -not covered but would logically fall here)
* Variables and measures
  - Operationalizing variables
  - Informational value (level of measurement)
  - Dependent and independent variables
    * Measure of dependent variable
    * Number of independent variables
* Data
  - Populations and samples (aiming for a balanced, representative sample)
  - Format for analysis (tidy, basically)


WICKHAM, R for Data Science
EXPLORE

* Data science workflow (overview)
* Data visualization (ggplot2, grammar of graphics fundamentals)
* Exploratory data analysis (EDA)
  - Variation
  - Covariation
  - Patterns and models (modelling concepts)

MODEL

* Hypothesis generating (exploration) vs. hypothesis confirmation (testing/ inference)
  - Observations may be used only once for inference, but multiple times for exploration
  - Modelling: creating a low-dimension summary of the dataset
  - Choosing a model (model selection)

JOHNSON, Quantitative Methods in Linguistics

Chapter 1:

* Goals of quantitative analyses
  - Data reduction
  - Inference (confirm a hypothesized relationship)
  - Exploration (discover relationships)
  - Explore processes (? not sure I like this; I would add Prediction (leverage/ harness relationships for predicting events) )
* Observations
  - What is an observation?
  - Informational values of observations (variables)
* Measures
  - Frequency distributions
  - Shapes of distributions
    * Uniform, skewed (right or left), bimodal, normal, J-shaped, U-shaped
  - Importance of the normal distribution
    * What is the normal distribution and why is it important?
      * Assumption for many statistical tests
    * How do we know if the data is normal?
    * Transforming data to fit the normal distribution
  - Measures of central tendency
    * Mean (arithmatic), median, and mode
  - Measure of dispersion
    * Variance
    * Standard deviation

Chapter 2:

* Sampling
  - What makes a good sample?
* Data
  - Observations
  - Good samples are critical to a sound analysis (your findings cannot be more reliable than your data!)
* Hypothesis testing
  - Central Limit Theorem
  - Null and alternative hypotheses
  - Type I and Type II error
  - Correlation
    
BAAYEN, Analyzing linguistic data: A practical introduction to statistics using R

* Not (obviously) a good source for intro statistical thinking

STANTON, An Introduction to Data Science

Data Science: Many Skills
* 

GRIES, Quantitative Corpus Linguistics with R

Chapter 5: Some statistics for Corpus Linguistics

* Intro to statistical thinking
  - Variables and their roles in an analysis
  - Variables and their informational value
  - Hypotheses: formulation and operationalization
  - Data analysis
* Categorical dependent variables
  - 1 DV, no IV
  - 1 DV, 1 IV (categorical)
  - 1 DV, 2+ IV (categorical)
* Interval/ ratio-scaled dependent variables
  - 1 DV, no IV
  - 1 DV, 1 IV (categorical)
  - 1 DV, 1 IV (interval/ratio)
  - 1 DV, 2+ IV (mixed)

--&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Gries2013a&#34;&gt;
&lt;p&gt;Gries, ST. 2013. &lt;em&gt;Statistics for Linguistics with R. A Practical Introduction&lt;/em&gt;. 2nd revise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Johnson:2008&#34;&gt;
&lt;p&gt;Johnson, K. 2008. &lt;em&gt;Quantitative methods in linguistics&lt;/em&gt;. Blackwell Pub.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Munoz2006&#34;&gt;
&lt;p&gt;Muñoz, Carme, ed. 2006. &lt;em&gt;Age and the rate of foreign language learning&lt;/em&gt;. Clevedon: Multilingual Matters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wickham2017&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Garrett Grolemund. 2017. &lt;em&gt;R for Data Science&lt;/em&gt;. First edit. O’Reilly Media. &lt;a href=&#34;http://r4ds.had.co.nz/&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The &lt;a href=&#34;http://talkbank.org/&#34;&gt;HomeBank&lt;/a&gt; section of TalkBank is restricted to members only. Membership is free but you will need to contact the repository and request membership.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that I’ve added a ‘jitter’ to the data points in this scatter plot to avoid overplotting.&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
