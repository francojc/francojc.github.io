<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Repositories on francojc ⟲</title>
    <link>https://francojc.github.io/tags/repositories/</link>
    <description>Recent content in Repositories on francojc ⟲</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jerid Francom</copyright>
    <lastBuildDate>Fri, 20 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/repositories/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Acquiring data for language research (1/3): direct downloads</title>
      <link>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There are three main ways to acquire corpus data using R that I will introduce you to: &lt;strong&gt;direct download&lt;/strong&gt;, &lt;strong&gt;package interfaces&lt;/strong&gt;, and &lt;strong&gt;web scraping&lt;/strong&gt;. In this post we will start by directly downloading a corpus as it is the most straightforward process for the novice R programmer and incurs the least number of steps. Along the way I will introduce some key R coding concepts including control statements and custom functions.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The following code is available on GitHub &lt;code&gt;recipes-acquiring_data&lt;/code&gt; and is built on the &lt;code&gt;recipes-project_template&lt;/code&gt; I have discussed in detail &lt;a href=&#34;https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/&#34;&gt;here&lt;/a&gt; and made accessible &lt;a href=&#34;https://github.com/francojc/recipes-project_template.git&#34;&gt;here&lt;/a&gt;. I encourage you to follow along by downloading the &lt;code&gt;recipes-project_template&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; from the Terminal or create a new RStudio R Project and select the “Version Control” option.&lt;/p&gt;

&lt;/div&gt;

&lt;!-- TODO: add {#anchor} below --&gt;
&lt;div id=&#34;direct-downloads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Direct downloads&lt;/h2&gt;
&lt;p&gt;Published corpus data found in repositories or individual sources are usually the easiest to start working with as it is generally a matter of identifying a resource to download and then downloading it with R. OK, there’s a little more involved, but that’s the basic idea.&lt;/p&gt;
&lt;p&gt;Let’s take a look at how this works starting with the a sample from the Switchboard Corpus, a corpus of 2,400 telephone conversations by 543 speakers. First we navigate to the site with a browser and download the file that we are looking for. In this case I found the Switchboard Corpus on the &lt;a href=&#34;http://www.nltk.org/nltk_data/&#34;&gt;NLTK data repository site&lt;/a&gt;. More often than not this file will be some type of compressed archive file with an extension such as &lt;code&gt;.zip&lt;/code&gt; or &lt;code&gt;.tz&lt;/code&gt;, which is the case here. Archive files make downloading multiple files easy by grouping files and directories into one file. In R we can used the &lt;code&gt;download.file()&lt;/code&gt; function from the base R library&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. There are a number of &lt;strong&gt;arguments&lt;/strong&gt; that a function may require or provide optionally. The &lt;code&gt;download.file()&lt;/code&gt; function minimally requires two: &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;destfile&lt;/code&gt;. That is the file to download and the location where it is to be saved to disk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download .zip file and write to disk
download.file(url = &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;, destfile = &amp;quot;data/original/switchboard.zip&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once an archive file is downloaded, however, the file needs to be ‘decompressed’ to reveal the file structure. The file we downloaded is located on our disk at &lt;code&gt;data/original/switchboard.zip&lt;/code&gt;. To decompress this file we use the &lt;code&gt;unzip()&lt;/code&gt; function with the arguments &lt;code&gt;zipfile&lt;/code&gt; pointing to the &lt;code&gt;.zip&lt;/code&gt; file and &lt;code&gt;exdir&lt;/code&gt; specifying the directory where we want the files to be extracted to.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;I encourage you to use the &lt;code&gt;TAB&lt;/code&gt; key to expand the list of options of a function to avoid having to remember the arguments of a function and also to avoid typos. After typing the name of the function and opening &lt;code&gt;(&lt;/code&gt; hit &lt;code&gt;TAB&lt;/code&gt; to view and select the argument(s) you want. Furthermore, the &lt;code&gt;TAB&lt;/code&gt; key can also help you expand paths to files and directories. Note that the expansion will default to the current working directory.&lt;/p&gt;

&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Decompress .zip file and extract to our target directory
unzip(zipfile = &amp;quot;data/original/switchboard.zip&amp;quot;, exdir = &amp;quot;data/original/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The directory structure of &lt;code&gt;data/&lt;/code&gt; now should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/
├── derived
└── original
    ├── switchboard
    │   ├── README
    │   ├── discourse
    │   ├── disfluency
    │   ├── tagged
    │   ├── timed-transcript
    │   └── transcript
    └── switchboard.zip

3 directories, 7 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have acquired the data programmatically and with this code as part of our workflow anyone could run this code and reproduce the same results. The code as it is, however, is not ideally efficient. Firstly the &lt;code&gt;switchboard.zip&lt;/code&gt; file is not strictly needed after we decompress it and it occupies disk space if we keep it. And second, each time we run this code the file will be downloaded from the remote serve leading to unnecessary data transfer and server traffic. Let’s tackle each of these issues in turn.&lt;/p&gt;
&lt;p&gt;To avoid writing the &lt;code&gt;switchboard.zip&lt;/code&gt; file to disk (long-term) we can use the &lt;code&gt;tempfile()&lt;/code&gt; function to open a temporary holding space for the file. This space can then be used to store the file, unzip it, and then the temporary file will be destroyed. We assign the temporary space to an R object we will name &lt;code&gt;temp&lt;/code&gt; with the &lt;code&gt;tempfile()&lt;/code&gt; function. This object can now be used as the value of the argument &lt;code&gt;destfile&lt;/code&gt; in the &lt;code&gt;download.file()&lt;/code&gt; function. Let’s also assign the web address to another object &lt;code&gt;url&lt;/code&gt; which we will use as the value of the &lt;code&gt;url&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a temporary file space for our .zip file
temp &amp;lt;- tempfile()
# Assign our web address to `url`
url &amp;lt;- &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;
# Download .zip file and write to disk
download.file(url, temp)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;In the previous code I’ve used the values stored in the objects &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;temp&lt;/code&gt; in the &lt;code&gt;download.file()&lt;/code&gt; function without specifying the argument names –only providing the names of the objects. R will assume that values of a function map to the ordering of the arguments. If your values do not map to ordering of the arguments you are required to specify the argument name and the value. To view the ordering of objects hit &lt;code&gt;TAB&lt;/code&gt; after entering the function name or consult the function documentation by prefixing the function name with &lt;code&gt;?&lt;/code&gt; and hitting &lt;code&gt;ENTER&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;At this point our downloaded file is stored temporarily on disk and can be accessed and decompressed to our target directory using &lt;code&gt;temp&lt;/code&gt; as the value for the argument &lt;code&gt;zipfile&lt;/code&gt; from the &lt;code&gt;unzip()&lt;/code&gt; function. I’ve assigned our target directory path to &lt;code&gt;target_dir&lt;/code&gt; and used it as the value for the argument &lt;code&gt;exdir&lt;/code&gt; to prepare us for the next tweak on our approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Assign our target directory to `target_dir`
target_dir &amp;lt;- &amp;quot;data/original/&amp;quot;
# Decompress .zip file and extract to our target directory
unzip(zipfile = temp, exdir = target_dir)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/
├── derived
└── original
    └── switchboard
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

3 directories, 6 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second issue I raised concerns the fact that running this code as part of our project will repeat the download each time. Since we would like to be good citizens and avoid unnecessary traffic on the web it would be nice if our code checked to see if we already have the data on disk and if it exists, then skip the download, if not then download it. To achieve this we need to introduce two new functions &lt;code&gt;if()&lt;/code&gt; and &lt;code&gt;dir.exists()&lt;/code&gt;. &lt;code&gt;dir.exists()&lt;/code&gt; takes a path to a directory as an argument and returns the logical value, &lt;code&gt;TRUE&lt;/code&gt;, if that directory exists, and &lt;code&gt;FALSE&lt;/code&gt; if it does not. &lt;code&gt;if()&lt;/code&gt; evaluates logical statements and processes subsequent code based on the logical value it is passed as an argument. Let’s look at a toy example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num &amp;lt;- 1
if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1 is 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I assigned &lt;code&gt;num&lt;/code&gt; to the value &lt;code&gt;1&lt;/code&gt; and created a logical evaluation &lt;code&gt;num ==&lt;/code&gt; whose result is passed as the argument to &lt;code&gt;if()&lt;/code&gt;. If the statement returns &lt;code&gt;TRUE&lt;/code&gt; then the code withing the first set of curly braces &lt;code&gt;{...}&lt;/code&gt; is run. If &lt;code&gt;num == 1&lt;/code&gt; is false, like in the code below, the code withing the braces following the &lt;code&gt;else&lt;/code&gt; will be run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num &amp;lt;- 2
if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;if()&lt;/code&gt; is one of various functions that are called &lt;strong&gt;control statements&lt;/strong&gt;. Theses functions provide a lot of power to make dynamic choices as code is run.&lt;/p&gt;
&lt;p&gt;Before we get back to our key objective to avoid downloading resources that we already have on disk, let me introduce another strategy to making code more powerful and ultimately more efficient and as well as more legible –the &lt;strong&gt;custom function&lt;/strong&gt;. Custom functions are functions that the user writes to create a set of procedures that can be run in similar contexts. I’ve created a custom function named &lt;code&gt;eval_num()&lt;/code&gt; below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num &amp;lt;- function(num) {
  if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look at what’s going on here. The function &lt;code&gt;function()&lt;/code&gt; creates a function in which the user decides what arguments are necessary for the code to perform its task. In this case the only necessary argument is the object to store a numeric value to be evaluated. I’ve called it &lt;code&gt;num&lt;/code&gt; because it reflects the name of the object in our toy example, but there is nothing special about this name. It’s only important that the object names be consistently used. I’ve included our previous code (except for the hard-coded assignment of &lt;code&gt;num&lt;/code&gt;) inside the curly braces and assigned the entire code chunk to &lt;code&gt;eval_num&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now use the function &lt;code&gt;eval_num()&lt;/code&gt; to perform the task of evaluating whether a value of &lt;code&gt;num&lt;/code&gt; is or is not equal to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1 is 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve put these coding strategies together with our previous code in a function I named &lt;code&gt;get_zip_data()&lt;/code&gt;. There is a lot going on here. Take a look first and see if you can follow the logic involved given what you now know.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_zip_data &amp;lt;- function(url, target_dir) {
  # Function: to download and decompress a .zip file to a target directory
  
  # Check to see if the data already exists
  if(!dir.exists(target_dir)) { # if data does not exist, download/ decompress
    cat(&amp;quot;Creating target data directory \n&amp;quot;) # print status message
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&amp;quot;Downloading data... \n&amp;quot;) # print status message
    temp &amp;lt;- tempfile() # create a temporary space for the file to be written to
    download.file(url = url, destfile = temp) # download the data to the temp file
    unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE) # decompress the temp file in the target directory
    cat(&amp;quot;Data downloaded! \n&amp;quot;) # print status message
  } else { # if data exists, don&amp;#39;t download it again
    cat(&amp;quot;Data already exists \n&amp;quot;) # print status message
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK. You should have recognized the general steps in this function: the argument &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;target_dir&lt;/code&gt; specify where to get the data and where to write the decompressed files, the &lt;code&gt;if()&lt;/code&gt; statement evaluates whether the data already exists, if not (&lt;code&gt;!dir.exists(target_dir)&lt;/code&gt;) then the data is downloaded and decompressed, if it does exist (&lt;code&gt;else&lt;/code&gt;) then it is not downloaded.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The prefixed &lt;code&gt;!&lt;/code&gt; in the logical expression &lt;code&gt;dir.exists(target_dir)&lt;/code&gt; returns the opposite logical value. This is needed in this case so when the target directory exists, the expression will return &lt;code&gt;FALSE&lt;/code&gt;, not &lt;code&gt;TRUE&lt;/code&gt;, and therefore not proceed in downloading the resource.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;There are a couple key tweaks I’ve added that provide some additional functionality. For one I’ve included the function &lt;code&gt;dir.create()&lt;/code&gt; to create the target directory where the data will be written. I’ve also added an additional argument to the &lt;code&gt;unzip()&lt;/code&gt; function, &lt;code&gt;junkpaths = TRUE&lt;/code&gt;. Together these additions allow the user to create an arbitrary directory path where the files, and only the files, will be extracted to on our disk. This will discard the containing directory of the &lt;code&gt;.zip&lt;/code&gt; file which can be helpful when we want to add multiple &lt;code&gt;.zip&lt;/code&gt; files to the same target directory.&lt;/p&gt;
&lt;p&gt;A practical scenario where this applies is when we want to download data from a corpus that is contained in multiple &lt;code&gt;.zip&lt;/code&gt; files but still maintain these files in a single primary data directory. Take for example the &lt;a href=&#34;http://www.linguistics.ucsb.edu/research/santa-barbara-corpus&#34;&gt;Santa Barbara Corpus&lt;/a&gt;. This corpus resource includes a series of interviews in which there is one &lt;code&gt;.zip&lt;/code&gt; file, &lt;code&gt;SBCorpus.zip&lt;/code&gt; which contains the &lt;a href=&#34;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip&#34;&gt;transcribed interviews&lt;/a&gt; and another &lt;code&gt;.zip&lt;/code&gt; file, &lt;code&gt;metadata.zip&lt;/code&gt; which organizes the &lt;a href=&#34;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip&#34;&gt;meta-data&lt;/a&gt; associated with each speaker. Applying our initial strategy to download and decompress the data will lead to the following directory structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── SBCorpus
    │   ├── TRN
    │   └── __MACOSX
    │       └── TRN
    └── metadata
        └── __MACOSX

8 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By applying our new custom function &lt;code&gt;get_zip_data()&lt;/code&gt; to the transcriptions and then the meta-data we can better organize the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download corpus transcriptions
get_zip_data(url = &amp;quot;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip&amp;quot;, target_dir = &amp;quot;data/original/sbc/transcriptions/&amp;quot;)

# Download corpus meta-data
get_zip_data(url = &amp;quot;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip&amp;quot;, target_dir = &amp;quot;data/original/sbc/meta-data/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now our &lt;code&gt;data/&lt;/code&gt; directory is better organized; both the transcriptions and the meta-data are housed under &lt;code&gt;data/original/sbc/&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    └── sbc
        ├── meta-data
        └── transcriptions

5 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we add data from other sources we can keep them logical separate and allow our data collection to scale without creating unnecessary complexity. Let’s add the Switchboard Corpus sample using our &lt;code&gt;get_zip_data()&lt;/code&gt; function to see this in action.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download corpus
get_zip_data(url = &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;, target_dir = &amp;quot;data/original/scs/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our corpora our housed in their own directories and the files are clearly associated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs

6 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have what we need to continue to the next step in our data analysis project. But before we go, we should do some housekeeping to document and organize this process to make our work reproducible. We will take advantage of the &lt;code&gt;project-template&lt;/code&gt; directory structure, seen below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── README.md
├── _pipeline.R
├── code
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data
│   ├── derived
│   └── original
├── figures
├── functions
├── log
├── recipes-acquire-data.Rproj
└── report
    ├── article.Rmd
    ├── bibliography.bib
    ├── slides.Rmd
    └── web.Rmd

8 directories, 13 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First it is good practice to separate custom functions from our processing scripts. We can create a file in our &lt;code&gt;functions/&lt;/code&gt; directory named &lt;code&gt;acquire_functions.R&lt;/code&gt; and add our custom function &lt;code&gt;get_zip_data()&lt;/code&gt; there. We then use the &lt;code&gt;source()&lt;/code&gt; function to read that function into our current script to make it available to use as needed. It is good practice to source your functions in the SETUP section of your script.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load custom functions for this project
source(file = &amp;quot;functions/acquire_functions.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second it is advisable to log the structure of the data in plain text files. You can create a directory tree (as those seen in this post) with the bash command &lt;code&gt;tree&lt;/code&gt; on the command line. R provides a function &lt;code&gt;system()&lt;/code&gt; which will interface the command line. Adding the following code to the LOG section of your &lt;code&gt;acquire_data.R&lt;/code&gt; R script will generate the directory structure for each of the corpora that we have downloaded in this post in the files &lt;code&gt;data_original_sbc.log&lt;/code&gt; and &lt;code&gt;data_original_scs.log&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log the directory structure of the Santa Barbara Corpus
system(command = &amp;quot;tree data/original/sbc &amp;gt;&amp;gt; log/data_original_sbc.log&amp;quot;)
# Log the directory structure of the Switchboard Corpus sample
system(command = &amp;quot;tree data/original/scs &amp;gt;&amp;gt; log/data_original_scs.log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our project directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── README.md
├── _pipeline.R
├── code
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data
│   ├── derived
│   └── original
├── figures
├── functions
│   └── acquire_functions.R
├── log
│   ├── data_original_sbc.log
│   └── data_original_scs.log
├── recipes-acquire-data.Rproj
└── report
    ├── article.Rmd
    ├── bibliography.bib
    ├── slides.Rmd
    └── web.Rmd

8 directories, 15 files&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we’ve covered how to access, download, and organize data contained in .zip files; the most common format for language data found on repositories and individual sites. This included an introduction to a few key R programming concepts and strategies including using functions, writing custom functions, and controlling program flow with control statements. Our approach was to gather data while also keeping in mind the reproducibility of the code. To this end I introduced programming strategies for avoiding unnecessary web traffic (downloads), scalable directory creation, and data documentation.&lt;/p&gt;
&lt;p&gt;In the next post in this three part mini-series I will cover acquiring data from web services such as Project Gutenberg, Twitter, and Facebook through R packages. Using package interfaces will require additional knowledge of R objects. I will discuss vector types and data frames and show how to manipulate these objects in practical situations like filtering data and writing data to disk in plain-text files.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Remember base R packages are installed by default with R and are loaded and accessible by default in each R session.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data for language research -types and sources</title>
      <link>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</guid>
      <description>&lt;p&gt;In this Recipe you will learn about the types of data available for language research and where to find data. The goal, then, is to introduce you to the landscape of language data available and provide a general overview of the characteristics of language data from a variety of sources providing you with resources to begin your own quantitative investigations.&lt;/p&gt;
&lt;div id=&#34;data-for-language-research&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data for language research&lt;/h2&gt;
&lt;p&gt;Language research can include data from a variety of sources, linguistic and non-linguistic, that record observations about the world. A typical type of data used in quantitative language research is a &lt;strong&gt;corpus&lt;/strong&gt;. In short, a corpus is set of machine-readable texts that have been compiled with an eye towards linguistic research. All corpora are not created equal, however, in content and/or format. A corpus may aim to represent a wide swath of language behavior or very specific aspects. It can be language specific (English or French), target a particular modality (spoken or written), or approximate domains of language use (medicine, business, etc.). A corpus that aims to represent a language (including modalities, registers, and sub-domains), for example, is known as a &lt;em&gt;generalized corpus&lt;/em&gt;. Corpora that aim to capture a snapshot of a particular modality or sub-domain of language use are known as &lt;em&gt;specialized corpora&lt;/em&gt;. Each corpus will have an underlying target population and the sampling process will reflect the authors’ best attempt (given the conditions at the point the corpus was compiled) at representing the stated target population. Whether a corpus is generalized or specialized can become difficult to nail down between the extremes. As such, it is key to be clear about the scope of a particular corpus to be able to ascertain its potential applications and gauge the extent to which these applications entail the research goals of your particular project.&lt;/p&gt;
&lt;p&gt;A corpus will often include various types of non-linguistic attributes, or &lt;em&gt;meta-data&lt;/em&gt;, as well. Ideally this will include information regarding the source(s) of the data, dates when it was acquired or published, and other author or speaker information. It may also include any number of other attributes that were identified as potentially important in order to appropriately document the target population. Again, it is key to match the available meta-data with the goals of your research. In some cases a corpus may be ideal in some aspects but not contain all the key information to address your research question. This may mean you will need to compile your own corpus if there are fundamental attributes missing. Before you consider compiling your own corpus, however, it is worth investigating the possibility of augmenting an available corpus to bring it inline with your particular goals. This may include adding new language sources, harnessing software for linguistic annotation (part-of-speech, syntactic structure, named entities, etc.), or linking available corpus meta-data to other resources, linguistic or non-linguistic.&lt;/p&gt;
&lt;p&gt;Corpora come in various formats, the main three being: running text, structured documents, and databases. The format of a corpus is often influenced by characteristics of the data but may also reflect an author’s individual preferences as well. It is typical for corpora with few meta-data characteristics to take the form of running text. In corpora with more meta-data a header may be appended to the top of each running text document or the meta-data may be contained in a separate file with appropriate coding to coordinate meta-data attributes with each text in the corpus. When meta-data increases in complexity it is common to structure each corpus document more explicitly with a markup language such as XML (Extensible Markup Language) or organize relationships between language and meta-data attributes in a database. Although there has been a push towards standardization of corpus formats, most available resources display some degree of idiosyncrasy. Being able to parse the structure of a corpus is a skill that will develop with time. With more experience working with corpora you will become more adept at identifying how the data is stored and whether its content and format will serve the needs of your analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sources-of-language-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources of language data&lt;/h2&gt;
&lt;p&gt;The most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes and language used in natural settings that can be coerced by the investigator into a corpus. Many organizations exist around the globe that provide access to corpora in browsable catalogs, or &lt;strong&gt;repositories&lt;/strong&gt;. There are repositories dedicated to language research, in general, such as the &lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Language Data Consortium&lt;/a&gt; or for specific language domains, such as the language acquisition repository &lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication. A standardized resource many times will be easier to interpret and evaluate for its appropriateness for a particular research project.&lt;/p&gt;
&lt;p&gt;In the table below I’ve compiled a list of some corpus repositories to help you get started.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-repositories&#34;&gt;Table 1: &lt;/span&gt;A list of some language corpora repositories.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus.byu.edu/&#34;&gt;BYU corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A repository of corpora that includes billions of words of data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://corporafromtheweb.org/&#34;&gt;COW (COrpora from the Web)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of linguistically processed gigatoken web corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://wortschatz.uni-leipzig.de/en/download/&#34;&gt;Leipzig Corpora Collection&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpora in different languages using the same format and comparable sources.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Linguistic Data Consortium&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.resourcebook.eu/searchll.php#&#34;&gt;LRE Map&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.nltk.org/nltk_data/&#34;&gt;NLTK language data&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of corpora and language datasets included with the Python package NLTK.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/&#34;&gt;OPUS - an open source parallel corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of translated texts from the web.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus1.mpi.nl/ds/asv/?4&#34;&gt;The Language Archive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Various corpora and language datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://ota.ox.ac.uk/&#34;&gt;The Oxford Text Archive (OTA)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of thousands of texts in more than 25 different languages.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Repositories are by no means the only source of corpora on the web. Researchers from around the world provide access to corpora and other data sources on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. Finding these resources is a matter of doing a web search with the word ‘corpus’ and a list of desired attributes, including language, modality, register, etc. As part of a general movement towards reproducible more corpora are available on the web than ever before. Therefore data sharing platforms supporting reproducible research, such as &lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;, &lt;a href=&#34;http://www.re3data.org/&#34;&gt;Re3data&lt;/a&gt;, etc., are a good place to look as well, if searching repositories and targeted web searches do not yield results.&lt;/p&gt;
&lt;p&gt;In the table below you will find a list of corpus resources and datasets.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-corpora&#34;&gt;Table 2: &lt;/span&gt;Corpora and language datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://storage.googleapis.com/books/ngrams/books/datasetsv2.html&#34;&gt;Google Ngram Viewer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Google web corpus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~enron/&#34;&gt;Enron Email Dataset&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Enron email data from about 150 users, mostly senior management.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://cesa.arizona.edu/&#34;&gt;Corpus of Spanish in Southern Arizona&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Spanish varieties spoken in Arizona.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/OpenSubtitles_v2.php&#34;&gt;OpenSubtitles2011&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of documents from &lt;a href=&#34;http://www.opensubtitles.org/&#34; class=&#34;uri&#34;&gt;http://www.opensubtitles.org/&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.statmt.org/europarl/&#34;&gt;Europarl Parallel Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A parallel corpus based on the proceedings of the European Parliament&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.lllf.uam.es/~fmarcos/informes/corpus/coarginl.html&#34;&gt;Corpus Argentino&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpus of Argentine Spanish&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.ruscorpora.ru/en/&#34;&gt;Russian National Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus of modern Russian language incorporating over 300 million words.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html&#34;&gt;Cornell Movie-Dialogs Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is important to note that there can be access and use restrictions for data from particular sources. Compiling, hosting, and maintaining corpus resources can be costly. To gain full access to data, some repositories and homepages of larger corpora require a fee to offset these costs. In other cases, resources may require individual license agreements to ensure that the data is not being used in ways it was not intended or to ensure potentially sensitive participant information will be treated appropriately. You can take a look at a &lt;a href=&#34;https://www.corpusdata.org/restrictions.asp&#34;&gt;license agreement for the BYU Corpora&lt;/a&gt; as an example. If you are a member of an academic institution and aim to conduct research for scholarly purposes licensing is often easily obtained. Fees, on the other hand, may present a more challenging obstacle. If you are an affiliate of an academic institution it is worth checking with your library to see if there are funds for acquiring licensing for you as an individual, a research group or lab or, for the institution.&lt;/p&gt;
&lt;p&gt;If your corpus search ends in a dead-end, either because a suitable resource does not appear to exist or an existing resource is unattainable given licensing restrictions or fees, it may be time to compile your own corpus. Turning to machine readable texts on the internet is usually the logical first step to access language for a new corpus. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on social media, news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by hand instead of automating the task. Our approach here, however, will be to automate the process as much as possible whether that means leveraging R package interfaces to language data, converting hundreds of pdf documents to plain text, or scraping content from web documents.&lt;/p&gt;
&lt;p&gt;The table below lists some R packages that serve to interface language data directly through R.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-apis&#34;&gt;Table 3: &lt;/span&gt;R Package interfaces to language corpora and datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/crminer&#34;&gt;crminer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface focusing on getting the user full text via the Crossref search API.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/arxiv_tutorial.html&#34;&gt;aRxiv&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/internetarchive_tutorial.html&#34;&gt;internetarchive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/dvn&#34;&gt;dvn&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to access to the Dataverse Network APIs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/gutenbergr_tutorial.html&#34;&gt;gutenbergr&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to download and process public domain works from the Project Gutenberg collection.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/fulltext_tutorial.html&#34;&gt;fulltext&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query open access journals, such as PLOS.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/hrbrmstr/newsflash&#34;&gt;newsflash&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive and GDELT Television Explorer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/oai&#34;&gt;oai&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query any OAI-PMH repository, including Zenodo.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/rfigshare&#34;&gt;rfigshare&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the data sharing platform FigShare.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Data for language research is not limited to (primary) text sources. Other sources may include processed data from previous research; word lists, linguistic features, etc.. Alone or in combination with text sources this data can be a rich and viable source of data for a research project.&lt;/p&gt;
&lt;p&gt;Below I’ve included some processed language resources.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-experimental&#34;&gt;Table 4: &lt;/span&gt;Language data from previous research and meta-studies.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/lingtypology&#34;&gt;lingtypology&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://elexicon.wustl.edu/WordStart.asp&#34;&gt;English Lexicon Project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://icon.shef.ac.uk/Moby/&#34;&gt;The Moby lexicon project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Language wordlists and resources from the Moby project.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The list of data available for language research is constantly growing. I’ve document very few of the wide variety of resources. Below I’ve included attempts by others to provide a summary of the corpus data and language resources available.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-listing&#34;&gt;Table 5: &lt;/span&gt;Lists of corpus resources.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html&#34;&gt;Learner corpora around the world&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A listing of learner corpora around the world&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/&#34;&gt;Where can you find language data on the web?&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of various corpora and language datasets.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://nlp.stanford.edu/links/statnlp.html#Corpora&#34;&gt;Stanford NLP corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of corpora and language resources aimed at the NLP community.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we have covered some of the basics on data for language research including types of data and sources to get you started on the path of identifying a viable source for your data analysis project. In the next post we will begin working directly with R code to access and acquire data through R. Along the way I will introduce fundamental programming concepts of the language you will use throughout your project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- Topics:

- natural and naturalistic
modality, register (formal/ informal), 
  - spoken (Callhome -LDC, ...)
  - written (literature -gutenbergr, periodicals -LDC, blogs, social media -streamR, ...)
  - other (tv/film closed captions -newsflash, Brysbaert/ ACTIV-ES, translation -OPEL?, ...)
- elicited data
  - essays (-BELC, )
  - interviews (Santa Barbara Corpus)
  - surveys (US Census -acs, Harvard Dialect Survey, Language attitude)
  - experimental findings
    - response times (MRC lexicon), eye-gaze, acceptability ratings, etc. 
    
- 


- Types of data
  - Uncurated
  - Curated 
- Sources of data
    - Repositories
    - Sources
    - Web
- Data formats
  - Plain text
    - .txt
    - .csv/ .tsv
    - .xml/ .json
  - Other files
    - .doc(x)
    - .pdf
  - Databases
- How to acquire data
    - Package interface (`gutenbergr`)
    - API interface (`streamR`)
    - Download, read (multiple files, w/ lapply and scan?
    - Web scraping (`rvest`)
--&gt;
&lt;!-- ## Types of data --&gt;
&lt;!-- In the last post we discussed what data is and the importance of data sampling and organization is for subsequent data analysis. We touched briefly on an example in which we were working with files which our language data was in running text format. Running text is one of the types of data that you will encounter when you look to obtain data to conduct research into the topic you are interested in knowing something more about. In this post we leared that text in this format needed to be organized into a format that was more conducive for statistical analysis. The aim, then, was to **curate** the **uncurated** data. Athough data is often talked about in terms of being curated or uncurated, it is important to understand that this is less a dicotomy and more of a continuum. Since each analysis has specific goals the primary data is always in need of some amount of curation to prepare the data in the particular ways necessary to faciliate the particular goals of the particular analysis.   --&gt;
&lt;!-- ## Sources of data --&gt;
&lt;!-- Having said that it is convenient to talk about curation in binary terms as it facilitates a discussion of the sources of data out there and to what degree the researcher needs to manipulate the data from particular sources to be able to conduct an analysis. When looking at the landscape of language data available, sources that have been prepared to some degree to facilitate analysis are often found in *repositories*. There are countless repositories that can be accessed on the web.  --&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-mcenery:2012&#34;&gt;
&lt;p&gt;McEnery, Tony, and Andrew Hardie. 2012. &lt;em&gt;Corpus Linguistics: Method, Theory and Practice&lt;/em&gt;. Cambridge University Press. doi:&lt;a href=&#34;https://doi.org/10.1017/CBO9780511981395&#34;&gt;10.1017/CBO9780511981395&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
