<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on francojc ⟲</title>
    <link>https://francojc.github.io/post/</link>
    <description>Recent content in Posts on francojc ⟲</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2018 by Jerid Francom</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://francojc.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Curate language data (1/2): organizing meta-data</title>
      <link>https://francojc.github.io/2017/12/01/curate-language-data-organizing-meta-data/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/12/01/curate-language-data-organizing-meta-data/</guid>
      <description>When working with raw data, whether is comes from a corpus repository, web download, or a web scrape, it is important to recognize that the attributes that we want to organize can be stored or represented in various formats. The three I will cover here have to do with meta-data that is: (1) contained in the file name of a set of corpus files, (2) embedded in the corpus documents inline with the corpus text, and (3) stored separate from the the text data.</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (3/3): web scraping </title>
      <link>https://francojc.github.io/2017/11/02/acquiring-data-for-language-research-web-scraping/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/11/02/acquiring-data-for-language-research-web-scraping/</guid>
      <description>Web scraping There are many resources available through direct downloads from repositories and individual sites and R package interfaces to web resources with APIs, but these resources are relatively limited to the amount of public-facing textual data recorded on the web. In the case that you want to acquire data from webpages R can be used to access the web programmatically through a process known as web scraping. The complexity of web scrapes can vary but in general it requires more advanced knowledge of R as well as the structure of the language of the web: HTML (Hypertext Markup Language).</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (2/3): package interfaces</title>
      <link>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</guid>
      <description>Package interfaces A convenient alternative method for acquiring data in R is through package interfaces to web services. These interfaces are built using R code to make connections with resources on the web through Automatic Programming Interfaces (APIs). Websites such as Project Gutenberg, Twitter, Facebook, and many others provide APIs to allow access to their data under certain conditions, some more limiting for data collection than others. Programmers (like you!</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (1/3): direct downloads</title>
      <link>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</guid>
      <description>There are three main ways to acquire corpus data using R that I will introduce you to: direct download, package interfaces, and web scraping. In this post we will start by directly downloading a corpus as it is the most straightforward process for the novice R programmer and incurs the least number of steps. Along the way I will introduce some key R coding concepts including control statements and custom functions.</description>
    </item>
    
    <item>
      <title>Data for language research -types and sources</title>
      <link>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</guid>
      <description>In this Recipe you will learn about the types of data available for language research and where to find data. The goal, then, is to introduce you to the landscape of language data available and provide a general overview of the characteristics of language data from a variety of sources providing you with resources to begin your own quantitative investigations.
Data for language research Language research can include data from a variety of sources, linguistic and non-linguistic, that record observations about the world.</description>
    </item>
    
    <item>
      <title>Introduction to statistical thinking</title>
      <link>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</guid>
      <description>Before we begin working on the specifics of our data project, it is important to have a clear understanding of some of the basic concepts that need to be in place to guide our work. In this post I will cover some of these topics including the importance of identifying a research question, how different statistical approaches relate to different types of research, and understanding data from a sampling and organizational standpoint.</description>
    </item>
    
    <item>
      <title>Project management for scalable data analysis</title>
      <link>https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/</link>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/</guid>
      <description>Project management This post can really be seen as an extension of the last post Getting started with R and RStudio in that we will be getting to know some more advanced, but indispensable features of RStudio. These features, in combination with some organizational and programming strategies, will enable us to conduct efficient data analysis and set the stage for research is is both scalable and ready for sharing with either collaborators or the research community.</description>
    </item>
    
    <item>
      <title>Getting started with R and RStudio</title>
      <link>https://francojc.github.io/2017/08/14/getting-started-with-r-and-rstudio/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/08/14/getting-started-with-r-and-rstudio/</guid>
      <description>Why R? The R programming language is free software developed with an eye towards statistical computing and data visualization that has has taken off in popularity over the last decade and is now finds itself among the most used programming languages, in general and is often the go-to language for data science.
So what’s all the fuss about? Among the things that you will come to love about R, you will be hard pressed to find a more active community surrounding a programming language.</description>
    </item>
    
    <item>
      <title>Introducing the Recipe series</title>
      <link>https://francojc.github.io/2017/08/03/introducing-the-recipe-series/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/08/03/introducing-the-recipe-series/</guid>
      <description>The Recipe series: an overview My goal in this series is to explore the ‘why’ and the ‘how’ of doing quantitative language research. The content of this series will, in large part, overlap with resources available on doing Data Science, generally (see (Wickham and Grolemund 2017)), or in field-specific areas and domains (Beckerman, Childs, and Petchey 2017; Hodeghatta and Nayak 2016; Perlin 2017). However, this series will focus exclusively on issues and methods concerning language data and linguistic analyses through practical data sources and realistic examples.</description>
    </item>
    
    <item>
      <title>Testing features in `blogdown`</title>
      <link>https://francojc.github.io/2017/07/24/testing-features-in-blogdown/</link>
      <pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/07/24/testing-features-in-blogdown/</guid>
      <description>This is a post to test the functionality of blogdown. We are going to look at how various types of outputs are rendered and do some tweaking to get things to work right. The hope is to get an idea of what the best practices for creating posts and pages is when working with this software.
Tables Here’s some more code working with tables with different functions.</description>
    </item>
    
    <item>
      <title>Hello world!</title>
      <link>https://francojc.github.io/2017/07/23/hello-world/</link>
      <pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/07/23/hello-world/</guid>
      <description>This is the first post in the academic theme for Hugo. The scaffolding was created using the Addin menu dropdown and selecting New Post. This dialogue will provide the appropriate fields to set up a post or page. By default the subdirectory for posts is post. In the academic theme, other subdirectories are available for pages with particular content and content-specific attributes.
Here’s a bit of code to show the highlight_style used here github.</description>
    </item>
    
    <item>
      <title>Mapping US Census data and adding Twitter posts</title>
      <link>https://francojc.github.io/2016/03/08/mapping-us-census-data-and-adding-twitter-posts/</link>
      <pubDate>Tue, 08 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2016/03/08/mapping-us-census-data-and-adding-twitter-posts/</guid>
      <description>Not all tweets have geolocation information available. Accessing the Twitter API via the streamR package particular parameters can be set to only include those tweets with geolocation enabled. You can also specify a bounding box to further filter the geographic area from which you would like to draw your sample tweets. Now, the bounding box is going to be just that, a box. And we are dealing with polygons. To isolate tweets from a specific geo-political region, such as a census tract, you can use the sp package.</description>
    </item>
    
    <item>
      <title>Mapping US Census data</title>
      <link>https://francojc.github.io/2016/02/06/mapping-us-census-data/</link>
      <pubDate>Sat, 06 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2016/02/06/mapping-us-census-data/</guid>
      <description>I’m currently working on a project involving Twitter posts and demographics. One of the best resources for demographic information in the US is the census. Having not worked with US census data in a very long time, I was excited to see that there is an R package available to make the process easier.
In this exploRation I will provide a tutorial on 1) how to acquire the US census data and other demographic data through American Fact Finder, 2) how to visualize the data in regional choropleths, 3) how to overlay geo-tagged tweets, and finally 4) how to display the map as an interactive plot.</description>
    </item>
    
    <item>
      <title>Generating annotated text in Shiny</title>
      <link>https://francojc.github.io/2015/08/15/generating-annotated-text-in-shiny/</link>
      <pubDate>Sat, 15 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2015/08/15/generating-annotated-text-in-shiny/</guid>
      <description>I’ve been working on a Shiny web app to visualize the results from a text classification algorithm. Specifically, the app aims to classify a particular document in Spanish as either approximating the usage from one of three Spanish subvarieties: Argentine, Mexican, and Peninsular.
In addition to an overall classification, and returning the corresponding probability score, I also want to be able to see how the individual features in the text contribute to the classification.</description>
    </item>
    
    <item>
      <title>Creating hexbin plots</title>
      <link>https://francojc.github.io/2015/08/12/creating-hexbin-plots/</link>
      <pubDate>Wed, 12 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2015/08/12/creating-hexbin-plots/</guid>
      <description>I was just browsing the web today and stumbled on an overview of data visualization in R. As I was scrolling the page something caught my eye: a hexbin plot. I had never heard of such a plot before.
To create a hexbin plot in base R with plot() you need to install the hexbin package.
install.packages(&amp;quot;hexbin&amp;quot;) Then you can load the library and create your hexbin object setting x and y.</description>
    </item>
    
    <item>
      <title>Keyword analysis of State of the Union Speeches</title>
      <link>https://francojc.github.io/2015/04/04/keyword-analysis-of-state-of-the-union-speeches/</link>
      <pubDate>Sat, 04 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2015/04/04/keyword-analysis-of-state-of-the-union-speeches/</guid>
      <description>Not long ago I gave an introduction to text analysis and data mining for the Wake Forest University community. In that tutorial I demonstrated how to do a basic keyword analysis on “State of the Union” speeches from 1947 to present using the freeware AntConc. Although AntConc is a fantastic resource, it’s not my go-to approach for doing text analysis. In this post I will re-create the analysis using R: reading, exploring, and cleaning data as well as the main keyword analysis and corresponding visualizations.</description>
    </item>
    
    <item>
      <title>Web scraping with `rvest` in R</title>
      <link>https://francojc.github.io/2015/03/01/web-scraping-with-rvest-in-r/</link>
      <pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2015/03/01/web-scraping-with-rvest-in-r/</guid>
      <description>In this ExploRation, I will demonstrate how to scrape text data from the web with R. This particular example aims to collect a series of State of the Union (SOTU) speeches [1947-present] from http://www.presidency.ucsb.edu/ and write the plain-text contents to disc. The bulk of the work will be done with the recently released rvest package. The scripting will also employ the magrittr package for writing legible code.
To get started first we identify the sub-page .</description>
    </item>
    
    <item>
      <title>Selecting and copying a random subset of files in the Fish shell</title>
      <link>https://francojc.github.io/2015/02/02/selecting-and-copying-a-random-subset-of-files-in-the-fish-shell/</link>
      <pubDate>Mon, 02 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2015/02/02/selecting-and-copying-a-random-subset-of-files-in-the-fish-shell/</guid>
      <description>I&amp;rsquo;ve recently switched from bash to fish for my shell in UNIX. In this Note-to-self I want to document copying a random sample of files that fit certain characteristics to another directory using fish. Specifically, I select 200 files from the ACTIV-ES corpus from years 2000-present and copy them to a new directory.
The files look like this:
... es_Argentina_1996_Bromato-de-armonio_video-movie_Comedy_333189.run es_Argentina_1996_Moebius_movie_Sci-Fi_117069.run es_Argentina_1997_La-furia_movie_Action_202362.run es_Argentina_1998_Diario-para-un-cuento_movie_Drama_177705.run es_Argentina_1999_La-venganza_movie_Action_226587.run es_Argentina_2000_Nueve-reinas_movie_Crime_247586.run es_Argentina_2001_El-hijo-de-la-novia_movie_Comedy_292542.run es_Argentina_2002_Angel_tv-movie_n_315769.run es_Argentina_2002_Ciudad-de-María_movie_Documentary_317252.run es_Argentina_2002_Mercano,-el-marciano_movie_Animation_283476.run es_Argentina_2002_Tres-pájaros_movie_Adventure_341625.run es_Argentina_2002_Valentín_movie_Comedy_296915.</description>
    </item>
    
    <item>
      <title>Publishing Rmarkdown to Wordpress or Jekyll</title>
      <link>https://francojc.github.io/2014/12/19/publishing-rmarkdown-to-wordpress-or-jekyll/</link>
      <pubDate>Fri, 19 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2014/12/19/publishing-rmarkdown-to-wordpress-or-jekyll/</guid>
      <description>Publish to Wordpress When I first starting looking into matters, I was using a Wordpress site hosted on Wordpress.com. I did some research on how to knit and send an .Rmd file to Wordpress with all images going along for the ride. I found Yuhui Xie’s documentation on the official Knitr page which was very helpful and some documentation on uploading images to imgur.com.
This approach requires the knitr, markdown and RWordpress packages.</description>
    </item>
    
    <item>
      <title>Access Twitter posts by country</title>
      <link>https://francojc.github.io/2014/12/17/access-twitter-posts-by-country/</link>
      <pubDate>Wed, 17 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2014/12/17/access-twitter-posts-by-country/</guid>
      <description>In this ExploRation I will cover how to retrieve and filter tweets from Twitter by country. The first step will be to create and connect to the Twitter API using the twitteR and ROAuth packages. If you don’t already have one you will also need to register for a Twitter developer account and then create an application. This will give you access to an API key and secret. With these packages and credentials, we then will use streamR to download tweets.</description>
    </item>
    
  </channel>
</rss>