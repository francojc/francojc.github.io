<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recipe on francojc ⟲</title>
    <link>https://francojc.github.io/categories/recipe/</link>
    <description>Recent content in Recipe on francojc ⟲</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2018 by Jerid Francom</copyright>
    <lastBuildDate>Fri, 01 Dec 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://francojc.github.io/categories/recipe/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Curate language data (1/2): organizing meta-data</title>
      <link>https://francojc.github.io/2017/12/01/curate-language-data-organizing-meta-data/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/12/01/curate-language-data-organizing-meta-data/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;!-- TODO:
- explore the possibility of adding an image left or right justified with flowing text
- 
--&gt;
&lt;p&gt;When working with raw data, whether is comes from a corpus repository, web download, or a web scrape, it is important to recognize that the attributes that we want to organize can be stored or represented in various formats. The three I will cover here have to do with meta-data that is: (1) contained in the file name of a set of corpus files, (2) embedded in the corpus documents inline with the corpus text, and (3) stored separate from the the text data. Our goal will be to wrangle this information into a tidy dataset format where each row is an observation and each column a corresponding attribute of the data.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The following code is available on GitHub &lt;a href=&#34;https://github.com/francojc/recipes-curate_data&#34;&gt;&lt;code&gt;recipes-curate_data&lt;/code&gt;&lt;/a&gt; and is built on the &lt;code&gt;recipes-project_template&lt;/code&gt; I have discussed in detail &lt;a href=&#34;https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/&#34;&gt;here&lt;/a&gt; and made accessible &lt;a href=&#34;https://github.com/francojc/recipes-project_template.git&#34;&gt;here&lt;/a&gt;. I encourage you to follow along by downloading the &lt;code&gt;recipes-project_template&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; from the Terminal or create a new RStudio R Project and select the “Version Control” option.&lt;/p&gt;

&lt;/div&gt;

&lt;!-- Another important step when tidying raw data is to be aware of the potential erroneous data in the text that may be present that we would like to remove. For example, a corpus may contain resource-particular information that is not needed for the purposes of the analysis (ex. sound file alignment timestamps) or may be part of a scheme that depends on particular software to interpret (ex. formatting for CHAT software). We will also work to clean the data of the extraneous elements as well. --&gt;
&lt;div id=&#34;running-text-with-meta-data-in-file-names&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running text with meta-data in file names&lt;/h2&gt;
&lt;p&gt;A common format for storing meta-data for corpora is in the file names of the corpus documents. When this is the approach of the corpus designer, the names will contain the relevant attributes in some regular format, usually using some common character as the delimiter between the distinct attribute elements.&lt;/p&gt;
&lt;div id=&#34;download-corpus-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Download corpus data&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/francojc/activ-es&#34;&gt;ACTIV-ES Corpus&lt;/a&gt; is structured this way. ACTIV-ES is a corpus of TV/film transcripts from Argentina, Mexico, and Spain. Let’s use this corpus as an example. First we need to download the data. The ACTIV-ES corpus is stored in a GitHub repository. We can download the entire corpus using &lt;code&gt;git&lt;/code&gt; to clone the repository, or we can access the specific corpus format (plain-text or part-of-speech annotated) as a compressed &lt;code&gt;.zip&lt;/code&gt; file. Let’s download the compressed file for the plain text data. Navigate to the &lt;code&gt;https://github.com/francojc/activ-es/blob/master/activ-es-v.02/corpus/plain.zip&lt;/code&gt; file and then copy the link for the ‘Download’ button. We can use the &lt;code&gt;get_zip_data()&lt;/code&gt; function we developed in the &lt;a href=&#34;https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/&#34;&gt;Acquiring data for language research (1/3): direct downloads&lt;/a&gt; post.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_zip_data(url = &amp;quot;https://github.com/francojc/activ-es/raw/master/activ-es-v.02/corpus/plain.zip&amp;quot;, 
             target_dir = &amp;quot;data/original/actives/plain&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Taking a look at the &lt;code&gt;data/original/actives/plain/&lt;/code&gt; directory we can see the files. Below is a subset of files from each of the three countries.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;es_Argentina_2008_Lluvia_movie_Drama_1194615.run
es_Argentina_2008_Los-paranoicos_movie_Comedy_1178654.run
es_Mexico_2008_Rudo-y-Cursi_movie_Comedy_405393.run
es_Mexico_2009_Sin-nombre_movie_Adventure_1127715.run
es_Spain_2010_También-la-lluvia_movie_Drama_1422032.run
es_Spain_2010_Tres-metros-sobre-el-cielo_movie_Drama_1648216.run&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy-the-corpus&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidy the corpus&lt;/h3&gt;
&lt;p&gt;Each of the meta-data attributes is separated by an underscore &lt;code&gt;_&lt;/code&gt;. The extension on these files is &lt;code&gt;.run&lt;/code&gt;. There is nothing special about this extension, the data is plain text, but it is used to contrast the ‘running text’ version of these files with similar names that have linguistic annotations associated in other versions of the corpus. The delimited elements correspond to &lt;code&gt;language&lt;/code&gt;, &lt;code&gt;country&lt;/code&gt;, &lt;code&gt;year&lt;/code&gt;, &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;genre&lt;/code&gt;, and &lt;code&gt;imdb_id&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Ideally we want a data set with columns for each of these attributes in the file names and an extra two columns for the &lt;code&gt;text&lt;/code&gt; itself and an id to distinguish each document &lt;code&gt;doc_id&lt;/code&gt;. The &lt;a href=&#34;https://CRAN.R-project.org/package=readtext&#34;&gt;readtext&lt;/a&gt; package comes in handy here. So let’s load (or install) this package to read the corpus files and the tidyverse package for other miscellaneous helper functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(readtext, tidyverse) # use the pacman package to load-install&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;readtext()&lt;/code&gt; function is quite versatile. It allows us to read multiple files simultaneously and organize the data in a tidy dataset. The files argument will allow us to add the path to the directory where the files are located and use a pattern matching syntax known as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Regular_expression&#34;&gt;Regular Expressions&lt;/a&gt; to match only the files we want to extract the data from. Regular expressions are a powerful tool for manipulating character strings. Getting familiar with how they work is highly recommended.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; We will see them in action at various points throughout the rest of this series. In this case we want all the files from the &lt;code&gt;data/original/actives/plain/&lt;/code&gt; directory that have the extension &lt;code&gt;.run&lt;/code&gt;. So we using the Kleene start &lt;code&gt;*&lt;/code&gt; as a wildcard match in combination with &lt;code&gt;.run&lt;/code&gt; to match all files that end in &lt;code&gt;.run&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Furthermore, the &lt;code&gt;readtext()&lt;/code&gt; function allows for us to specify where the meta-data is to be found with the &lt;code&gt;docvarsfrom&lt;/code&gt; argument, in our case &lt;code&gt;&amp;quot;filenames&amp;quot;&lt;/code&gt;. The default separator value is the underscore, so we do not have to add this argument. In the case, however, the the separator is not an underscore, you will add this argument with the separator value necessary. The actual names we want to give to the attributes can be added with the &lt;code&gt;docvarnames&lt;/code&gt; argument. Note that the &lt;code&gt;docvarnames&lt;/code&gt; argument takes a character vector as a value. Remember to create a character vector we use the &lt;code&gt;c()&lt;/code&gt; function with each element quoted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes &amp;lt;- 
  readtext(file = &amp;quot;data/original/actives/plain/*.run&amp;quot;, # read each file .run
           docvarsfrom = &amp;quot;filenames&amp;quot;, # get attributes from filename
           docvarnames = c(&amp;quot;language&amp;quot;, &amp;quot;country&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;title&amp;quot;, &amp;quot;type&amp;quot;, &amp;quot;genre&amp;quot;, &amp;quot;imdb_id&amp;quot;)) # add the column names we want for each attribute

glimpse(aes) # preview structure of the object&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;Observations: 430
Variables: 9
$ doc_id   &amp;lt;chr&amp;gt; &amp;quot;es_Argentina_1950_Esposa-último-modelo_movie_n_199500.run&amp;quot;, &amp;quot;es_Arge...
$ text     &amp;lt;chr&amp;gt; &amp;quot;No está , señora . Aquí tampoco . No aparece , señora . ¿ Dónde se ha...
$ language &amp;lt;chr&amp;gt; &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;...
$ country  &amp;lt;chr&amp;gt; &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Arge...
$ year     &amp;lt;int&amp;gt; 1950, 1952, 1955, 1965, 1969, 1973, 1975, 1977, 1979, 1980, 1981, 1983...
$ title    &amp;lt;chr&amp;gt; &amp;quot;Esposa-último-modelo&amp;quot;, &amp;quot;No-abras-nunca-esa-puerta&amp;quot;, &amp;quot;El-amor-nunca-m...
$ type     &amp;lt;chr&amp;gt; &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;video-...
$ genre    &amp;lt;chr&amp;gt; &amp;quot;n&amp;quot;, &amp;quot;Mystery&amp;quot;, &amp;quot;Drama&amp;quot;, &amp;quot;Documentary&amp;quot;, &amp;quot;Horror&amp;quot;, &amp;quot;Adventure&amp;quot;, &amp;quot;Drama&amp;quot;...
$ imdb_id  &amp;lt;int&amp;gt; 199500, 184782, 47823, 282622, 62433, 70250, 71897, 333883, 333954, 17...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output from &lt;code&gt;glimpse(aes)&lt;/code&gt; shows us that there are 430 observations and 9 attributes corresponding to the 430 files in the corpus and the 7 meta-data information attributes in the file names plus the added columns &lt;code&gt;doc_id&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt; which contain the name of the file and the text in the file for each file. The information in the &lt;code&gt;doc_id&lt;/code&gt; is captured in our meta-data, yet the values are not ideal –seeing as they are quite long and informationally redundant. Although not strictly necessary, let’s change the &lt;code&gt;doc_id&lt;/code&gt; values to unique numeric values. To transform the data overwriting &lt;code&gt;doc_id&lt;/code&gt; with numerical values we can use the &lt;code&gt;mutate()&lt;/code&gt; function from the tidyverse package in combination with the &lt;code&gt;row_number()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes &amp;lt;- 
  aes %&amp;gt;% 
  mutate(doc_id = row_number()) # change doc_id to numbers

glimpse(aes) # preview structure of the object&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;Observations: 430
Variables: 9
$ doc_id   &amp;lt;int&amp;gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,...
$ text     &amp;lt;chr&amp;gt; &amp;quot;No está , señora . Aquí tampoco . No aparece , señora . ¿ Dónde se ha...
$ language &amp;lt;chr&amp;gt; &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;, &amp;quot;es&amp;quot;...
$ country  &amp;lt;chr&amp;gt; &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Arge...
$ year     &amp;lt;int&amp;gt; 1950, 1952, 1955, 1965, 1969, 1973, 1975, 1977, 1979, 1980, 1981, 1983...
$ title    &amp;lt;chr&amp;gt; &amp;quot;Esposa-último-modelo&amp;quot;, &amp;quot;No-abras-nunca-esa-puerta&amp;quot;, &amp;quot;El-amor-nunca-m...
$ type     &amp;lt;chr&amp;gt; &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;movie&amp;quot;, &amp;quot;video-...
$ genre    &amp;lt;chr&amp;gt; &amp;quot;n&amp;quot;, &amp;quot;Mystery&amp;quot;, &amp;quot;Drama&amp;quot;, &amp;quot;Documentary&amp;quot;, &amp;quot;Horror&amp;quot;, &amp;quot;Adventure&amp;quot;, &amp;quot;Drama&amp;quot;...
$ imdb_id  &amp;lt;int&amp;gt; 199500, 184782, 47823, 282622, 62433, 70250, 71897, 333883, 333954, 17...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-the-tidy-dataset&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explore the tidy dataset&lt;/h3&gt;
&lt;p&gt;Now that we have the data in a tidy format where each row is one of our corpus files and each column is a meta-data attribute that describes each corpus file, let’s do some quick exploration of the distribution of the data to get a better feel for what our corpus is like. One thing we can do is to calculate the size of the corpus. A rudimentary approach to corpus size is the number of word tokens. The &lt;a href=&#34;http://dx.doi.org/10.21105/joss.00037&#34;&gt;tidytext&lt;/a&gt; package provides a very useful function &lt;code&gt;unnest_tokens()&lt;/code&gt; that provides a simple and efficient way to tokenize text while maintaining the tidy structure we have created. In combination with a set of functions from the tidyverse package, we can tokenize the text into words and count the number of words (&lt;code&gt;count()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let’s take this in two steps so you can appreciate what &lt;code&gt;unnest_tokens()&lt;/code&gt; does. First load (or install) tidytext.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(tidytext) # use the pacman package to load-install&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s tokenize the &lt;code&gt;text&lt;/code&gt; column into word terms and preview the first 25 rows in the output.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes_tokens &amp;lt;- 
  aes %&amp;gt;% 
  unnest_tokens(output = terms, input = text) # tokenize `text` into words `terms`
aes_tokens %&amp;gt;% 
  head(25) # view first 25 tokenized terms&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;doc_id&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;language&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;country&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;year&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;title&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;type&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;genre&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;imdb_id&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;terms&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;no&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;está&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;señora&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;aquí&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;tampoco&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;no&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;aparece&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;señora&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;dónde&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;se&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;habrá&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;metido&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;no&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;está&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;doña&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;carlota&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;no&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;aparece&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;por&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;ninguna&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;parte&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;qué&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;busca&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;ahí&#34;},{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;es&#34;,&#34;3&#34;:&#34;Argentina&#34;,&#34;4&#34;:&#34;1950&#34;,&#34;5&#34;:&#34;Esposa-último-modelo&#34;,&#34;6&#34;:&#34;movie&#34;,&#34;7&#34;:&#34;n&#34;,&#34;8&#34;:&#34;199500&#34;,&#34;9&#34;:&#34;y&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;We see in the previous table that a column &lt;code&gt;terms&lt;/code&gt; has replaced &lt;code&gt;text&lt;/code&gt; in our tidy dataset. The meta-data, however, is still in tact.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The &lt;code&gt;unnest_tokens()&lt;/code&gt; function from &lt;code&gt;tidytext&lt;/code&gt; is very flexible. Here we have used the default arguments which produce word tokens. There are many other tokenization parameters that can be used, and we will use, to create sentence tokens, ngram tokens, and custom tokenization schemes. View &lt;code&gt;?unnest_tokens&lt;/code&gt; to find out more in the R documentation.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;After applying the &lt;code&gt;unnest_tokens()&lt;/code&gt; function in the previous code, the rows correspond to tokenized words. Therefore the number of rows corresponds to the total number of words in the corpus. To find the total number of words we can use the &lt;code&gt;count()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes_tokens %&amp;gt;% 
  count() # count terms&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;n&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;2642338&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;count()&lt;/code&gt; function can be used with a data frame, like our &lt;code&gt;aes_tokens&lt;/code&gt; object, to group our rows by the values of a particular column. A practical application for this functionality is to group the rows (word terms) by the values of &lt;code&gt;country&lt;/code&gt; (‘Argentina’, ‘Mexico’, and ‘Spain’). This will give us the number of words in each country sub-corpus.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes_tokens %&amp;gt;% 
  count(country) # count terms by `country`&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;country&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;n&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;Argentina&#34;,&#34;2&#34;:&#34;838976&#34;},{&#34;1&#34;:&#34;Mexico&#34;,&#34;2&#34;:&#34;754955&#34;},{&#34;1&#34;:&#34;Spain&#34;,&#34;2&#34;:&#34;1048407&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;So now we know the total word count of the corpus and the number of words in each country sub-corpus. If we would like to have a description of the proportion of words from each sub-corpus in the total corpus, we can use the &lt;code&gt;mutate()&lt;/code&gt; function to create a new column &lt;code&gt;prop&lt;/code&gt; which calculates the total size of the corpus (&lt;code&gt;sum(n)&lt;/code&gt;) and then divides each sub-corpus size (&lt;code&gt;n&lt;/code&gt;) by this number.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes_country_props &amp;lt;- 
  aes_tokens %&amp;gt;% 
  count(country) %&amp;gt;% # count terms by `country`
  mutate(prop = n / sum(n) ) # add the word term proportion for each country
aes_country_props&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;country&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;n&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;prop&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;Argentina&#34;,&#34;2&#34;:&#34;838976&#34;,&#34;3&#34;:&#34;0.3175127&#34;},{&#34;1&#34;:&#34;Mexico&#34;,&#34;2&#34;:&#34;754955&#34;,&#34;3&#34;:&#34;0.2857148&#34;},{&#34;1&#34;:&#34;Spain&#34;,&#34;2&#34;:&#34;1048407&#34;,&#34;3&#34;:&#34;0.3967725&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;As we have seen in the previous examples tidy datasets are easy to work with. Another advantage to data frames is that we can use them to create graphics using the &lt;a href=&#34;http://ggplot2.org&#34;&gt;ggplot2&lt;/a&gt; package. ggplot2 is a powerful package for creating graphics in R that applies what is known as the &lt;a href=&#34;https://ramnathv.github.io/pycon2014-r/visualize/ggplot2.html&#34;&gt;‘Grammar of Graphics’&lt;/a&gt;. The Grammar of Graphics recognizes that there are three principle components to any graphic: (1) data, (2) mappings or ‘aesthetics’ as they are called, and (3) geometries, or ‘geoms’. Data is the data frame which contains our observations (rows) and our variables (columns). We connect certain variables of interest from our data set to certain parameters in the visual space. Typical parameters include the ‘x-axis’ and the ‘y-axis’. The x-axis corresponds to the horizontal plane and the y-axis the vertical plane. This sets up a base coordinate system for visualizing the data. Once our data has been mapped to a visual space, we then designate an appropriate geometry to represent this space (bar plot, line graphs, scatter plots, etc). There are many &lt;a href=&#34;http://ggplot2.tidyverse.org/reference/#section-layer-geoms&#34;&gt;geometries available in ggplot2 for relevant mapping types&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s visualize the &lt;code&gt;aes_country_props&lt;/code&gt; object as a bar graph, as an example. ggplot2 is included as part of the tidyverse package so we already have access to it. So first we pass the &lt;code&gt;aes_country_props&lt;/code&gt; data frame to the &lt;code&gt;ggplot()&lt;/code&gt; function. Then we map the x-axis to the &lt;code&gt;country&lt;/code&gt; column and the y-axis to the &lt;code&gt;prop&lt;/code&gt; column. This mapping is then passed with the plus &lt;code&gt;+&lt;/code&gt; operator to the &lt;code&gt;geom_col()&lt;/code&gt; function to visualize the mapping in columns, or bars.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes_country_props %&amp;gt;% # pass the data frame as our data source
  ggplot(aes(x = country, y = prop)) + # create x- and y-axis mappings
  geom_col() # visualize a column-wise geometry&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code will do the heavy lifting to create our plot. Below I’ve added the &lt;code&gt;labs()&lt;/code&gt; function to create a more informative graphic with prettier x- and y-axis labels and a title and subtitle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;aes_country_props %&amp;gt;% # pass the data frame as our data source
  ggplot(aes(x = country, y = prop)) + # create x- and y-axis mappings
  geom_col() + # visualize a column-wise geometry
  labs(x = &amp;quot;Country&amp;quot;, y = &amp;quot;Proportion (%)&amp;quot;, title = &amp;quot;ACTIV-ES Corpus Distribution&amp;quot;, subtitle = &amp;quot;Proportion of words in each country sub-corpus&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://francojc.github.io/post/2017-12-01-curate-language-data-organizing-metadata_files/figure-html/aes-country-plot-labs-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example we covered reading a corpus and the meta-data information contained withing the file names of this corpus with the readtext package. We then did some quick exploratory work to find the corpus size and the proportions of the corpus by country sub-corpus with the tidytext package and assorted functions from the tidyverse package. We rounded things out with a brief introduction to the ggplot2 package which we used to visualize the country sub-corpus proportions.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-text-with-inline-meta-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running text with inline meta-data&lt;/h2&gt;
&lt;p&gt;In the previous example, our corpus contained meta-data stored in the individual file names of the corpus. In some other cases the meta-data is stored inline with the corpus text itself. The goal in cases such as these is to separate the meta-data from the text and coerce all the information into a tidy dataset.&lt;/p&gt;
&lt;div id=&#34;download-corpus-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Download corpus data&lt;/h3&gt;
&lt;p&gt;As an example we will work with the The Switchboard Dialog Act Corpus (SDAC) which extends the &lt;a href=&#34;https://catalog.ldc.upenn.edu/LDC97S62&#34;&gt;Switchboard Corpus&lt;/a&gt; with speech act annotation. The SDAC dialogues (&lt;code&gt;swb1_dialogact_annot.tar.gz&lt;/code&gt;) are available as a &lt;a href=&#34;https://catalog.ldc.upenn.edu/docs/LDC97S62/&#34;&gt;free download from the LDC&lt;/a&gt;. The dialogues are contained within a compressed &lt;code&gt;.tar.gz&lt;/code&gt; file. This file can be downloaded manually and its contents extracted to disk, but since we are working to create a reproducible workflow we will approach this task programmatically.&lt;/p&gt;
&lt;p&gt;We have an available custom function that deals with &lt;code&gt;.zip&lt;/code&gt; compressed files &lt;code&gt;get_zip_data()&lt;/code&gt; but we need a function that works on &lt;code&gt;.tar.gz&lt;/code&gt; files. R has a function to extract &lt;code&gt;.tar.gz&lt;/code&gt; files &lt;code&gt;untar()&lt;/code&gt; that we can use to mimic the same functionality as the &lt;code&gt;unzip()&lt;/code&gt; function used in the &lt;code&gt;get_zip_data()&lt;/code&gt; function. Instead of writing a new custom function to deal specifically with &lt;code&gt;.tar.gz&lt;/code&gt; files, I’ve created a function that deals with both compressed file formats, named it &lt;code&gt;get_compressed_data()&lt;/code&gt;, and added it to my &lt;code&gt;functions/acquire_functions.R&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_compressed_data &amp;lt;- function(url, target_dir, force = FALSE) {
  # Get the extension of the target file
  ext &amp;lt;- tools::file_ext(url)
  # Check to see if the target file is a compressed file
  if(!ext %in% c(&amp;quot;zip&amp;quot;, &amp;quot;gz&amp;quot;, &amp;quot;tar&amp;quot;)) stop(&amp;quot;Target file given is not supported&amp;quot;)
  # Check to see if the data already exists
  if(!dir.exists(target_dir) | force == TRUE) { # if data does not exist, download/ decompress
    cat(&amp;quot;Creating target data directory \n&amp;quot;) # print status message
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&amp;quot;Downloading data... \n&amp;quot;) # print status message
    temp &amp;lt;- tempfile() # create a temporary space for the file to be written to
    download.file(url = url, destfile = temp) # download the data to the temp file
    # Decompress the temp file in the target directory
    if(ext == &amp;quot;zip&amp;quot;) {
      unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE) # zip files
    } else {
      untar(tarfile = temp, exdir = target_dir) # tar, gz files
    }
    cat(&amp;quot;Data downloaded! \n&amp;quot;) # print status message
  } else { # if data exists, don&amp;#39;t download it again
    cat(&amp;quot;Data already exists \n&amp;quot;) # print status message
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once this function is loaded into R, either by sourcing the &lt;code&gt;functions/aquire_functions.R&lt;/code&gt; file (&lt;code&gt;source(&amp;quot;functions/acquire_functions.R&amp;quot;)&lt;/code&gt;) or running the code directly, we apply the function to the resource URL targeting the &lt;code&gt;data/original/sdac/&lt;/code&gt; directory as the extraction location.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_compressed_data(url = &amp;quot;https://catalog.ldc.upenn.edu/docs/LDC97S62/swb1_dialogact_annot.tar.gz&amp;quot;, 
                    target_dir = &amp;quot;data/original/sdac/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main directory structure of the &lt;code&gt;sdac/&lt;/code&gt; data looks like this:&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;.
├── README
├── doc
├── sw00utt
├── sw01utt
├── sw02utt
├── sw03utt
├── sw04utt
├── sw05utt
├── sw06utt
├── sw07utt
├── sw08utt
├── sw09utt
├── sw10utt
├── sw11utt
├── sw12utt
└── sw13utt

15 directories, 1 file&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;README&lt;/code&gt; file contains basic information about the resource, the &lt;code&gt;doc/&lt;/code&gt; directory contains more detailed information about the dialog annotations, and each of the following directories prefixed with &lt;code&gt;sw...&lt;/code&gt; contain individual conversation files. Here’s a peek at internal structure of the first couple directories.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;.
├── README
├── doc
│   └── manual.august1.html
├── sw00utt
│   ├── sw_0001_4325.utt
│   ├── sw_0002_4330.utt
│   ├── sw_0003_4103.utt
│   ├── sw_0004_4327.utt
│   ├── sw_0005_4646.utt&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a look at the first conversation file (&lt;code&gt;sw_0001_4325.utt&lt;/code&gt;) to see how it is structured.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
*x*                                                                     *x*
*x*            Copyright (C) 1995 University of Pennsylvania            *x*
*x*                                                                     *x*
*x*    The data in this file are part of a preliminary version of the   *x*
*x*    Penn Treebank Corpus and should not be redistributed.  Any       *x*
*x*    research using this corpus or based on it should acknowledge     *x*
*x*    that fact, as well as the preliminary nature of the corpus.      *x*
*x*                                                                     *x*
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*
*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*


FILENAME:   4325_1632_1519
TOPIC#:     323
DATE:       920323
TRANSCRIBER:    glp
UTT_CODER:  tc
DIFFICULTY: 1
TOPICALITY: 3
NATURALNESS:    2
ECHO_FROM_B:    1
ECHO_FROM_A:    4
STATIC_ON_A:    1
STATIC_ON_B:    1
BACKGROUND_A:   1
BACKGROUND_B:   2
REMARKS:        None.

=========================================================================
  

o          A.1 utt1: Okay.  /
qw          A.1 utt2: {D So, }   

qy^d          B.2 utt1: [ [ I guess, +   

+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are few things to take note of here. First we see that the conversation files have a meta-data header offset from the conversation text by a line of &lt;code&gt;=&lt;/code&gt; characters. Second the header contains meta-information of various types. Third, the text is interleaved with an annotation scheme.&lt;/p&gt;
&lt;p&gt;Some of the information may be readily understandable, such as the various pieces of meta-data in the header, but to get a better understanding of what information is encoded here let’s take a look at the &lt;code&gt;README&lt;/code&gt; file. In this file we get a birds eye view of what is going on. In short, the data includes 1155 telephone conversations between two people annotated with 42 ‘DAMSL’ dialog act labels. The &lt;code&gt;README&lt;/code&gt; file refers us to the &lt;code&gt;doc/manual.august1.html&lt;/code&gt; file for more information on this scheme.&lt;/p&gt;
&lt;p&gt;At this point we open the the &lt;code&gt;doc/manual.august1.html&lt;/code&gt; file in a browser and do some investigation. We find out that ‘DAMSL’ stands for ‘Discourse Annotation and Markup System of Labeling’ and that the first characters of each line of the conversation text correspond to one or a combination of labels for each utterance. So for our first utterances we have:&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;o = &amp;quot;Other&amp;quot;
qw = &amp;quot;Wh-Question&amp;quot;
qy^d = &amp;quot;Declarative Yes-No-Question&amp;quot;
+ = &amp;quot;Segment (multi-utterance)&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each utterance is also labeled for speaker (‘A’ or ‘B’), speaker turn (‘1’, ‘2’, ‘3’, etc.), and each utterance within that turn (‘utt1’, ‘utt2’, etc.). There is other annotation provided withing each utterance, but this should be enough to get us started on the conversations.&lt;/p&gt;
&lt;p&gt;Now let’s turn to the meta-data in the header. We see here that there is information about the creation of the file: ‘FILENAME’, ‘TOPIC’, ‘DATE’, etc. The &lt;code&gt;doc/manual.august1.html&lt;/code&gt; file doesn’t have much to say about this information so I returned to the &lt;a href=&#34;https://catalog.ldc.upenn.edu/docs/LDC97S62/&#34;&gt;LDC Documentation&lt;/a&gt; and found more information in the &lt;a href=&#34;https://catalog.ldc.upenn.edu/docs/LDC97S62/&#34;&gt;Online Documentation&lt;/a&gt; section. After some poking around in this documentation I discovered that that meta-data for each speaker in the corpus is found in the &lt;code&gt;caller_tab.csv&lt;/code&gt; file. This tabular file does not contain column names, but the &lt;code&gt;caller_doc.txt&lt;/code&gt; does. After inspecting these files manually and comparing them with the information in the conversation file I noticed that the ‘FILENAME’ information contained three pieces of useful information delimited by underscores &lt;code&gt;_&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*x*


FILENAME:   4325_1632_1519
TOPIC#:     323
DATE:       920323
TRANSCRIBER:    glp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first information is the document id (&lt;code&gt;4325&lt;/code&gt;), the second and third correspond to the speaker number: the first being speaker A (&lt;code&gt;1632&lt;/code&gt;) and the second speaker B (&lt;code&gt;1519&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy-the-corpus-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidy the corpus&lt;/h3&gt;
&lt;p&gt;In sum, we have 1155 conversation files. Each file has two parts, a header and text section, separated by a line of &lt;code&gt;=&lt;/code&gt; characters. The header section contains a ‘FILENAME’ line which has the document id, and ids for speaker A and speaker B. The text section is annotated with DAMSL tags beginning each line, followed by speaker, turn number, utterance number, and the utterance text. With this knowledge in hand, let’s set out to create a tidy dataset with the following column structure:&lt;/p&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;doc_id&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;damsl_tag&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;speaker&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;turn_num&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;utterance_num&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;utterance_text&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;speaker_id&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;o&#34;,&#34;3&#34;:&#34;A&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;Okay.  /&#34;,&#34;7&#34;:&#34;1632&#34;,&#34;_rn_&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;qw&#34;,&#34;3&#34;:&#34;A&#34;,&#34;4&#34;:&#34;1&#34;,&#34;5&#34;:&#34;2&#34;,&#34;6&#34;:&#34;{D So, }&#34;,&#34;7&#34;:&#34;1632&#34;,&#34;_rn_&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;qy^d&#34;,&#34;3&#34;:&#34;B&#34;,&#34;4&#34;:&#34;2&#34;,&#34;5&#34;:&#34;1&#34;,&#34;6&#34;:&#34;[ [ I guess, +&#34;,&#34;7&#34;:&#34;1519&#34;,&#34;_rn_&#34;:&#34;3&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Let’s begin by reading one of the conversation files into R as a character vector using the &lt;code&gt;read_lines()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc &amp;lt;- read_lines(file = &amp;quot;data/original/sdac/sw00utt/sw_0001_4325.utt&amp;quot;) # read file by lines&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To isolate the vector element that contains the document and speaker ids, we use &lt;code&gt;str_detect()&lt;/code&gt; from the &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;stringr&lt;/a&gt; package. This function takes two arguments, a string and a pattern, and returns a logical value, &lt;code&gt;TRUE&lt;/code&gt; if the pattern is matched or &lt;code&gt;FALSE&lt;/code&gt; if not. We can use the output of this function, then, to subset the &lt;code&gt;doc&lt;/code&gt; character vector and only return the vector element (line) that contains &lt;code&gt;digits_digits_digits&lt;/code&gt; with a regular expression. The expression combines the digit matching operator &lt;code&gt;\\d&lt;/code&gt; with the &lt;code&gt;+&lt;/code&gt; operator to match 1 or more contiguous digits. We then separate three groups of &lt;code&gt;\\d+&lt;/code&gt; with underscores &lt;code&gt;_&lt;/code&gt;. The result is &lt;code&gt;\\d+_\\d+_\\d+&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(stringr) # load-install `stringr` package
doc[str_detect(doc, pattern = &amp;quot;\\d+_\\d+_\\d+&amp;quot;)] # isolate pattern&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;FILENAME:\t4325_1632_1519&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to extract the three digit sequences that correspond to the &lt;code&gt;doc_id&lt;/code&gt;, &lt;code&gt;speaker_a_id&lt;/code&gt;, and &lt;code&gt;speaker_b_id&lt;/code&gt;. First we extract the pattern that we have identified with &lt;code&gt;str_extract()&lt;/code&gt; and then we can break up the single character vector into multiple parts based on the underscore &lt;code&gt;_&lt;/code&gt;. The &lt;code&gt;str_split()&lt;/code&gt; function takes a string and then a pattern to use to split a character vector. It will return a list of character vectors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc[str_detect(doc, &amp;quot;\\d+_\\d+_\\d+&amp;quot;)] %&amp;gt;% # isolate pattern
  str_extract(pattern = &amp;quot;\\d+_\\d+_\\d+&amp;quot;) %&amp;gt;% # extract the pattern
  str_split(pattern = &amp;quot;_&amp;quot;) # split the character vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [[1]]
## [1] &amp;quot;4325&amp;quot; &amp;quot;1632&amp;quot; &amp;quot;1519&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A &lt;strong&gt;list&lt;/strong&gt; is a special object type in R. It is an unordered collection of objects whose lengths can differ (contrast this with a data frame which is a collection of objects whose lengths are the same –hence the tabular format). In this case we have a list of length 1, whose sole element is a character vector of length 3 –one element per segment returned from our split. This is a desired result in most cases as if we were to pass multiple character vectors to our &lt;code&gt;str_split()&lt;/code&gt; function we don’t want the results to be conflated as a single character vector blurring the distinction between the individual character vectors. If we &lt;em&gt;would&lt;/em&gt; like to conflate, or &lt;em&gt;flatten&lt;/em&gt; a list, we can use the &lt;code&gt;unlist()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc[str_detect(doc, &amp;quot;\\d+_\\d+_\\d+&amp;quot;)] %&amp;gt;% # isolate pattern
  str_extract(pattern = &amp;quot;\\d+_\\d+_\\d+&amp;quot;) %&amp;gt;% # extract the pattern
  str_split(pattern = &amp;quot;_&amp;quot;) %&amp;gt;% # split the character vector
  unlist() # flatten the list to a character vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;4325&amp;quot; &amp;quot;1632&amp;quot; &amp;quot;1519&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;!-- In this case we won&#39;t flatten the list object just yet. Instead we want to extract the digits from our three character vectors. The `str_extract_all()` will take the list output of character vectors and allow us to provide a pattern to match and extract. Where in the `str_split()` case our pattern was a fixed character (`_`), the pattern to match for digits is more abstract --we want the contiguous sequences of digits. For this we will build a regular expression to match the digits. The expression we want is `\\d+` which means match one or more contiguous digits. Regular expressions provide hooks for many different character types. We will soon see a number of them in action as we proceed to curate the `sdac/` data. --&gt;
&lt;!-- ```{r sdac-doc-info-4, echo=TRUE, eval=TRUE} --&gt;
&lt;!-- doc[str_detect(doc, &#34;FILENAME:&#34;)] %&gt;%  --&gt;
&lt;!--   str_split(pattern = &#34;_&#34;) %&gt;%  --&gt;
&lt;!--   str_extract_all(pattern = &#34;\\d+&#34;) --&gt;
&lt;!-- ``` --&gt;
&lt;!-- The `str_extract_all()` took our list and returned a list. At this point we can flatten and assign the results to the variable `doc_speaker_info`. --&gt;
&lt;!-- ```{r sdac-doc-info-5, echo=TRUE, eval=TRUE} --&gt;
&lt;!-- doc_speaker_info &lt;-  --&gt;
&lt;!--   doc[str_detect(doc, &#34;FILENAME:&#34;)] %&gt;%  --&gt;
&lt;!--   str_split(pattern = &#34;_&#34;) %&gt;%  --&gt;
&lt;!--   str_extract_all(pattern = &#34;\\d+&#34;) %&gt;%  --&gt;
&lt;!--   unlist() --&gt;
&lt;!-- ``` --&gt;
&lt;p&gt;Let’s flatten the list in this case, as we have a single character vector, and assign this result to &lt;code&gt;doc_speaker_info&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc_speaker_info &amp;lt;- 
  doc[str_detect(doc, &amp;quot;\\d+_\\d+_\\d+&amp;quot;)] %&amp;gt;% # isolate pattern
  str_extract(pattern = &amp;quot;\\d+_\\d+_\\d+&amp;quot;) %&amp;gt;% # extract the pattern
  str_split(pattern = &amp;quot;_&amp;quot;) %&amp;gt;%  # split the character vector
  unlist() # flatten the list to a character vector&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;doc_speaker_info&lt;/code&gt; is now a character vector of length three. Let’s subset each of the elements and assign them to meaningful variable names so we can conveniently use them later on in the tidying process.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc_id &amp;lt;- doc_speaker_info[1]
speaker_a_id &amp;lt;- doc_speaker_info[2]
speaker_b_id &amp;lt;- doc_speaker_info[3]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to isolate the text section extracting it from rest of the document. As noted previously, a sequence of &lt;code&gt;=&lt;/code&gt; separates the header section from the text section. What we need to do is to index the point in our character vector &lt;code&gt;doc&lt;/code&gt; where that line occurs and then subset the &lt;code&gt;doc&lt;/code&gt; from that point until the end of the character vector. Let’s first find the point where the &lt;code&gt;=&lt;/code&gt; sequence occurs. We will again use the &lt;code&gt;str_detect()&lt;/code&gt; function to find the pattern we are looking for (a contiguous sequence of &lt;code&gt;=&lt;/code&gt;), but then we will pass the logical result to the &lt;code&gt;which()&lt;/code&gt; function which will return the element index number of this match.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc %&amp;gt;% 
  str_detect(pattern = &amp;quot;=+&amp;quot;) %&amp;gt;% # match 1 or more `=`
  which() # find vector index&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So for this file &lt;code&gt;31&lt;/code&gt; is the index in &lt;code&gt;doc&lt;/code&gt; where the &lt;code&gt;=&lt;/code&gt; sequence occurs. Now it is important to keep in mind that we are working with a single file from the &lt;code&gt;sdac/&lt;/code&gt; data. We need to be cautious to not create a pattern that may be matched multiple times in another document in the corpus. As the &lt;code&gt;=+&lt;/code&gt; pattern will match &lt;code&gt;=&lt;/code&gt;, or &lt;code&gt;==&lt;/code&gt;, or &lt;code&gt;===&lt;/code&gt;, etc. it is not implausible to believe that there might be a &lt;code&gt;=&lt;/code&gt; character on some other line in one of the other files. Let’s update our regular expression to avoid this potential scenario by only matching sequences of three or more &lt;code&gt;=&lt;/code&gt;. In this case we will make use of the curly bracket operators &lt;code&gt;{}&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;doc %&amp;gt;% 
  str_detect(pattern = &amp;quot;={3,}&amp;quot;) %&amp;gt;% # match 3 or more `=`
  which() # find vector index&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will get the same result for this file, but will safeguard ourselves a bit as it is unlikely we will find multiple matches for &lt;code&gt;===&lt;/code&gt;, &lt;code&gt;====&lt;/code&gt;, etc.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;31&lt;/code&gt; is the index for the &lt;code&gt;=&lt;/code&gt; sequence, but we want the next line to be where we start reading the text section. To do this we increment the index by 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text_start_index &amp;lt;- 
  doc %&amp;gt;% 
  str_detect(pattern = &amp;quot;={3,}&amp;quot;) %&amp;gt;% # match 3 or more `=` 
  which() # find vector index
text_start_index &amp;lt;- text_start_index + 1 # increment index by 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The index for the end of the text is simply the length of the &lt;code&gt;doc&lt;/code&gt; vector. We can use the &lt;code&gt;length()&lt;/code&gt; function to get this index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text_end_index &amp;lt;- length(doc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the bookends, so to speak, for our text section. To extract the text we subset the &lt;code&gt;doc&lt;/code&gt; vector by these indices.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text &amp;lt;- doc[text_start_index:text_end_index] # extract text
head(text) # preview first lines of `text`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;  &amp;quot;                                       
## [2] &amp;quot;&amp;quot;                                         
## [3] &amp;quot;o          A.1 utt1: Okay.  /&amp;quot;            
## [4] &amp;quot;qw          A.1 utt2: {D So, }   &amp;quot;        
## [5] &amp;quot;&amp;quot;                                         
## [6] &amp;quot;qy^d          B.2 utt1: [ [ I guess, +   &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The text has some extra whitespace on some lines and there are blank lines as well. We should do some cleaning up before moving forward to organize the data. To get rid of the whitespace we use the &lt;code&gt;str_trim()&lt;/code&gt; function which by default will remove leading and trailing whitespace from each line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text &amp;lt;- str_trim(text) # remove leading and trailing whitespace
head(text) # preview first lines of `text`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;&amp;quot;                                      
## [2] &amp;quot;&amp;quot;                                      
## [3] &amp;quot;o          A.1 utt1: Okay.  /&amp;quot;         
## [4] &amp;quot;qw          A.1 utt2: {D So, }&amp;quot;        
## [5] &amp;quot;&amp;quot;                                      
## [6] &amp;quot;qy^d          B.2 utt1: [ [ I guess, +&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To remove blank lines we will create a logical expression to subset the &lt;code&gt;text&lt;/code&gt; vector. &lt;code&gt;text != &amp;quot;&amp;quot;&lt;/code&gt; means return TRUE where lines are not blank, and FALSE where they are.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;text &amp;lt;- text[text != &amp;quot;&amp;quot;] # remove blank lines
head(text) # preview first lines of `text`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;o          A.1 utt1: Okay.  /&amp;quot;                                                                  
## [2] &amp;quot;qw          A.1 utt2: {D So, }&amp;quot;                                                                 
## [3] &amp;quot;qy^d          B.2 utt1: [ [ I guess, +&amp;quot;                                                         
## [4] &amp;quot;+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /&amp;quot;
## [5] &amp;quot;+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /&amp;quot;                        
## [6] &amp;quot;qy          A.5 utt1: Does it say something? /&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our first step towards a tidy dataset is to now combine the &lt;code&gt;doc_id&lt;/code&gt; and each element of &lt;code&gt;text&lt;/code&gt; in a data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- data.frame(doc_id, text) # tidy format `doc_id` and `text`
head(data) # preview first lines of `text`&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;doc_id&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;text&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;o          A.1 utt1: Okay.  /&#34;,&#34;_rn_&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;qw          A.1 utt2: {D So, }&#34;,&#34;_rn_&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;qy^d          B.2 utt1: [ [ I guess, +&#34;,&#34;_rn_&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /&#34;,&#34;_rn_&#34;:&#34;4&#34;},{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /&#34;,&#34;_rn_&#34;:&#34;5&#34;},{&#34;1&#34;:&#34;4325&#34;,&#34;2&#34;:&#34;qy          A.5 utt1: Does it say something? /&#34;,&#34;_rn_&#34;:&#34;6&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;With our data now in a data frame, its time to parse the &lt;code&gt;text&lt;/code&gt; column and extract the damsl tags, speaker, speaker turn, utterance number, and the utterance text itself into separate columns. To do this we will make extensive use of regular expressions. Our aim is to find a consistent pattern that distinguishes each piece of information from other other text in a given row of &lt;code&gt;data$text&lt;/code&gt; and extract it.&lt;/p&gt;
&lt;p&gt;The best way to learn regular expressions is to use them. To this end I’ve included a window to the interactive regular expression practice website &lt;a href=&#34;https://regex101.com&#34;&gt;regex101&lt;/a&gt; in Figure &lt;a href=&#34;#fig:regex&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Copy the text below into the ‘TEST STRING’ field.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;o          A.1 utt1: Okay.  /
qw          A.1 utt2: {D So, }
qy^d          B.2 utt1: [ [ I guess, +
+          A.3 utt1: What kind of experience [ do you, + do you ] have, then with child care? /
+          B.4 utt1: I think, ] + {F uh, } I wonder ] if that worked. /
qy          A.5 utt1: Does it say something? /
sd          B.6 utt1: I think it usually does.  /
ad          B.6 utt2: You might try, {F uh, }  /
h          B.6 utt3: I don&amp;#39;t know,  /
ad          B.6 utt4: hold it down a little longer,  /&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:regex&#34;&gt;&lt;/span&gt;
&lt;iframe src=&#34;https://regex101.com/?flags=gm&#34; width=&#34;100%&#34; height=&#34;600px&#34;&gt;
&lt;/iframe&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Interactive interface to the &lt;a href=&#34;https://regex101.com&#34;&gt;regex101&lt;/a&gt; practice website.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now manually type the following regular expressions into the ‘REGULAR EXPRESSION’ field one-by-one (each is on a separate line). Notice what is matched as you type and when you’ve finished typing. You can find out exactly what the component parts of each expression are doing by toggling the top right icon in the window or hovering your mouse over the relevant parts of the expression.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;^.+?\s
[AB]\.\d+
utt\d+
:.+$&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can now see, we have regular expressions that will match the damsl tags, speaker and speaker turn, utterance number, and the utterance text. To apply these expressions to our data and extract this information into separate columns we will make use of the &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;str_extract()&lt;/code&gt; functions. &lt;code&gt;mutate()&lt;/code&gt; will take our data frame and create new columns with values we match and extract from each row in the data frame with &lt;code&gt;str_extract()&lt;/code&gt;. Notice that &lt;code&gt;str_extract()&lt;/code&gt; is different than &lt;code&gt;str_extract_all()&lt;/code&gt;. When we work with &lt;code&gt;mutate()&lt;/code&gt; each row will be evaluated in turn, therefore we only need to make one match per row in &lt;code&gt;data$text&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I’ve chained each of these steps in the code below, dropping the original &lt;code&gt;text&lt;/code&gt; column with &lt;code&gt;select(-text)&lt;/code&gt;, and overwriting &lt;code&gt;data&lt;/code&gt; with the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- # extract column information from `text`
  data %&amp;gt;% 
  mutate(damsl_tag = str_extract(string = text, pattern = &amp;quot;^.+?\\s&amp;quot;)) %&amp;gt;%  # extract damsl tags
  mutate(speaker_turn = str_extract(string = text, pattern = &amp;quot;[AB]\\.\\d+&amp;quot;)) %&amp;gt;% # extract speaker_turn pairs
  mutate(utterance_num = str_extract(string = text, pattern = &amp;quot;utt\\d+&amp;quot;)) %&amp;gt;% # extract utterance number
  mutate(utterance_text = str_extract(string = text, pattern = &amp;quot;:.+$&amp;quot;)) %&amp;gt;%  # extract utterance text
  select(-text) # drop the `text` column

glimpse(data) # preview the data set&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 159
## Variables: 5
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;o &amp;quot;, &amp;quot;qw &amp;quot;, &amp;quot;qy^d &amp;quot;, &amp;quot;+ &amp;quot;, &amp;quot;+ &amp;quot;, &amp;quot;qy &amp;quot;, &amp;quot;sd &amp;quot;,...
## $ speaker_turn   &amp;lt;chr&amp;gt; &amp;quot;A.1&amp;quot;, &amp;quot;A.1&amp;quot;, &amp;quot;B.2&amp;quot;, &amp;quot;A.3&amp;quot;, &amp;quot;B.4&amp;quot;, &amp;quot;A.5&amp;quot;, &amp;quot;B.6&amp;quot;...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;utt1&amp;quot;, &amp;quot;utt2&amp;quot;, &amp;quot;utt1&amp;quot;, &amp;quot;utt1&amp;quot;, &amp;quot;utt1&amp;quot;, &amp;quot;utt1&amp;quot;,...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;: Okay.  /&amp;quot;, &amp;quot;: {D So, }&amp;quot;, &amp;quot;: [ [ I guess, +&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;One twist you will notice is that regular expressions in R require double backslashes (&lt;code&gt;\\&lt;/code&gt;) where other programming environments use a single backslash (&lt;code&gt;\&lt;/code&gt;).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;There are a couple things left to do to the columns we extracted from the text before we move on to finishing up our tidy dataset. First, we need to separate the &lt;code&gt;speaker_turn&lt;/code&gt; column into &lt;code&gt;speaker&lt;/code&gt; and &lt;code&gt;turn_num&lt;/code&gt; columns and second we need to remove unwanted characters from the &lt;code&gt;damsl_tag&lt;/code&gt;, &lt;code&gt;utterance_num&lt;/code&gt;, and &lt;code&gt;utterance_text&lt;/code&gt; columns.&lt;/p&gt;
&lt;p&gt;To separate the values of a column into two columns we use the &lt;code&gt;separate()&lt;/code&gt; function. It takes a column to separate and character vector of the names of the new columns to create. By default the values of the input column will be separated by non-alphanumeric characters. In our case this means the &lt;code&gt;.&lt;/code&gt; will be our separator.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;-
  data %&amp;gt;% 
  separate(col = speaker_turn, into = c(&amp;quot;speaker&amp;quot;, &amp;quot;turn_num&amp;quot;)) # separate speaker_turn into distinct columns

glimpse(data) # preview the data set&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 159
## Variables: 6
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;o &amp;quot;, &amp;quot;qw &amp;quot;, &amp;quot;qy^d &amp;quot;, &amp;quot;+ &amp;quot;, &amp;quot;+ &amp;quot;, &amp;quot;qy &amp;quot;, &amp;quot;sd &amp;quot;,...
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B...
## $ turn_num       &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;utt1&amp;quot;, &amp;quot;utt2&amp;quot;, &amp;quot;utt1&amp;quot;, &amp;quot;utt1&amp;quot;, &amp;quot;utt1&amp;quot;, &amp;quot;utt1&amp;quot;,...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;: Okay.  /&amp;quot;, &amp;quot;: {D So, }&amp;quot;, &amp;quot;: [ [ I guess, +&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To remove unwanted leading or trailing whitespace we apply the &lt;code&gt;str_trim()&lt;/code&gt; function. For removing other characters we matching the character(s) and replace them with an empty string (&lt;code&gt;&amp;quot;&amp;quot;&lt;/code&gt;) with the &lt;code&gt;str_replace()&lt;/code&gt; function. Again, I’ve chained these functions together and overwritten &lt;code&gt;data&lt;/code&gt; with the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- # clean up column information
  data %&amp;gt;% 
  mutate(damsl_tag = str_trim(damsl_tag)) %&amp;gt;% # remove leading/ trailing whitespace
  mutate(utterance_num = str_replace(string = utterance_num, pattern = &amp;quot;utt&amp;quot;, replacement = &amp;quot;&amp;quot;)) %&amp;gt;% # remove &amp;#39;utt&amp;#39;
  mutate(utterance_text = str_replace(string = utterance_text, pattern = &amp;quot;:\\s&amp;quot;, replacement = &amp;quot;&amp;quot;)) %&amp;gt;% # remove &amp;#39;: &amp;#39;
  mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace

glimpse(data) # preview the data set&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 159
## Variables: 6
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;o&amp;quot;, &amp;quot;qw&amp;quot;, &amp;quot;qy^d&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;qy&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;ad&amp;quot;, ...
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B...
## $ turn_num       &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;Okay.  /&amp;quot;, &amp;quot;{D So, }&amp;quot;, &amp;quot;[ [ I guess, +&amp;quot;, &amp;quot;What...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To round out our tidy dataset for this single conversation file we will connect the &lt;code&gt;speaker_a_id&lt;/code&gt; and &lt;code&gt;speaker_b_id&lt;/code&gt; with speaker A and B in our current dataset adding a new column &lt;code&gt;speaker_id&lt;/code&gt;. The &lt;code&gt;case_when()&lt;/code&gt; function does exactly this: allows us to map rows of &lt;code&gt;speaker&lt;/code&gt; with the value “A” to &lt;code&gt;speaker_a_id&lt;/code&gt; and rows with value “B” to &lt;code&gt;speaker_b_id&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- # link speaker with speaker_id
  data %&amp;gt;% 
  mutate(speaker_id = case_when(
    speaker == &amp;quot;A&amp;quot; ~ speaker_a_id,
    speaker == &amp;quot;B&amp;quot; ~ speaker_b_id
  ))

glimpse(data) # preview the data set&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 159
## Variables: 7
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;o&amp;quot;, &amp;quot;qw&amp;quot;, &amp;quot;qy^d&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;qy&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;ad&amp;quot;, ...
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B...
## $ turn_num       &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;Okay.  /&amp;quot;, &amp;quot;{D So, }&amp;quot;, &amp;quot;[ [ I guess, +&amp;quot;, &amp;quot;What...
## $ speaker_id     &amp;lt;chr&amp;gt; &amp;quot;1632&amp;quot;, &amp;quot;1632&amp;quot;, &amp;quot;1519&amp;quot;, &amp;quot;1632&amp;quot;, &amp;quot;1519&amp;quot;, &amp;quot;1632&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We now have the tidy dataset we set out to create. But this dataset only includes on conversation file. We want to apply this code to all 1155 conversation files in the &lt;code&gt;sdac/&lt;/code&gt; corpus. The approach will be to create a custom function which groups the code we’ve done for this single file and then iterative send each file from the corpus through this function and combine the results into one data frame.&lt;/p&gt;
&lt;p&gt;Here’s the function with some extra code to print a progress message for each file when it runs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_sdac_metadata &amp;lt;- function(file) {
  # Function: to read a Switchboard Corpus Dialogue file and extract meta-data
  cat(&amp;quot;Reading&amp;quot;, basename(file), &amp;quot;...&amp;quot;)
  
  # Read `file` by lines
  doc &amp;lt;- read_lines(file) 
  
  # Extract `doc_id`, `speaker_a_id`, and `speaker_b_id`
  doc_speaker_info &amp;lt;- 
    doc[str_detect(doc, &amp;quot;\\d+_\\d+_\\d+&amp;quot;)] %&amp;gt;% # isolate pattern
    str_extract(&amp;quot;\\d+_\\d+_\\d+&amp;quot;) %&amp;gt;% # extract the pattern
    str_split(pattern = &amp;quot;_&amp;quot;) %&amp;gt;% # split the character vector
    unlist() # flatten the list to a character vector
  doc_id &amp;lt;- doc_speaker_info[1] # extract `doc_id`
  speaker_a_id &amp;lt;- doc_speaker_info[2] # extract `speaker_a_id`
  speaker_b_id &amp;lt;- doc_speaker_info[3] # extract `speaker_b_id`
  
  # Extract `text`
  text_start_index &amp;lt;- # find where header info stops
    doc %&amp;gt;% 
    str_detect(pattern = &amp;quot;={3,}&amp;quot;) %&amp;gt;% # match 3 or more `=`
    which() # find vector index
  
  text_start_index &amp;lt;- text_start_index + 1 # increment index by 1
  text_end_index &amp;lt;- length(doc) # get the end of the text section
  
  text &amp;lt;- doc[text_start_index:text_end_index] # extract text
  text &amp;lt;- str_trim(text) # remove leading and trailing whitespace
  text &amp;lt;- text[text != &amp;quot;&amp;quot;] # remove blank lines
  
  data &amp;lt;- data.frame(doc_id, text) # tidy format `doc_id` and `text`
  
  data &amp;lt;- # extract column information from `text`
    data %&amp;gt;% 
    mutate(damsl_tag = str_extract(string = text, pattern = &amp;quot;^.+?\\s&amp;quot;)) %&amp;gt;%  # extract damsl tags
    mutate(speaker_turn = str_extract(string = text, pattern = &amp;quot;[AB]\\.\\d+&amp;quot;)) %&amp;gt;% # extract speaker_turn pairs
    mutate(utterance_num = str_extract(string = text, pattern = &amp;quot;utt\\d+&amp;quot;)) %&amp;gt;% # extract utterance number
    mutate(utterance_text = str_extract(string = text, pattern = &amp;quot;:.+$&amp;quot;)) %&amp;gt;%  # extract utterance text
    select(-text)
  
  data &amp;lt;- # separate speaker_turn into distinct columns
    data %&amp;gt;% 
    separate(col = speaker_turn, into = c(&amp;quot;speaker&amp;quot;, &amp;quot;turn_num&amp;quot;)) 
  
  data &amp;lt;- # clean up column information
    data %&amp;gt;% 
    mutate(damsl_tag = str_trim(damsl_tag)) %&amp;gt;% # remove leading/ trailing whitespace
    mutate(utterance_num = str_replace(string = utterance_num, pattern = &amp;quot;utt&amp;quot;, replacement = &amp;quot;&amp;quot;)) %&amp;gt;% # remove &amp;#39;utt&amp;#39;
    mutate(utterance_text = str_replace(string = utterance_text, pattern = &amp;quot;:\\s&amp;quot;, replacement = &amp;quot;&amp;quot;)) %&amp;gt;% # remove &amp;#39;: &amp;#39;
    mutate(utterance_text = str_trim(utterance_text)) # trim leading/ trailing whitespace
  
  data &amp;lt;- # link speaker with speaker_id
    data %&amp;gt;% 
    mutate(speaker_id = case_when(
      speaker == &amp;quot;A&amp;quot; ~ speaker_a_id,
      speaker == &amp;quot;B&amp;quot; ~ speaker_b_id
    )) 
  cat(&amp;quot; done.\n&amp;quot;)
  return(data) # return the data frame object
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a sanity check we will run the &lt;code&gt;extract_sdac_metadata()&lt;/code&gt; function on a the conversation file we were just working on to make sure it works as expected.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;extract_sdac_metadata(file = &amp;quot;data/original/sdac/sw00utt/sw_0001_4325.utt&amp;quot;) %&amp;gt;% 
  glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Reading sw_0001_4325.utt ... done.
## Observations: 159
## Variables: 7
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;o&amp;quot;, &amp;quot;qw&amp;quot;, &amp;quot;qy^d&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;qy&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;ad&amp;quot;, ...
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B...
## $ turn_num       &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;Okay.  /&amp;quot;, &amp;quot;{D So, }&amp;quot;, &amp;quot;[ [ I guess, +&amp;quot;, &amp;quot;What...
## $ speaker_id     &amp;lt;chr&amp;gt; &amp;quot;1632&amp;quot;, &amp;quot;1632&amp;quot;, &amp;quot;1519&amp;quot;, &amp;quot;1632&amp;quot;, &amp;quot;1519&amp;quot;, &amp;quot;1632&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks good so now it’s time to create a vector with the paths to all of the conversation files. &lt;code&gt;list_files()&lt;/code&gt; interfaces with our OS file system and will return the paths to the files in the specified directory. We also add a pattern to match conversation files (&lt;code&gt;\\.utt&lt;/code&gt;) so we don’t accidently include other files in the corpus. &lt;code&gt;full.names&lt;/code&gt; and &lt;code&gt;recursive&lt;/code&gt; set to &lt;code&gt;TRUE&lt;/code&gt; means we will get the full path to each file and files in all sub-directories will be returned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;files &amp;lt;- 
  list.files(path = &amp;quot;data/original/sdac&amp;quot;, # path to main directory 
             pattern = &amp;quot;\\.utt&amp;quot;, # files to match
             full.names = TRUE, # extract full path to each file
             recursive = TRUE) # drill down in each sub-directory of `sdac/`
head(files) # preview character vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;data/original/sdac/sw00utt/sw_0001_4325.utt&amp;quot;
## [2] &amp;quot;data/original/sdac/sw00utt/sw_0002_4330.utt&amp;quot;
## [3] &amp;quot;data/original/sdac/sw00utt/sw_0003_4103.utt&amp;quot;
## [4] &amp;quot;data/original/sdac/sw00utt/sw_0004_4327.utt&amp;quot;
## [5] &amp;quot;data/original/sdac/sw00utt/sw_0005_4646.utt&amp;quot;
## [6] &amp;quot;data/original/sdac/sw00utt/sw_0006_4108.utt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To pass each conversation file in the vector of paths to our conversation files iteratively to the &lt;code&gt;extract_sdac_metadata()&lt;/code&gt; function we use &lt;code&gt;map()&lt;/code&gt;. This will apply the function to each conversation file and return a data frame for each. &lt;code&gt;bind_rows()&lt;/code&gt; will then join the resulting data frames by rows to give us a single tidy dataset for all 1155 conversations. Note there is a lot of processing going on here so be patient.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read files and return a tidy dataset
sdac &amp;lt;- 
  files %&amp;gt;% # pass file names
  map(extract_sdac_metadata) %&amp;gt;% # read and tidy iteratively 
  bind_rows() # bind the results into a single data frame&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(sdac) # preview the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 223,606
## Variables: 7
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;o&amp;quot;, &amp;quot;qw&amp;quot;, &amp;quot;qy^d&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;qy&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;ad&amp;quot;, ...
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B...
## $ turn_num       &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;Okay.  /&amp;quot;, &amp;quot;{D So, }&amp;quot;, &amp;quot;[ [ I guess, +&amp;quot;, &amp;quot;What...
## $ speaker_id     &amp;lt;chr&amp;gt; &amp;quot;1632&amp;quot;, &amp;quot;1632&amp;quot;, &amp;quot;1519&amp;quot;, &amp;quot;1632&amp;quot;, &amp;quot;1519&amp;quot;, &amp;quot;1632&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-the-tidy-dataset-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explore the tidy dataset&lt;/h3&gt;
&lt;p&gt;It is always a good idea to perform some diagnostics on the data to confirm the integrity of the data. One thing that can go wrong in tidying a dataset, as we’ve done here, is that our pattern matching failed and did not return what we expected. This can be because our patterns were not specific enough or can arise from transcriber/annotator error. In any case this can lead to missing values, or &lt;code&gt;NA&lt;/code&gt; values. R provides the function &lt;code&gt;complete.cases()&lt;/code&gt; to test for &lt;code&gt;NA&lt;/code&gt; values, returning &lt;code&gt;TRUE&lt;/code&gt; for rows in a data frame which do not include &lt;code&gt;NA&lt;/code&gt; values. We can use this to subset the same data set to identify any rows in &lt;code&gt;sdac&lt;/code&gt; dataset that are missing. Note that because we are subsetting a data frame by rows, we will add our expression to the row position in the subsetting operation, i.e. ‘data_frame_name[&lt;strong&gt;row&lt;/strong&gt;, column]’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac[!complete.cases(sdac), ] # check for missing values&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;doc_id&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;damsl_tag&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;speaker&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;turn_num&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;utterance_num&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;utterance_text&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;speaker_id&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Great! No missing data. Now let’s make sure we have captured all 1155 conversation files. We will pipe the &lt;code&gt;sdac$doc_id&lt;/code&gt; column to the &lt;code&gt;unique()&lt;/code&gt; function which returns the unique values of the column. Then we can get the length of that result to find out how many unique conversation files we have in our dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac$doc_id %&amp;gt;% unique() %&amp;gt;% length() # check for unique files&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1155&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also good news, we have all 1155 conversations in the dataset.&lt;/p&gt;
&lt;p&gt;Let’s find out how many individual speakers are in the dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac$speaker_id %&amp;gt;% unique() %&amp;gt;% length() # check for unique speakers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 441&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Good to know before we proceed to adding speaker meta-data from the stand-off file &lt;code&gt;caller_tab.csv&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;running-text-with-stand-off-meta-data-files&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Running text with stand-off meta-data files&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;sdac&lt;/code&gt; dataset now contains various pieces of linguistic and non-linguistic meta-data that we extracted from the conversation files in the &lt;code&gt;sdac/&lt;/code&gt; corpus. As part of that extraction we isolated the ids of the speakers involved in each conversation. As noted during the preliminary investigation portion of the curation of the data, these ids appear in a stand-off meta-data file named &lt;code&gt;caller_tab.csv&lt;/code&gt; in the &lt;a href=&#34;https://catalog.ldc.upenn.edu/docs/LDC97S62/&#34;&gt;online documentation for the Switchboard Dialog Act Corpus&lt;/a&gt;. These ids provide us a link between the corpus data and the speaker meta-data that we can exploit to incorporate that meta-data into our existing &lt;code&gt;sdac&lt;/code&gt; tidy dataset.&lt;/p&gt;
&lt;p&gt;It is common that stand-off meta-data files are in structured format. That is, they will typically be stored in a &lt;code&gt;.csv&lt;/code&gt; file or an &lt;code&gt;.xml&lt;/code&gt; document. The goal then is to read the data into R as a data frame and then join that data with the existing tidy corpus dataset. To read a &lt;code&gt;.csv&lt;/code&gt; file, like the &lt;code&gt;caller_tab.csv&lt;/code&gt; we use the &lt;code&gt;read_csv()&lt;/code&gt; function. Before we read it we should manually download and inspect the data for a couple things: (1) how is the file delimited? and (2) is there a header row that names the columns in the data?&lt;/p&gt;
&lt;p&gt;We can generally assume that a &lt;code&gt;.csv&lt;/code&gt; file will be comma-separated, but this is not always the case sometimes the file will be delimited by semi-colons (&lt;code&gt;;&lt;/code&gt;), tabs (&lt;code&gt;\\t&lt;/code&gt;), or single or multiple spaces (&lt;code&gt;\\s+&lt;/code&gt;). Whether there will be a header row or not can vary. If a header row does not exist in the file itself there is a good chance there is some file that documents what each column in the data represents.&lt;/p&gt;
&lt;p&gt;Let’s take a look at a few rows from the &lt;code&gt;caller_tab.csv&lt;/code&gt; to see what we have.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;1000, 32, &amp;quot;N&amp;quot;, &amp;quot;FEMALE&amp;quot;, 1954, &amp;quot;SOUTH MIDLAND&amp;quot;, 1, 0, &amp;quot;CASH&amp;quot;, 15, &amp;quot;N&amp;quot;, &amp;quot;&amp;quot;, 2, &amp;quot;DN2&amp;quot;
1001, 102, &amp;quot;N&amp;quot;, &amp;quot;MALE&amp;quot;, 1940, &amp;quot;WESTERN&amp;quot;, 3, 0, &amp;quot;GIFT&amp;quot;, 10, &amp;quot;N&amp;quot;, &amp;quot;&amp;quot;, 0, &amp;quot;XP&amp;quot;
1002, 104, &amp;quot;N&amp;quot;, &amp;quot;FEMALE&amp;quot;, 1963, &amp;quot;SOUTHERN&amp;quot;, 2, 0, &amp;quot;GIFT&amp;quot;, 11, &amp;quot;N&amp;quot;, &amp;quot;&amp;quot;, 0, &amp;quot;XP&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First we see that columns are indeed separated by commas. Second we see there is no header row. Some of the columns seem interpretable, like column 4, but we should try to find documentation to guide us. Poking around in the online documentation I noticed the &lt;code&gt;caller_doc.txt&lt;/code&gt; file has names for the columns. This is a file used to generate a database table, but it contains the information we need so we’ll use it to assign names to our columns in &lt;code&gt;caller_tab.csv&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;CREATE TABLE caller (
    caller_no numeric(4) NOT NULL,
    pin numeric(4) NOT NULL,
    target character(1),
    sex character(6),
    birth_year numeric(4),
    dialect_area character(13),
    education numeric(1),
    ti numeric(1),
    payment_type character(5),
    amt_pd numeric(6),
    con character(1),
    remarks character(120),
    calls_deleted numeric(3),
    speaker_partition character(3)
);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can combine this information with the &lt;code&gt;read_csv()&lt;/code&gt; function to read the &lt;code&gt;caller_tab.csv&lt;/code&gt; and add the column names. Note that I’ve changed the &lt;code&gt;caller_no&lt;/code&gt; name to &lt;code&gt;speaker_id&lt;/code&gt; to align the nomenclature with the current &lt;code&gt;sdac&lt;/code&gt; dataset. This renaming will facilitate the upcoming step to join the tidy dataset and this meta-data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac_speaker_meta &amp;lt;- 
  read_csv(file = &amp;quot;https://catalog.ldc.upenn.edu/docs/LDC97S62/caller_tab.csv&amp;quot;, 
           col_names = c(&amp;quot;speaker_id&amp;quot;, # changed from `caller_no`
                         &amp;quot;pin&amp;quot;,
                         &amp;quot;target&amp;quot;,
                         &amp;quot;sex&amp;quot;,
                         &amp;quot;birth_year&amp;quot;,
                         &amp;quot;dialect_area&amp;quot;,
                         &amp;quot;education&amp;quot;,
                         &amp;quot;ti&amp;quot;,
                         &amp;quot;payment_type&amp;quot;,
                         &amp;quot;amt_pd&amp;quot;,
                         &amp;quot;con&amp;quot;,
                         &amp;quot;remarks&amp;quot;,
                         &amp;quot;calls_deleted&amp;quot;,
                         &amp;quot;speaker_partition&amp;quot;))

glimpse(sdac_speaker_meta) # preview the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 543
## Variables: 14
## $ speaker_id        &amp;lt;int&amp;gt; 1000, 1001, 1002, 1003, 1004, 1005, 1007, 10...
## $ pin               &amp;lt;int&amp;gt; 32, 102, 104, 5656, 123, 166, 274, 322, 445,...
## $ target            &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;,...
## $ sex               &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;FEMALE\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;MALE\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;FEMALE\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;M...
## $ birth_year        &amp;lt;int&amp;gt; 1954, 1940, 1963, 1947, 1958, 1956, 1965, 19...
## $ dialect_area      &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;SOUTH MIDLAND\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;WESTERN\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;SOUTH...
## $ education         &amp;lt;int&amp;gt; 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3,...
## $ ti                &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ payment_type      &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;CASH\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;GIFT\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;GIFT\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;NONE\...
## $ amt_pd            &amp;lt;int&amp;gt; 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, ...
## $ con               &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;Y\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;N\&amp;quot;&amp;quot;,...
## $ remarks           &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;\...
## $ calls_deleted     &amp;lt;int&amp;gt; 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,...
## $ speaker_partition &amp;lt;chr&amp;gt; &amp;quot;\&amp;quot;DN2\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;XP\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;XP\&amp;quot;&amp;quot;, &amp;quot;\&amp;quot;DN2\&amp;quot;&amp;quot;, &amp;quot;\...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The columns mapped to the data as expected. The character columns contain double quotes (&lt;code&gt;&amp;quot;\&amp;quot;&lt;/code&gt;), however. We could proceed without issue (R will treat them as character values just the same) but I would like to clean up the character values for aesthetic purposes. To do this I applied the following code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac_speaker_meta &amp;lt;- # remove double quotes
  sdac_speaker_meta %&amp;gt;% 
  map(str_replace_all, pattern = &amp;#39;&amp;quot;&amp;#39;, replacement = &amp;#39;&amp;#39;) %&amp;gt;% # iteratively remove doubled quotes
  bind_rows() %&amp;gt;%  # combine the results by rows
  type_convert() # return columns to orignal data types

glimpse(sdac_speaker_meta) # preview the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 543
## Variables: 14
## $ speaker_id        &amp;lt;int&amp;gt; 1000, 1001, 1002, 1003, 1004, 1005, 1007, 10...
## $ pin               &amp;lt;int&amp;gt; 32, 102, 104, 5656, 123, 166, 274, 322, 445,...
## $ target            &amp;lt;chr&amp;gt; &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;,...
## $ sex               &amp;lt;chr&amp;gt; &amp;quot;FEMALE&amp;quot;, &amp;quot;MALE&amp;quot;, &amp;quot;FEMALE&amp;quot;, &amp;quot;MALE&amp;quot;, &amp;quot;FEMALE&amp;quot;...
## $ birth_year        &amp;lt;int&amp;gt; 1954, 1940, 1963, 1947, 1958, 1956, 1965, 19...
## $ dialect_area      &amp;lt;chr&amp;gt; &amp;quot;SOUTH MIDLAND&amp;quot;, &amp;quot;WESTERN&amp;quot;, &amp;quot;SOUTHERN&amp;quot;, &amp;quot;NOR...
## $ education         &amp;lt;int&amp;gt; 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3,...
## $ ti                &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ payment_type      &amp;lt;chr&amp;gt; &amp;quot;CASH&amp;quot;, &amp;quot;GIFT&amp;quot;, &amp;quot;GIFT&amp;quot;, &amp;quot;NONE&amp;quot;, &amp;quot;GIFT&amp;quot;, &amp;quot;GIF...
## $ amt_pd            &amp;lt;int&amp;gt; 15, 10, 11, 7, 11, 22, 20, 3, 11, 9, 25, 9, ...
## $ con               &amp;lt;chr&amp;gt; &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;N&amp;quot;, &amp;quot;Y&amp;quot;, &amp;quot;N&amp;quot;,...
## $ remarks           &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, ...
## $ calls_deleted     &amp;lt;int&amp;gt; 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,...
## $ speaker_partition &amp;lt;chr&amp;gt; &amp;quot;DN2&amp;quot;, &amp;quot;XP&amp;quot;, &amp;quot;XP&amp;quot;, &amp;quot;DN2&amp;quot;, &amp;quot;XP&amp;quot;, &amp;quot;ET&amp;quot;, &amp;quot;DN1&amp;quot;,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the preview of the &lt;code&gt;sdac_speaker_meta&lt;/code&gt; dataset we can see that there are 14 columns, including the &lt;code&gt;speaker_id&lt;/code&gt;. We also see that there are 543 observations. We can assume that each row corresponds to an individual speaker, but to make sure let’s find the length of the unique values of &lt;code&gt;sdac_speaker_meta$speaker_id&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac_speaker_meta$speaker_id %&amp;gt;% unique() %&amp;gt;% length() # check for unique speakers&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 543&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So this confirms each row in &lt;code&gt;sdac_speaker_meta&lt;/code&gt; corresponds to an individual speaker. It is also clear now that the &lt;code&gt;sdac&lt;/code&gt; dataset, which contains 441 individual speakers, is a subset of all the data collected in the Switchboard Corpus project.&lt;/p&gt;
&lt;p&gt;Let’s select the columns that seem most interesting for a future analysis dropping the other columns. The &lt;code&gt;select()&lt;/code&gt; function allows us to specify columns to keep (or drop).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac_speaker_meta &amp;lt;- # select columns of interest
  sdac_speaker_meta %&amp;gt;% 
  select(speaker_id, sex, birth_year, dialect_area, education)

glimpse(sdac_speaker_meta) # preview the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 543
## Variables: 5
## $ speaker_id   &amp;lt;int&amp;gt; 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1...
## $ sex          &amp;lt;chr&amp;gt; &amp;quot;FEMALE&amp;quot;, &amp;quot;MALE&amp;quot;, &amp;quot;FEMALE&amp;quot;, &amp;quot;MALE&amp;quot;, &amp;quot;FEMALE&amp;quot;, &amp;quot;FE...
## $ birth_year   &amp;lt;int&amp;gt; 1954, 1940, 1963, 1947, 1958, 1956, 1965, 1939, 1...
## $ dialect_area &amp;lt;chr&amp;gt; &amp;quot;SOUTH MIDLAND&amp;quot;, &amp;quot;WESTERN&amp;quot;, &amp;quot;SOUTHERN&amp;quot;, &amp;quot;NORTH MI...
## $ education    &amp;lt;int&amp;gt; 1, 3, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 3, 3, 2...&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;tidy-the-corpus-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tidy the corpus&lt;/h3&gt;
&lt;p&gt;The next step is to join the two datasets linking the values of &lt;code&gt;sdac$speaker_id&lt;/code&gt; with the values of &lt;code&gt;sdac_speaker_meta$speaker_id&lt;/code&gt;. We want to keep all the data in the &lt;code&gt;sdac&lt;/code&gt; dataset and only include data from &lt;code&gt;sdac_speaker_meta&lt;/code&gt; where there are matching speaker ids. To do this we use the &lt;code&gt;left_join()&lt;/code&gt; function. &lt;code&gt;left_join()&lt;/code&gt; requires two arguments which correspond to two data frames. We can optionally specify which column(s) to use as the columns to use as the joining condition, but by default it will use any column names that match in the two data frames. In our case the only column that matches is the &lt;code&gt;speaker_id&lt;/code&gt; column so we can proceed without explicitly specifying the join column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac &amp;lt;- left_join(sdac, sdac_speaker_meta) # join by `speaker_id`&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in left_join_impl(x, y, by$x, by$y, suffix$x, suffix$y, check_na_matches(na_matches)): Can&amp;#39;t join on &amp;#39;speaker_id&amp;#39; x &amp;#39;speaker_id&amp;#39; because of incompatible types (integer / character)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get an error! Reading the error it appears we are trying to join columns of differing data types; the &lt;code&gt;sdac$speaker_id&lt;/code&gt; is of type character and &lt;code&gt;sdac_speaker_meta$speaker_id&lt;/code&gt; is of type integer.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Error messages can be difficult to make sense of. If the issue is not clear to you, copy the error and search the web to see if others have had the same issue. Chances are someone has! If not, &lt;a href=&#34;https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example&#34;&gt;follow these steps to create a reproducible example&lt;/a&gt; and post it to a site such as &lt;a href=&#34;https://stackoverflow.com&#34;&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;To remedy the situation we need to coerce the &lt;code&gt;sdac$speaker_id&lt;/code&gt; column into a integer. The &lt;code&gt;as.numeric()&lt;/code&gt; function will do this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac$speaker_id &amp;lt;- sdac$speaker_id %&amp;gt;% as.numeric() # convert to integer&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s apply our join operation again.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac &amp;lt;- left_join(sdac, sdac_speaker_meta) # join by `speaker_id`

glimpse(sdac) # preview the joined dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 223,606
## Variables: 11
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;, &amp;quot;4325&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;o&amp;quot;, &amp;quot;qw&amp;quot;, &amp;quot;qy^d&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;qy&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;ad&amp;quot;, ...
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B&amp;quot;, &amp;quot;B...
## $ turn_num       &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6&amp;quot;, &amp;quot;6...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;4...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;Okay.  /&amp;quot;, &amp;quot;{D So, }&amp;quot;, &amp;quot;[ [ I guess, +&amp;quot;, &amp;quot;What...
## $ speaker_id     &amp;lt;dbl&amp;gt; 1632, 1632, 1519, 1632, 1519, 1632, 1519, 1519,...
## $ sex            &amp;lt;chr&amp;gt; &amp;quot;FEMALE&amp;quot;, &amp;quot;FEMALE&amp;quot;, &amp;quot;FEMALE&amp;quot;, &amp;quot;FEMALE&amp;quot;, &amp;quot;FEMALE...
## $ birth_year     &amp;lt;int&amp;gt; 1962, 1962, 1971, 1962, 1971, 1962, 1971, 1971,...
## $ dialect_area   &amp;lt;chr&amp;gt; &amp;quot;WESTERN&amp;quot;, &amp;quot;WESTERN&amp;quot;, &amp;quot;SOUTH MIDLAND&amp;quot;, &amp;quot;WESTERN...
## $ education      &amp;lt;int&amp;gt; 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Result! Now let’s check our data for any missing data points generated in the join.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac[!complete.cases(sdac), ] %&amp;gt;% glimpse # view incomplete cases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 100
## Variables: 11
## $ doc_id         &amp;lt;chr&amp;gt; &amp;quot;3554&amp;quot;, &amp;quot;3554&amp;quot;, &amp;quot;3554&amp;quot;, &amp;quot;3554&amp;quot;, &amp;quot;3554&amp;quot;, &amp;quot;3554&amp;quot;,...
## $ damsl_tag      &amp;lt;chr&amp;gt; &amp;quot;sd@&amp;quot;, &amp;quot;+@&amp;quot;, &amp;quot;sv@&amp;quot;, &amp;quot;+@&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;sd&amp;quot;, &amp;quot;+&amp;quot;, &amp;quot;+&amp;quot;, ...
## $ speaker        &amp;lt;chr&amp;gt; &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;A...
## $ turn_num       &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;7&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;9&amp;quot;, &amp;quot;11&amp;quot;, &amp;quot;13&amp;quot;, &amp;quot;13&amp;quot;,...
## $ utterance_num  &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;1...
## $ utterance_text &amp;lt;chr&amp;gt; &amp;quot;Of a exercise program you have.&amp;quot;, &amp;quot;Right. /&amp;quot;, ...
## $ speaker_id     &amp;lt;dbl&amp;gt; 155, 155, 155, 155, 155, 155, 155, 155, 155, 15...
## $ sex            &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...
## $ birth_year     &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...
## $ dialect_area   &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...
## $ education      &amp;lt;int&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have 100 observations that are missing data. Inspecting the dataset preview it appears that there was at least one &lt;code&gt;speaker_id&lt;/code&gt; that appears in the conversation files that does not appear in the speaker meta-data. Let’s check to see how many speakers this might affect.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac[!complete.cases(sdac), ] %&amp;gt;% select(speaker_id) %&amp;gt;% unique()&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;speaker_id&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;dbl&#34;],&#34;align&#34;:[&#34;right&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;155&#34;,&#34;_rn_&#34;:&#34;167190&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Just one speaker. This could very well be annotator error. Since it effects a relatively small proportion of the data, let’s drop this speaker from the dataset. We can use the &lt;code&gt;filter()&lt;/code&gt; function to select the values of &lt;code&gt;speaker_id&lt;/code&gt; that are not equal to &lt;code&gt;155&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac &amp;lt;- # remove rows where speaker_id == 155
  sdac %&amp;gt;% 
  filter(speaker_id != 155)

sdac[!complete.cases(sdac), ] %&amp;gt;% glimpse # view incomplete cases&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 0
## Variables: 11
## $ doc_id         &amp;lt;chr&amp;gt; 
## $ damsl_tag      &amp;lt;chr&amp;gt; 
## $ speaker        &amp;lt;chr&amp;gt; 
## $ turn_num       &amp;lt;chr&amp;gt; 
## $ utterance_num  &amp;lt;chr&amp;gt; 
## $ utterance_text &amp;lt;chr&amp;gt; 
## $ speaker_id     &amp;lt;dbl&amp;gt; 
## $ sex            &amp;lt;chr&amp;gt; 
## $ birth_year     &amp;lt;int&amp;gt; 
## $ dialect_area   &amp;lt;chr&amp;gt; 
## $ education      &amp;lt;int&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-the-tidy-dataset-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Explore the tidy dataset&lt;/h3&gt;
&lt;p&gt;At this point we have a well-curated dataset which includes linguistic and non-linguistic meta-data. As we did for the ACTIV-ES corpus, let’s get a sense of the distribution of some of the meta-data.&lt;/p&gt;
&lt;p&gt;First we will visualize the number of utterances from speakers of the different dialect regions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac %&amp;gt;% 
  group_by(dialect_area) %&amp;gt;% 
  count() %&amp;gt;%
  ggplot(aes(x = dialect_area, y = n)) + 
  geom_col() +
  labs(x = &amp;quot;Dialect region&amp;quot;, y = &amp;quot;Utterance count&amp;quot;, title = &amp;quot;Switchboard Dialog Act Corpus&amp;quot;, subtitle = &amp;quot;Utterances per dialect region&amp;quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://francojc.github.io/post/2017-12-01-curate-language-data-organizing-metadata_files/figure-html/sdac-dialect-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s see how men and women figure across the dialect areas.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sdac %&amp;gt;% 
  group_by(dialect_area, sex) %&amp;gt;% 
  count() %&amp;gt;% 
  ggplot(aes(x = dialect_area, y = n, fill = sex)) + 
  geom_col() +
  labs(x = &amp;quot;Dialect region&amp;quot;, y = &amp;quot;Utterance count&amp;quot;, title = &amp;quot;Switchboard Dialog Act Corpus&amp;quot;, subtitle = &amp;quot;Utterances per dialect region and sex&amp;quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://francojc.github.io/post/2017-12-01-curate-language-data-organizing-metadata_files/figure-html/sdac-dialect-sex-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There are many other ways to group and count the dataset but I’ll leave that to you to look at!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post I covered tidying a corpus from running text files. We looked at three cases where meta-data is typically stored: in filenames, embedded inline with the text itself, and in stand-off files. As usual we made extensive use of the tidyverse package set (readr, dplyr, ggplot2, etc.) and included discussion of other packages: readtext for reading and organizing meta-data from file names, tidytext for tokenizing text, and stringr for text cleaning and pattern matching. I also briefly introduced the ggplot2 package for creating plots based on the Grammar of Graphics philosophy. Along the way we continued to extend our knowledge of R data and object types working with vectors, data frames, and lists manipulating them in various ways (subsetting, sorting, transforming, and summarizing).&lt;/p&gt;
&lt;p&gt;In the next post I will turn to working with meta-data in structured documents, specifically &lt;code&gt;.xml&lt;/code&gt; documents. These type of documents tend to have rich meta-data including linguistic and non-linguistic information. We will focus on working with linguistic annnotations such as part-of-speech and syntactic structure. We will work to parse the linguistic information in these documents into a tidy dataset and also see how to create linguistic annotations for data does not already contain them.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-readtext&#34;&gt;
&lt;p&gt;Benoit, Kenneth, and Adam Obeng. 2017. &lt;em&gt;Readtext: Import and Handling for Plain and Formatted Text Files&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=readtext&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=readtext&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-purrr&#34;&gt;
&lt;p&gt;Henry, Lionel, and Hadley Wickham. 2017. &lt;em&gt;Purrr: Functional Programming Tools&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=purrr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=purrr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidytext&#34;&gt;
&lt;p&gt;Robinson, David, and Julia Silge. 2018. &lt;em&gt;Tidytext: Text Mining Using ’Dplyr’, ’Ggplot2’, and Other Tidy Tools&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidytext&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidytext&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-stringr&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2018. &lt;em&gt;Stringr: Simple, Consistent Wrappers for Common String Operations&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=stringr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-ggplot2&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Winston Chang. 2018. &lt;em&gt;Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-dplyr&#34;&gt;
&lt;p&gt;Wickham, Hadley, Romain Francois, Lionel Henry, and Kirill Müller. 2017. &lt;em&gt;Dplyr: A Grammar of Data Manipulation&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=dplyr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=dplyr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-readr&#34;&gt;
&lt;p&gt;Wickham, Hadley, Jim Hester, and Romain Francois. 2017. &lt;em&gt;Readr: Read Rectangular Text Data&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=readr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=readr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://regex101.com&#34; class=&#34;uri&#34;&gt;https://regex101.com&lt;/a&gt; is a great place to learn more about Regular Expressions and to practice using them.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (3/3): web scraping </title>
      <link>https://francojc.github.io/2017/11/02/acquiring-data-for-language-research-web-scraping/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/11/02/acquiring-data-for-language-research-web-scraping/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;!-- TODO:
--&gt;
&lt;div id=&#34;web-scraping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Web scraping&lt;/h2&gt;
&lt;p&gt;There are many resources available through direct downloads from repositories and individual sites and R package interfaces to web resources with APIs, but these resources are relatively limited to the amount of public-facing textual data recorded on the web. In the case that you want to acquire data from webpages R can be used to access the web programmatically through a process known as web scraping. The complexity of web scrapes can vary but in general it requires more advanced knowledge of R as well as the structure of the language of the web: HTML (Hypertext Markup Language).&lt;/p&gt;
&lt;div id=&#34;a-toy-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A toy example&lt;/h3&gt;
&lt;p&gt;HTML is a cousin of XML and as such organizes web documents in a hierarchical format that is read by your browser as you navigate the web. Take for example the toy webpage I created for this demonstration in Figure &lt;a href=&#34;#fig:example-webpage&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:example-webpage&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/example-webpage-1.png&#34; alt=&#34;Example web page.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Example web page.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The file accessed by my browser to render this webpage is &lt;code&gt;test.html&lt;/code&gt; and in plain-text format looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
&amp;lt;html&amp;gt;
  &amp;lt;head&amp;gt;
    &amp;lt;title&amp;gt;My website&amp;lt;/title&amp;gt;
  &amp;lt;/head&amp;gt;
  &amp;lt;body&amp;gt;
    &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;
      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;
      &amp;lt;p&amp;gt;This is my first website. &amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;table&amp;gt;
      &amp;lt;tr&amp;gt;
        &amp;lt;td&amp;gt;Contact me:&amp;lt;/td&amp;gt;
        &amp;lt;td&amp;gt;
          &amp;lt;a href=&amp;quot;mailto:francojc@wfu.edu&amp;quot;&amp;gt;francojc@wfu.edu&amp;lt;/a&amp;gt;
        &amp;lt;/td&amp;gt;
      &amp;lt;/tr&amp;gt;
    &amp;lt;/table&amp;gt;
    &amp;lt;div class=&amp;quot;conc&amp;quot;&amp;gt;
      &amp;lt;p&amp;gt;Good-bye!&amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each element in this file is delineated by an opening and closing tag, &lt;code&gt;&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&lt;/code&gt;. Tags are nested within other tags to create the structural hierarchy. Tags can take class and id labels to distinguish them from other tags and often contain other attributes that dictate how the tag is to behave when rendered visually by a browser. For example, there are two &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; tags in our toy example: one has the label &lt;code&gt;class = &amp;quot;intro&amp;quot;&lt;/code&gt; and the other &lt;code&gt;class = &amp;quot;conc&amp;quot;&lt;/code&gt;. &lt;code&gt;&amp;lt;div&amp;gt;&lt;/code&gt; tags are often used to separate sections of a webpage that may require special visual formatting. The &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; tag, on the other hand, creates a web link. As part of this tag’s function, it requires the attribute &lt;code&gt;href=&lt;/code&gt; and a web protocol –in this case it is a link to an email address &lt;code&gt;mailto:francojc@wfu.edu&lt;/code&gt;. More often than not, however, the &lt;code&gt;href=&lt;/code&gt; contains a URL (Uniform Resource Locator). A working example might look like this: &lt;code&gt;&amp;lt;a href=&amp;quot;https://francojc.github.io/&amp;quot;&amp;gt;My homepage&amp;lt;/a&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The aim of a web scrape is to download the HTML file, parse the document structure, and extract the elements containing the relevant information we wish to capture. Let’s attempt to extract some information from our toy example. To do this we will need the &lt;a href=&#34;https://CRAN.R-project.org/package=rvest&#34;&gt;rvest&lt;/a&gt; package. First, install/load the package, then, read and parse the HTML from the character vector named &lt;code&gt;web_file&lt;/code&gt; assigning the result to &lt;code&gt;html&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pacman::p_load(rvest) # install/ load `rvest`
html &amp;lt;- read_html(web_file) # read the raw html
html&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_document}
## &amp;lt;html&amp;gt;
## [1] &amp;lt;head&amp;gt;\n&amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset= ...
## [2] &amp;lt;body&amp;gt;\n    &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;\n      &amp;lt;p&amp;gt;Thi ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;read_html()&lt;/code&gt; parses the raw HTML into an object of class &lt;code&gt;xml_document&lt;/code&gt;. The summary output above shows that tags the HTML structure have been parsed into ‘nodes’. The tag nodes can be accessed by using the &lt;code&gt;html_nodes()&lt;/code&gt; function by specifying the tag to isolate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (2)}
## [1] &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;\n      &amp;lt;p&amp;gt;This is my firs ...
## [2] &amp;lt;div class=&amp;quot;conc&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Good-bye!&amp;lt;/p&amp;gt;\n    &amp;lt;/div&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The &lt;code&gt;%&amp;gt;%&lt;/code&gt; operator is used to ‘pipe’ the output of one R operation to the input of the next operation. Piping is equivalent to embedding functions but tends to lead to more legible code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(1:5) # embedding example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:5 %&amp;gt;% sum() # piping example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 15&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default the subsequent function assumes that the output will be used as the first argument. If this is not the case, the &lt;code&gt;.&lt;/code&gt; operator can be used to match the output to the correct argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1:5 %&amp;gt;% paste(&amp;quot;Number&amp;quot;, .) # directing output with &lt;code&gt;.&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number 1&amp;quot; &amp;quot;Number 2&amp;quot; &amp;quot;Number 3&amp;quot; &amp;quot;Number 4&amp;quot; &amp;quot;Number 5&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Notice that &lt;code&gt;html_nodes(&amp;quot;div&amp;quot;)&lt;/code&gt; has returned both &lt;code&gt;div&lt;/code&gt; tags. To isolate one of tags by its class, we add the class name to the tag separating it with a &lt;code&gt;.&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;div class=&amp;quot;intro&amp;quot;&amp;gt;\n      &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;\n      &amp;lt;p&amp;gt;This is my firs ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great. Now say we want to drill down and isolate the subordinate &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; nodes. We can add &lt;code&gt;p&lt;/code&gt; to our node filter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro p&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (2)}
## [1] &amp;lt;p&amp;gt;Welcome!&amp;lt;/p&amp;gt;
## [2] &amp;lt;p&amp;gt;This is my first website. &amp;lt;/p&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract the text contained within a node we use the &lt;code&gt;html_text()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro p&amp;quot;) %&amp;gt;% 
  html_text()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Welcome!&amp;quot;                   &amp;quot;This is my first website. &amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result is a character vector with two elements corresponding to the text contained in each &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tag. If you were paying close attention you might have noticed that the second element in our vector includes extra whitespace after the period. To trim leading and trailing whitespace from text we can add the &lt;code&gt;trim = TRUE&lt;/code&gt; argument to &lt;code&gt;html_text()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.intro p&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Welcome!&amp;quot;                  &amp;quot;This is my first website.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From here we would then work to organize the text into a format we want to store it in and write the results to disk. Let’s leave writing data to disk for later in the post. For now keep our focus on working with &lt;code&gt;rvest&lt;/code&gt; to acquire data from html documents working with a more practical example.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-practical-example&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A practical example&lt;/h3&gt;
&lt;p&gt;With some basic understanding of HTML and how to use the &lt;code&gt;rvest&lt;/code&gt; package, let’s turn to a realistic example. Say we want to acquire text from the Spanish news site &lt;a href=&#34;https://elpais.com/&#34;&gt;elpais.com&lt;/a&gt;. The first step in any web scrape is to investigate the site and page(s) we want to scrape. Minimally this includes identifying the URL we want to target and exploring the structure of the HTML document. Take the following webpage I have identified, seen in Figure &lt;a href=&#34;#fig:example-page-elpais&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:example-page-elpais&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/example-page-elpais-1.png&#34; alt=&#34;Content page from the Spanish new site El País.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Content page from the Spanish new site El País.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As in our toy example, first we want to feed the HTML document to the &lt;code&gt;read_html()&lt;/code&gt; function to parse the tags into nodes. In this case we will assign the web address to the variable &lt;code&gt;url&lt;/code&gt;. &lt;code&gt;read_html()&lt;/code&gt; will automatically connect to the web and download the raw html.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;https://elpais.com/elpais/2017/10/17/opinion/1508258340_992960.html&amp;quot;
html &amp;lt;- read_html(url)
html&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_document}
## &amp;lt;html lang=&amp;quot;es&amp;quot;&amp;gt;
## [1] &amp;lt;head&amp;gt;\n&amp;lt;meta http-equiv=&amp;quot;Content-Type&amp;quot; content=&amp;quot;text/html; charset= ...
## [2] &amp;lt;body id=&amp;quot;salida_articulo&amp;quot; class=&amp;quot;salida_articulo salida_articulo_op ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have captured and parsed the raw HTML assigning it to the object named &lt;code&gt;html&lt;/code&gt;. The next step is to identify the node or nodes that contain the information we want to extract from the page. To do this it is helpful to use a browser to inspect specific elements of the webpage. Your browser will be equipped with a command that you can enable by hovering your mouse over the element of the page you want to target and using a right click to select “Inspect Element”. This will split your browser window horizontally showing you the raw HTML underlying the webpage.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:inspect-element&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/inspect-element-1.png&#34; alt=&#34;Using the &amp;quot;Inspect Element&amp;quot; command to explore raw html.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Using the “Inspect Element” command to explore raw html.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From Figure &lt;a href=&#34;#fig:inspect-element&#34;&gt;3&lt;/a&gt; we see that the node we want to target is &lt;code&gt;h1&lt;/code&gt;. Now this tag is common and we don’t want to extract every &lt;code&gt;h1&lt;/code&gt; so we use the class &lt;code&gt;articulo-titulo&lt;/code&gt; to specify we only want the title of the article. Using the convention described in our toy example, we can isolate the title of the page.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;h1.articulo-titulo&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;h1 class=&amp;quot;articulo-titulo &amp;quot; id=&amp;quot;articulo-titulo&amp;quot; itemprop=&amp;quot;headline ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can then extract the text with &lt;code&gt;html_text()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;title &amp;lt;- 
  html %&amp;gt;% 
  html_nodes(&amp;quot;h1.articulo-titulo&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)
title&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Crímenes contra el periodismo en el seno de la UE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s extract the author’s name and the article text in the same way.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Author
author &amp;lt;- 
  html %&amp;gt;% 
  html_node(&amp;quot;span.autor-nombre&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)
# Article text
text &amp;lt;- 
  html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-cuerpo p&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another piece of information we might want to include in our web scrape is the date the article was published. Again, we use the “Inspect Element” tool in your browser to locate the tag we intend to isolate. This time, however, the information that returned by &lt;code&gt;html_text()&lt;/code&gt; is less than ideal –the date is inter-spliced with text formatting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-datos time&amp;quot;) %&amp;gt;% 
  html_text(trim = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;18 OCT 2017 - 14:26\t\t\t\t\tCEST&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looking at the &lt;code&gt;time&lt;/code&gt; node provides another angle: a clean date is contained as the &lt;code&gt;datetime&lt;/code&gt; attribute of the &lt;code&gt;time&lt;/code&gt; tag.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-datos time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## {xml_nodeset (1)}
## [1] &amp;lt;time datetime=&amp;quot;2017-10-18T14:26:30+02:00&amp;quot; class=&amp;quot;articulo-actualiza ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To extract a tag’s attribute we use the &lt;code&gt;html_attr()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Date
date &amp;lt;- 
  html %&amp;gt;% 
  html_nodes(&amp;quot;div.articulo-datos time&amp;quot;) %&amp;gt;% 
  html_attr(&amp;quot;datetime&amp;quot;)
date&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2017-10-18T14:26:30+02:00&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point, we have isolated and extracted the title, author, date, and text from the webpage. Each of these elements are stored in character vectors in our R session. To complete our task we need to write this data to disk as plain text. With an eye towards a tidy dataset, an ideal format to store the data is in a CSV file where each column corresponds to one of the elements from our scrape and each row an observation. The observations will contain the text from each &lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tag. A CSV file is a tabular format and so before we can write the data to disk let’s coerce the data that we have into tabular format. We will use the &lt;code&gt;tibble()&lt;/code&gt; function here to streamline our data frame creation.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Feeding each of the vectors &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;author&lt;/code&gt;, &lt;code&gt;date&lt;/code&gt;, and &lt;code&gt;text&lt;/code&gt; as arguments to &lt;code&gt;tibble()&lt;/code&gt; creates the tabular format we are looking for.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(title, author, date, text)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;title&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;author&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;date&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;text&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;México, Irak y Siria encabezan de manera destacada la lista de los países más peligrosos para los periodistas; allí donde los profesionales de la información están especialmente expuestos a la violencia, figuran en la diana de los conflictos bélicos o su trabajo se ve cercenado por Gobiernos totalitarios. El barómetro de las violaciones de la libertad de prensa de Reporteros sin Fronteras contabiliza en lo que va de año 11 crímenes en México, 8 en Siria y 7 en Irak. Yemen, Afganistán, Honduras, Brasil o Somalia aparecen también entre los Estados en los que los informadores son vilmente asesinados. Cuando parecía que la Unión Europea estaba libre de este tipo de ataques atroces a la libertad de prensa, dos países —Dinamarca y Malta— han pasado a engrosar la lista de la vergüenza.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Una bomba lapa situada en su coche acabó esta semana brutalmente con la vida de la periodista maltesa Daphne Caruana Galizia, de 53 años. Estaba involucrada en una investigación sobre los papeles de Malta, una derivación de los llamados papeles de Panamá,que revelaron en mayo cómo la pequeña isla mediterránea se había convertido en un paraíso fiscal dentro de la propia UE. Sus indagaciones salpicaron a la esposa del primer ministro y a varios miembros del Ejecutivo. Abocaron a un adelanto electoral y, pese a las revelaciones, el laborista Joseph Muscat volvió a ganar en junio.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Caruana Galizia, la víctima mortal número 41 computada por RSF en lo que va de año, estaba en el punto de mira. Pocos días antes de ser asesinada presentó una denuncia en la que aseguraba haber recibido amenazas de muerte. Ahora su hijo culpa al Gobierno de Muscat de permitir el crimen, la corrupción y una cultura de impunidad. “Mi madre ha sido asesinada porque se interponía entre el Estado de derecho y quienes quieren violarlo, como muchos otros fuertes periodistas”, ha denunciado Matthew Caruana Galizia.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;En el otro extremo de la UE, en la costa sur de Copenhague, la policía encontró a finales de agosto parte del cuerpo de la periodista sueca Kim Wall, de 30 años, que según todos los indicios fue asesinada cuando se encontraba a bordo de un submarino para realizar un reportaje. Su cadáver, mutilado salvajemente, fue hallado en el mar Báltico. Peter Madsen, excéntrico inventor y propietario del sumergible Nautilus, ha sido acusado de homicidio.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Crímenes destinados a acallar la voz de la prensa son moneda común en los países donde el narcotráfico, los paramilitares o los Estados corruptos se han hecho fuertes. Pero que estos ataques se produzcan en el seno de la Unión Europea son una noticia inquietante. La Comisión Europea, con su presidente, Jean-Claude Juncker en primera fila, se ha apresurado a condenar el asesinato de la reportera maltesa con una contundente declaración de intenciones: “El derecho de un periodista a investigar, hacer preguntas incómodas e informar de manera efectiva está en el corazón de nuestros valores y debe garantizarse siempre”.&#34;},{&#34;1&#34;:&#34;Crímenes contra el periodismo en el seno de la UE&#34;,&#34;2&#34;:&#34;Rosario G. Gómez&#34;,&#34;3&#34;:&#34;2017-10-18T14:26:30+02:00&#34;,&#34;4&#34;:&#34;Puedes seguir EL PAÍS Opinión en Facebook, Twitter o suscribirte aquí a la Newsletter.&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;Notice that there are six rows in this data frame, one corresponding to each paragraph in &lt;code&gt;text&lt;/code&gt;. R has a bias towards working with vectors of the same length. As such each of the other vectors (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;author&lt;/code&gt;, and &lt;code&gt;date&lt;/code&gt;) are replicated, or recycled, until they are the same length as the longest vector &lt;code&gt;text&lt;/code&gt;, which a length of six.&lt;/p&gt;
&lt;p&gt;For good documentation let’s add our object &lt;code&gt;url&lt;/code&gt; to the data frame, which contains the actual web link to this page, and assign the result to &lt;code&gt;webpage_data&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;webpage_data &amp;lt;- tibble(title, author, date, text, url)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The final step is to write this data to disk. To do this we will use the &lt;code&gt;write_csv()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(x = webpage_data, path = &amp;quot;data/original/elpais_webpage.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-all-together&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting it all together&lt;/h3&gt;
&lt;p&gt;At this point you may be think, ‘Great, I can download data from a single page, but what about downloading multiple pages?’ Good question. That’s really where the strength of a programming approach takes hold. Extracting information from multiple pages is not fundamentally different than working with a single page. However, it does require more sophisticated code. I will not document the code in this post but you are encouraged to download the GitHub repository which contains the working code and peruse the &lt;code&gt;functions/aquire_functions.R&lt;/code&gt; script to see the details and replicate the processing covered here. Yet I will give you a gist of the steps taken to scrape multiple pages from the El País website.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier in this section, the first step in any web scrape is to investigate the structure of the site and page(s) we want to scrape. The El País site is organized such that each article is ‘tagged’ with some meta-category. After doing some browsing on their site, I discovered there is a searchable archive page that lists all the ‘tags’ used on the site. By selecting a tag, a paginated interface listing all of the articles associated with said tag is made available.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:read-archives-elpais&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-11-02-acquiring-data-for-language-research-3-3-web-scraping_files/figure-html/read-archives-elpais-1.png&#34; alt=&#34;El País archives page for the `politica` tag.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: El País archives page for the &lt;code&gt;politica&lt;/code&gt; tag.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In a nutshell, the approach then is to leverage these archives to harvest links to article pages with a specific tag, download the content of these links and then organize and write the data to disk in CSV format. In more detail I’ve provided concrete steps with the custom functions I wrote to accomplish each:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Get the total number of archive pages available.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Includes an optional argument &lt;code&gt;sample_size&lt;/code&gt; to specify the number of archive pages to harvest links from. The default is &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_archive_pages &amp;lt;- function(tag_name, sample_size = 1) {
  # Function: Scrape tag main page and return selected number of archive pages
  url &amp;lt;- paste0(&amp;quot;https://elpais.com/tag/&amp;quot;, tag_name)
  html &amp;lt;- read_html(url) # load html from selected url
  pages_available &amp;lt;- 
    html %&amp;gt;% # pass html
    html_node(&amp;quot;li.paginacion-siguiente a&amp;quot;) %&amp;gt;% # isolate &amp;#39;next page&amp;#39; link
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% # extract &amp;#39;next page&amp;#39; link
    str_extract(&amp;quot;\\d+$&amp;quot;) %&amp;gt;% # extract the numeric value (num pages of links) in link
    as.numeric() + 1 # covert to a numeric vector and add 1 (to include first page)
  cat(pages_available, &amp;quot;pages available for the&amp;quot;, tag_name, &amp;quot;tag.\n&amp;quot;)
  archive_pages &amp;lt;- paste0(url, &amp;quot;/a/&amp;quot;, (pages_available - (sample_size - 1)):pages_available) # compile urls
  cat(sample_size, &amp;quot;pages selected.\n&amp;quot;)
  return(archive_pages)
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Harvest the links to the content pages.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;str_replace()&lt;/code&gt; function from the &lt;a href=&#34;https://CRAN.R-project.org/package=stringr&#34;&gt;stringr&lt;/a&gt; library is used here to create valid URLs by replacing the &lt;code&gt;//&lt;/code&gt; with &lt;code&gt;https://&lt;/code&gt; in the links harvested directly from the webpage.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_content_links &amp;lt;- function(url) {
  # Function: Scrape the content links from a tag archive page
  html &amp;lt;- read_html(url) # load html from selected url
  urls &amp;lt;- 
    html %&amp;gt;% # pass html
    html_nodes(&amp;quot;h2.articulo-titulo a&amp;quot;) %&amp;gt;% # isolate links
    html_attr(&amp;quot;href&amp;quot;) %&amp;gt;% # extract urls
    str_replace(pattern = &amp;quot;//&amp;quot;, replacement = &amp;quot;https://&amp;quot;) # create valid urls
  cat(length(urls),&amp;quot;content links scraped from tag archives.\n&amp;quot;)
  return(urls)
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Get the content for a given link and organize it into tabular format.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A conditional statement is included to identify webpages with no text content. All pages have a boilerplate paragraph, so pages with a &lt;code&gt;text&lt;/code&gt; vector of length greater than one will be content pages.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_content &amp;lt;- function(url) {
  # Function: Scrape the title, author, date, and text from a provided
  # content link. Return as a tibble/data.frame
  cat(&amp;quot;Scraping:&amp;quot;, url, &amp;quot;\n&amp;quot;)
  html &amp;lt;- read_html(url) # load html from selected url
  
  # Title
  title &amp;lt;- 
    html %&amp;gt;% # pass html
    html_node(&amp;quot;h1.articulo-titulo&amp;quot;) %&amp;gt;% # isolate title
    html_text(trim = TRUE) # extract title and trim whitespace
  
  # Author
  author &amp;lt;- 
    html %&amp;gt;% # pass html
    html_node(&amp;quot;span.autor-nombre&amp;quot;) %&amp;gt;% # isolate author
    html_text(trim = TRUE) # extract author and trim whitespace
  
  # Date
  date &amp;lt;- 
    html %&amp;gt;% # pass html
    html_nodes(&amp;quot;div.articulo-datos time&amp;quot;) %&amp;gt;% # isolate date
    html_attr(&amp;quot;datetime&amp;quot;) # extract date
  
  # Text
  text &amp;lt;- 
    html %&amp;gt;% # pass html
    html_nodes(&amp;quot;div.articulo-cuerpo p&amp;quot;) %&amp;gt;% # isolate text by paragraph
    html_text(trim = TRUE) # extract paragraphs and trim whitespace
  
  # Check to see if the article is text based
  # - only one paragraph suggests a non-text article (cartoon/ video/ album)
  if (length(text) &amp;gt; 1) { 
    # Create tibble/data.frame
    return(tibble(url, title, author, date, text, paragraph = (1:length(text))))
  } else {
    message(&amp;quot;Non-text based article. Link skipped.&amp;quot;)
    return(NULL)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;em&gt;Write the tabular data to disk.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’ve added code we’ve used in the previous data acquisition methods in this post to create a target directory before writing the file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_content &amp;lt;- function(content, target_file) {
  # Function: Write the tibble content to disk. Create the directory if
  # it does not already exist.
  target_dir &amp;lt;- dirname(target_file) # identify target file directory structure
  dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create directory
  write_csv(content, target_file) # write csv file to target location
  cat(&amp;quot;Content written to disk!\n&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These function each perform a task in our workflow and can be joined together to do our web scrape. To make this workflow maximally efficient I’ve wrapped them, and a conditional statement to avoid re-downloading a resource, in a function named &lt;code&gt;download_elpais_tag()&lt;/code&gt;. I’ve also added the &lt;code&gt;map()&lt;/code&gt; function to our workflow at a couple key points. &lt;code&gt;map()&lt;/code&gt; takes an object an iterates over each element in that object. Since the &lt;code&gt;get_content_links()&lt;/code&gt; and the &lt;code&gt;get_content()&lt;/code&gt; functions work on an object with a single element, we need the functions to be iteratively applied to objects with multiple elements. After &lt;code&gt;map()&lt;/code&gt; does its work applying the function to the elements of the object the results need to be joined. For the results from &lt;code&gt;map(get_content_links)&lt;/code&gt; will be a vector, so &lt;code&gt;combine()&lt;/code&gt; is the appropriate function. For &lt;code&gt;map(get_content)&lt;/code&gt; a tibble data frame will be returned so we use &lt;code&gt;bind_rows()&lt;/code&gt; to join the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;download_elpais_tag &amp;lt;- function(tag_name, sample_size, target_file, force = FALSE) {
  # Function: Download articles from elpais.com based on tag name. Select
  # number of archive pages to consult, then scrape and write the content 
  # to disk. If the target file exists, do not download again.
  if(!file.exists(target_file) | force == TRUE) {
    cat(&amp;quot;Downloading data.\n&amp;quot;)
    get_archive_pages(tag_name, sample_size) %&amp;gt;% # select tag archive pages
      map(get_content_links) %&amp;gt;% # get content links from pages sampled
      combine() %&amp;gt;% # combine the results as a single vector
      map(get_content) %&amp;gt;% # get the content for each content link
      bind_rows() %&amp;gt;% # bind the results as a single tibble
      write_content(target_file) # write content to disk
  } else {
    cat(&amp;quot;Data already downloaded!\n&amp;quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding these functions, including the &lt;code&gt;download_elpais_tag()&lt;/code&gt; function to the &lt;code&gt;functions/acquire_functions.R&lt;/code&gt; script in our project management template and then sourcing this script from the &lt;code&gt;acquire_data.R&lt;/code&gt; script in the &lt;code&gt;code/&lt;/code&gt; directory will allow us to use the function like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scrape archives of the Spanish news site elpais.com by tag
# To search for valid tags: https://elpais.com/tag/listado/
download_elpais_tag(tag_name = &amp;quot;politica&amp;quot;, 
                    target_file = &amp;quot;data/original/elpais/political_articles.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;90554 pages available for the politica tag.
1 pages selected.
22 content links scraped from tag archives.
Scraping: https://elpais.com/deportes/2017/10/20/actualidad/1508510590_014924.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508506425_813840.html 
Scraping: https://elpais.com/internacional/2017/10/20/actualidad/1508503663_430515.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508507460_569874.html 
Scraping: https://elpais.com/cultura/2017/10/20/actualidad/1508488913_681643.html 
Scraping: https://elpais.com/internacional/2017/10/20/actualidad/1508506096_337991.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508503572_812343.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508488656_838766.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508489106_542799.html 
Scraping: https://elpais.com/ccaa/2017/10/19/valencia/1508445805_457854.html 
Scraping: https://elpais.com/elpais/2017/10/20/album/1508487891_134872.html 
Non-text based article. Link skipped.
Scraping: https://elpais.com/ccaa/2017/10/20/catalunya/1508492661_274873.html 
Scraping: https://elpais.com/elpais/2017/10/19/ciencia/1508412461_971020.html 
Scraping: https://elpais.com/ccaa/2017/10/20/andalucia/1508499080_565687.html 
Scraping: https://elpais.com/ccaa/2017/10/20/catalunya/1508495565_034721.html 
Scraping: https://elpais.com/cultura/2017/10/19/actualidad/1508403967_099974.html 
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508496322_284364.html 
Scraping: https://elpais.com/economia/2017/10/19/actualidad/1508431364_731058.html 
Scraping: https://elpais.com/elpais/2017/10/20/album/1508491490_512616.html 
Non-text based article. Link skipped.
Scraping: https://politica.elpais.com/politica/2017/10/20/actualidad/1508481079_647952.html 
Scraping: https://elpais.com/ccaa/2017/10/20/valencia/1508493387_961965.html 
Scraping: https://elpais.com/economia/2017/10/20/actualidad/1508492104_302263.html 
Content written to disk!&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I applied the function to the tag &lt;code&gt;gastronomia&lt;/code&gt; (gastronomy) in the same fashion. The results are stored in the &lt;code&gt;data/original/&lt;/code&gt; directory. Our complete data structure for this post looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── elpais
    │   ├── gastronomy_articles.csv
    │   └── political_articles.csv
    ├── gutenberg
    │   ├── works_pq.csv
    │   └── works_pr.csv
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

8 directories, 10 files&lt;/code&gt;&lt;/pre&gt;
&lt;!-- then web scrape the State of the Union Addresses acquiring both raw text and meta-data.

Consider how to store the data: `.csv` or `.xml`?

--&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-text-from-other-formats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting text from other formats&lt;/h2&gt;
&lt;!-- TODO: include discussion on how to download Word and PDF files from the web. --&gt;
&lt;p&gt;As a final note it is worth pointing out that machine-readable data for analysis is often trapped in other formats such as Word or PDF files. R provides packages for working with these formats and can extract the text programmatically. See &lt;a href=&#34;https://github.com/ropensci/antiword#readme&#34;&gt;antiword&lt;/a&gt; for Word files and &lt;a href=&#34;https://ropensci.org/blog/2016/03/01/pdftools-and-jeroen&#34;&gt;pdftools&lt;/a&gt; for PDF files. In the case that a PDF is an image that needs OCR (Optical Character Recognition), you can experiment with the &lt;a href=&#34;https://ropensci.org/blog/blog/2016/11/16/tesseract&#34;&gt;tessseract&lt;/a&gt; package. It is important to be aware, however, that recovering plain text from these formats can often result in conversion artifacts; especially using OCR. Not to worry, we can still work with the data it just might mean more pre-processing before we get to doing our analysis.&lt;/p&gt;
&lt;!-- show how to extract text from Word documents `antiword`, from PDF files `pdftools`, and OCR `tesseract`. --&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we covered scraping language data from the web. The &lt;code&gt;rvest&lt;/code&gt; package provides a host of functions for downloading and parsing HTML. We first looked at a toy example to get a basic understanding of how HTML works and then moved to applying this knowledge to a practical example. To maintain a reproducible workflow, the code developed in this example was grouped into task-oriented functions which were in turn joined and wrapped into a function that provided convenient access to our workflow and avoided unnecessary downloads (in the case the data already exists on disk).&lt;/p&gt;
&lt;p&gt;Here we have built on previously introduced R coding concepts and demonstrated various others. Web scraping often requires more knowledge of and familiarity with R as well as other web technologies. Rest assured, however, practice will increase confidence in your abilities. I encourage you to practice on your own with other websites. You will encounter problems. Consult the R documentation in RStudio or online and lean on the R community on the web at sites such as &lt;a href=&#34;https://stackoverflow.com&#34;&gt;StackOverflow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At this point you have both a bird’s eye view of the data available on the web and strategies on how to access a great majority of it. It is now time to turn to the next step in our data analysis project: data curation. In the next posts I will cover how to wrangle your raw data into a tidy dataset. This will include working with and incorporating meta-data as well as augmenting a dataset with linguistic annotations.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-rvest&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2016. &lt;em&gt;Rvest: Easily Harvest (Scrape) Web Pages&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=rvest&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=rvest&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;———. 2017. &lt;em&gt;Tidyverse: Easily Install and Load the ’Tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;tibble&lt;/code&gt; objects are &lt;code&gt;data.frame&lt;/code&gt; objects with some added extra bells and whistles that we won’t get into here.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (2/3): package interfaces</title>
      <link>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/23/acquiring-data-for-language-research-package-interfaces/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;!-- TODO:
--&gt;
&lt;div id=&#34;package-interfaces&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Package interfaces&lt;/h2&gt;
&lt;p&gt;A convenient alternative method for acquiring data in R is through package interfaces to web services. These interfaces are built using R code to make connections with resources on the web through &lt;strong&gt;Automatic Programming Interfaces&lt;/strong&gt; (APIs). Websites such as Project Gutenberg, Twitter, Facebook, and many others provide APIs to allow access to their data under certain conditions, some more limiting for data collection than others. Programmers (like you!) in the R community take up the task of wrapping calls to an API with R code to make accessing that data from R possible. For example, &lt;a href=&#34;https://CRAN.R-project.org/package=gutenbergr&#34;&gt;gutenbergr&lt;/a&gt; provides access to Project Gutenberg, &lt;a href=&#34;https://CRAN.R-project.org/package=rtweet&#34;&gt;rtweet&lt;/a&gt; to Twitter, and &lt;a href=&#34;https://CRAN.R-project.org/package=Rfacebook&#34;&gt;Rfacebook&lt;/a&gt; to Facebook.&lt;/p&gt;
&lt;p&gt;Using R package interfaces, however, often requires some more knowledge about R objects and functions. Let’s take a look at how to access data from Project Gutenberg through the &lt;code&gt;gutenbergr&lt;/code&gt; package. Along the way we will touch upon various functions and concepts that are key to working with the R data types vectors and data frames including filtering and writing tabular data to disk in plain-text format.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The following code is available on GitHub &lt;code&gt;recipes-acquiring_data&lt;/code&gt; and is built on the &lt;code&gt;recipes-project_template&lt;/code&gt; I have discussed in detail &lt;a href=&#34;https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/&#34;&gt;here&lt;/a&gt; and made accessible &lt;a href=&#34;https://github.com/francojc/recipes-project_template.git&#34;&gt;here&lt;/a&gt;. I encourage you to follow along by downloading the &lt;code&gt;recipes-project_template&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; from the Terminal or create a new RStudio R Project and select the “Version Control” option.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;To get started let’s install and load the package. The most simple method for downloading an R package in RStudio is to select the ‘Packages’ tab in the Files pane and click the ‘Install’ icon. To ensure that our code is reproducible, however, it is better to approach the installation of packages programmatically. If the package is not part of the R base library, we will not assume that the user will have the package on their system. The code to install and load the &lt;code&gt;gutenbergr&lt;/code&gt; package is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;gutenbergr&amp;quot;) # install `gutenbergr` package
library(gutenbergr) # load the `gutenbergr` package&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach works just fine, but luck has it that there is an R package for installing and loading packages! The &lt;a href=&#34;https://CRAN.R-project.org/package=pacman&#34;&gt;pacman&lt;/a&gt; package includes a set of functions for managing packages. A very useful one is &lt;code&gt;p_load()&lt;/code&gt; which will look for a package on a system, load it if it is found, and install and then load it if it is not found. This helps potentially avoid using unnecessary bandwidth to install packages that may already exist on a user’s system. But, to use &lt;code&gt;pacman&lt;/code&gt; we need to include the code to install and load it with the functions &lt;code&gt;install.packages()&lt;/code&gt; and &lt;code&gt;library()&lt;/code&gt;. I’ve included some code that will mimic the behavior of &lt;code&gt;p_load()&lt;/code&gt; for installing &lt;code&gt;pacman&lt;/code&gt; itself, but as you can see it is not elegant, luckily it’s only used once as we add it to the SETUP section of our master file, &lt;code&gt;_pipeline.R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load `pacman`. If not installed, install then load.
if (!require(&amp;quot;pacman&amp;quot;, character.only = TRUE)) {
  install.packages(&amp;quot;pacman&amp;quot;)
  library(&amp;quot;pacman&amp;quot;, character.only = TRUE)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have &lt;code&gt;pacman&lt;/code&gt; installed and loaded into our R session, let’s use the &lt;code&gt;p_load()&lt;/code&gt; function to make sure to install/ load the two packages we will need for the upcoming tasks. If you are following along with the &lt;code&gt;recipes-project_template&lt;/code&gt;, add this code within the SETUP section of the &lt;code&gt;acquire_data.R&lt;/code&gt; file.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Script-specific options or packages
pacman::p_load(tidyverse, gutenbergr)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Note that the arguments &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;gutenbergr&lt;/code&gt; are comma-separated but not quoted.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Project Gutenberg provides access to thousands of texts in the public domain. The &lt;code&gt;gutenbergr&lt;/code&gt; package contains a set of tables, or &lt;strong&gt;data frames&lt;/strong&gt; in R speak, that index the meta-data for these texts broken down by text (&lt;code&gt;gutenberg_metadata&lt;/code&gt;), author (&lt;code&gt;gutenberg_authors&lt;/code&gt;), and subject (&lt;code&gt;gutenberg_subjects&lt;/code&gt;). I’ll use the &lt;code&gt;glimpse()&lt;/code&gt; function loaded in the &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34;&gt;tidyverse&lt;/a&gt; package&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; to summarize the structure of these data frames.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_metadata) # summarize text meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 51,997
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ...
## $ title               &amp;lt;chr&amp;gt; NA, &amp;quot;The Declaration of Independence of th...
## $ author              &amp;lt;chr&amp;gt; NA, &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;United States&amp;quot;, ...
## $ gutenberg_author_id &amp;lt;int&amp;gt; NA, 1638, 1, 1666, 3, 1, 4, NA, 3, 3, NA, ...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; NA, &amp;quot;United States Law/American Revolution...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_authors) # summarize authors meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 16,236
## Variables: 7
## $ gutenberg_author_id &amp;lt;int&amp;gt; 1, 3, 4, 5, 7, 8, 9, 10, 12, 14, 16, 17, 1...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;United States&amp;quot;, &amp;quot;Lincoln, Abraham&amp;quot;, &amp;quot;Henr...
## $ alias               &amp;lt;chr&amp;gt; NA, NA, NA, NA, &amp;quot;Dodgson, Charles Lutwidge...
## $ birthdate           &amp;lt;int&amp;gt; NA, 1809, 1736, NA, 1832, NA, 1819, 1860, ...
## $ deathdate           &amp;lt;int&amp;gt; NA, 1865, 1799, NA, 1898, NA, 1891, 1937, ...
## $ wikipedia           &amp;lt;chr&amp;gt; NA, &amp;quot;http://en.wikipedia.org/wiki/Abraham_...
## $ aliases             &amp;lt;chr&amp;gt; NA, &amp;quot;United States President (1861-1865)/L...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(gutenberg_subjects) # summarize subjects meta-data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 140,173
## Variables: 3
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5...
## $ subject_type &amp;lt;chr&amp;gt; &amp;quot;lcc&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcsh&amp;quot;, &amp;quot;lcs...
## $ subject      &amp;lt;chr&amp;gt; &amp;quot;E201&amp;quot;, &amp;quot;United States. Declaration of Independen...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The &lt;code&gt;gutenberg_metadata&lt;/code&gt;, &lt;code&gt;gutenberg_authors&lt;/code&gt;, and &lt;code&gt;gutenberg_subjects&lt;/code&gt; are periodically updated. To check to see when each data frame was last updated run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;attr(gutenberg_metadata, &amp;quot;date_updated&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;2016-05-05&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;To download the text itself we use the &lt;code&gt;gutenberg_download()&lt;/code&gt; function which takes one required argument, &lt;code&gt;gutenberg_id&lt;/code&gt;. The &lt;code&gt;gutenberg_download()&lt;/code&gt; function is what is known as ‘vectorized’, that is, it can take a single value or multiple values for the argument &lt;code&gt;gutenberg_id&lt;/code&gt;. Vectorization refers to the process of applying a function to each of the elements stored in a &lt;strong&gt;vector&lt;/strong&gt; –a primary object type in R. A vector is a grouping of values of one of various types including character (&lt;code&gt;chr&lt;/code&gt;), integer (&lt;code&gt;int&lt;/code&gt;), and logical (&lt;code&gt;lgl&lt;/code&gt;) and a data frame is a grouping of vectors. The &lt;code&gt;gutenberg_download()&lt;/code&gt; function takes an integer vector which can be manually added or selected from the &lt;code&gt;gutenberg_metadata&lt;/code&gt; or &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frames using the &lt;code&gt;$&lt;/code&gt; operator (e.g. &lt;code&gt;gutenberg_metadata$gutenberg_id&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Let’s first add them manually here as a toy example by generating a vector of integers from 1 to 5 assigned to the variable name &lt;code&gt;ids&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids &amp;lt;- 1:5 # integer vector of values 1 to 5
ids&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To download the works from Project Gutenberg corresponding to the &lt;code&gt;gutenberg_id&lt;/code&gt;s 1 to 5, we pass the &lt;code&gt;ids&lt;/code&gt; object to the &lt;code&gt;gutenberg_download()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;works_sample &amp;lt;- gutenberg_download(gutenberg_id = ids) # download works with `gutenberg_id` 1-5
glimpse(works_sample) # summarize `works` dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,939
## Variables: 2
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;December, 1971  [Etext #1]&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;The Projec...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two attributes are returned: &lt;code&gt;gutenberg_id&lt;/code&gt; and &lt;code&gt;text&lt;/code&gt;. The &lt;code&gt;text&lt;/code&gt; column contains values for each line of text (delimited by a carriage return) for each of the 5 works we downloaded. There are many more attributes available from the Project Gutenberg API that can be accessed by passing a character vector of the attribute names to the argument &lt;code&gt;meta_fields&lt;/code&gt;. The column names of the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame contains the available attributes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(gutenberg_metadata) # print the column names of the `gutenberg_metadata` data frame&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;gutenberg_id&amp;quot;        &amp;quot;title&amp;quot;               &amp;quot;author&amp;quot;             
## [4] &amp;quot;gutenberg_author_id&amp;quot; &amp;quot;language&amp;quot;            &amp;quot;gutenberg_bookshelf&amp;quot;
## [7] &amp;quot;rights&amp;quot;              &amp;quot;has_text&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s augment our previous download with the title and author of each of the works. To create a character vector we use the &lt;code&gt;c()&lt;/code&gt; function, then, quote and delimit the individual elements of the vector with a comma.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# download works with `gutenberg_id` 1-5 including `title` and `author` as attributes
works_sample &amp;lt;- gutenberg_download(gutenberg_id = ids, 
                            meta_fields = c(&amp;quot;title&amp;quot;,
                                            &amp;quot;author&amp;quot;))
glimpse(works_sample) # summarize dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 2,939
## Variables: 4
## $ gutenberg_id &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;December, 1971  [Etext #1]&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;The Projec...
## $ title        &amp;lt;chr&amp;gt; &amp;quot;The Declaration of Independence of the United St...
## $ author       &amp;lt;chr&amp;gt; &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;Jefferson, Thomas&amp;quot;, &amp;quot;Jeffer...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, in a more practical scenario we would like to select the values of &lt;code&gt;gutenberg_id&lt;/code&gt; by some principled query such as works from a specific author, language, or subject. To do this we first query either the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame or the &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frame. Let’s say we want to download a random sample of 10 works from English Literature (Library of Congress Classification, “PR”). Using the &lt;code&gt;filter()&lt;/code&gt; function (part of the &lt;code&gt;tidyverse&lt;/code&gt; package set) we first extract all the Gutenberg ids from &lt;code&gt;gutenberg_subjects&lt;/code&gt; where &lt;code&gt;subject_type == &amp;quot;lcc&amp;quot;&lt;/code&gt; and &lt;code&gt;subject == &amp;quot;PR&amp;quot;&lt;/code&gt; assigning the result to &lt;code&gt;ids&lt;/code&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids &amp;lt;- 
  filter(gutenberg_subjects, subject_type == &amp;quot;lcc&amp;quot;, subject == &amp;quot;PR&amp;quot;)
glimpse(ids)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 7,100
## Variables: 3
## $ gutenberg_id &amp;lt;int&amp;gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43, 46, 5...
## $ subject_type &amp;lt;chr&amp;gt; &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, &amp;quot;lcc&amp;quot;, ...
## $ subject      &amp;lt;chr&amp;gt; &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;PR&amp;quot;, &amp;quot;...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;The operators &lt;code&gt;=&lt;/code&gt; and &lt;code&gt;==&lt;/code&gt; are not equivalents. &lt;code&gt;==&lt;/code&gt; is used for logical evaluation and &lt;code&gt;=&lt;/code&gt; is an alternate notation for variable assignment (&lt;code&gt;&amp;lt;-&lt;/code&gt;).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;gutenberg_subjects&lt;/code&gt; data frame does not contain information as to whether a &lt;code&gt;gutenberg_id&lt;/code&gt; is associated with a plain-text version. To limit our query to only those English Literature works with text, we filter the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame by the ids we have selected in &lt;code&gt;ids&lt;/code&gt; and the attribute &lt;code&gt;has_text&lt;/code&gt; in the &lt;code&gt;gutenberg_metadata&lt;/code&gt; data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ids_has_text &amp;lt;- 
  filter(gutenberg_metadata, gutenberg_id %in% ids$gutenberg_id, has_text == TRUE)
glimpse(ids_has_text)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 6,724
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 11, 12, 13, 16, 20, 26, 27, 35, 36, 42, 43...
## $ title               &amp;lt;chr&amp;gt; &amp;quot;Alice&amp;#39;s Adventures in Wonderland&amp;quot;, &amp;quot;Throu...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;Carroll, Lewis&amp;quot;, &amp;quot;Carroll, Lewis&amp;quot;, &amp;quot;Carro...
## $ gutenberg_author_id &amp;lt;int&amp;gt; 7, 7, 7, 10, 17, 17, 23, 30, 30, 35, 35, 3...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; &amp;quot;Children&amp;#39;s Literature&amp;quot;, &amp;quot;Children&amp;#39;s Liter...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;A couple R programming notes on the code phrase &lt;code&gt;gutenberg_id %in% ids$gutenberg_id&lt;/code&gt;. First, the &lt;code&gt;$&lt;/code&gt; symbol in &lt;code&gt;ids$gutenberg_id&lt;/code&gt; is the programmatic way to target a particular column in an R data frame. In this example we select the &lt;code&gt;ids&lt;/code&gt; data frame and the column &lt;code&gt;gutenberg_id&lt;/code&gt;, which is a integer vector. The &lt;code&gt;gutenberg_id&lt;/code&gt; variable that precedes the &lt;code&gt;%in%&lt;/code&gt; operator does not need an explicit reference to a data frame because the primary argument of the &lt;code&gt;filter()&lt;/code&gt; function is this data frame (&lt;code&gt;gutenberg_metadata&lt;/code&gt;). Second, the &lt;code&gt;%in%&lt;/code&gt; operator logically evaluates whether the vector elements in &lt;code&gt;gutenberg_metadata$gutenberg_ids&lt;/code&gt; are also found in the vector &lt;code&gt;ids$gutenberg_id&lt;/code&gt; returning &lt;code&gt;TRUE&lt;/code&gt; and &lt;code&gt;FALSE&lt;/code&gt; accordingly. This effectively filters those ids which are not in both vectors.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;As we can see the number of works with text is fewer than the number of works listed, 7100 versus 6724. Now we can safely do our random selection of 10 works, with the function &lt;code&gt;sample_n()&lt;/code&gt; and be confident that the ids we select will contain text when we take the next step by downloading the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123) # make the sampling reproducible
ids_sample &amp;lt;- sample_n(ids_has_text, 10) # sample 10 works
glimpse(ids_sample) # summarize the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 10
## Variables: 8
## $ gutenberg_id        &amp;lt;int&amp;gt; 7688, 33533, 12160, 37761, 40406, 1050, 18...
## $ title               &amp;lt;chr&amp;gt; &amp;quot;Lucretia — Volume 04&amp;quot;, &amp;quot;The Convict&amp;#39;s Far...
## $ author              &amp;lt;chr&amp;gt; &amp;quot;Lytton, Edward Bulwer Lytton, Baron&amp;quot;, &amp;quot;Pa...
## $ gutenberg_author_id &amp;lt;int&amp;gt; 761, 35765, 1865, 1256, 25821, 467, 1062, ...
## $ language            &amp;lt;chr&amp;gt; &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, &amp;quot;en&amp;quot;, ...
## $ gutenberg_bookshelf &amp;lt;chr&amp;gt; NA, NA, NA, NA, NA, &amp;quot;One Act Plays&amp;quot;, NA, N...
## $ rights              &amp;lt;chr&amp;gt; &amp;quot;Public domain in the USA.&amp;quot;, &amp;quot;Public domai...
## $ has_text            &amp;lt;lgl&amp;gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As before, we can now pass our ids (&lt;code&gt;ids_sample$gutenberg_id&lt;/code&gt;) as the argument of &lt;code&gt;gutenberg_download()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;works_pr &amp;lt;- gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, meta_fields = c(&amp;quot;author&amp;quot;, &amp;quot;title&amp;quot;))
glimpse(works_pr) # summarize the dataset&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 79,200
## Variables: 4
## $ gutenberg_id &amp;lt;int&amp;gt; 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1050, 1...
## $ text         &amp;lt;chr&amp;gt; &amp;quot;THE DARK LADY OF THE SONNETS&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;By Bernard S...
## $ author       &amp;lt;chr&amp;gt; &amp;quot;Shaw, Bernard&amp;quot;, &amp;quot;Shaw, Bernard&amp;quot;, &amp;quot;Shaw, Bernard&amp;quot;...
## $ title        &amp;lt;chr&amp;gt; &amp;quot;The Dark Lady of the Sonnets&amp;quot;, &amp;quot;The Dark Lady of...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have data and could move on to processing this data in preparation for analysis. However, we are aiming for a reproducible workflow and this code does not conform to our principle of modularity: each subsequent step in our analysis will depend on running this code first. Furthermore, running this code as it is creates issues with bandwidth, as in our previous examples from direct downloads. To address modularity we will write the data to disk in &lt;strong&gt;plain-text format&lt;/strong&gt;. In this way each subsequent step in our analysis can access the data locally. To address bandwidth concerns, we will devise a method for checking to see if the data is already downloaded and skip the download, if possible, to avoid accessing the Project Gutenberg server unnecessarily.&lt;/p&gt;
&lt;p&gt;To write our data frame to disk we will export it into a standard plain-text format for two-dimensional data: a CSV file (comma-separated value). The CSV structure for this data will look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## gutenberg_id,text,author,title
## 1050,THE DARK LADY OF THE SONNETS,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,By Bernard Shaw,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets
## 1050,,&amp;quot;Shaw, Bernard&amp;quot;,The Dark Lady of the Sonnets&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first line contains the names of the columns and subsequent lines the observations. Data points that contain commas themselves (e.g. “Shaw, Bernard”) are quoted to avoid misinterpreting these commas a deliminators in our data. To write this data to disk we will use the &lt;code&gt;write_csv()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv(works_pr, path = &amp;quot;data/original/gutenberg_works_pr.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To avoid downloading data that already resides on disk, let’s implement a similar strategy to the one used in the previous post for &lt;a href=&#34;https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/&#34;&gt;direct downloads&lt;/a&gt;. I’ve incorporated the code for sampling and downloading data for a particular subject from Project Gutenberg with a control statement to check if the data file already exists into a function I named &lt;code&gt;get_gutenberg_subject()&lt;/code&gt;. Take a look at this function below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_gutenberg_subject &amp;lt;- function(subject, target_file, sample_size = 10) {
  # Function: to download texts from Project Gutenberg with 
  # a specific LCC subject and write the data to disk.
  
  # Check to see if the data already exists
  if(!file.exists(target_file)) { # if data does not exist, download and write
    target_dir &amp;lt;- dirname(x) # generate target directory for the .csv file
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&amp;quot;Downloading data... \n&amp;quot;) # print status message
    # Select all records with a particular LCC subject
    ids &amp;lt;- 
      filter(gutenberg_subjects, 
             subject_type == &amp;quot;lcc&amp;quot;, subject == subject) # select subject
    # Select only those records with plain text available
    set.seed(123) # make the sampling reproducible
    ids_sample &amp;lt;- 
      filter(gutenberg_metadata, 
             gutenberg_id %in% ids$gutenberg_id, # select ids in both data frames 
             has_text == TRUE) %&amp;gt;% # select those ids that have text
      sample_n(sample_size) # sample N works (default N = 10)
    # Download sample with associated `author` and `title` metadata
    works_sample &amp;lt;- 
      gutenberg_download(gutenberg_id = ids_sample$gutenberg_id, 
                         meta_fields = c(&amp;quot;author&amp;quot;, &amp;quot;title&amp;quot;))
    # Write the dataset to disk in .csv format
    write_csv(works_sample, path = target_file)
    cat(&amp;quot;Data downloaded! \n&amp;quot;) # print status message
  } else { # if data exists, don&amp;#39;t download it again
    cat(&amp;quot;Data already exists \n&amp;quot;) # print status message
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Adding this function to our function script &lt;code&gt;functions/acquire_functions.R&lt;/code&gt;, we can now use this function in our &lt;code&gt;code/acquire_data.R&lt;/code&gt; script to download multiple subjects and store them in on disk in their own file.&lt;/p&gt;
&lt;p&gt;Let’s download American Literature now (LCC code “PQ”).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download Project Gutenberg text for subject &amp;#39;PQ&amp;#39; (American Literature)
# and then write this dataset to disk in .csv format
get_gutenberg_subject(subject = &amp;quot;PQ&amp;quot;, target_file = &amp;quot;data/original/gutenberg/works_pq.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Applying this function to both the English and American Literature datasets, our data directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── gutenberg
    │   ├── works_pq.csv
    │   └── works_pr.csv
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

7 directories, 8 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And as before in the previous post, it is a good idea to log the results of our work.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log the directory structure of the Project Gutenberg data
system(command = &amp;quot;tree data/original/gutenberg &amp;gt;&amp;gt; log/data_original_gutenberg.log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;!-- then work with a more complex package interface. Introduce the `fulltext` package (or [crminer](https://github.com/ropensci/crminer) . Show how to search a specific publication, download the full text (in XML) format. Then extract the `doi`, `title`, and `abstract`, convert it to a data.frame and store it as a `.csv` file. 

TODO: - work with `fulltext` and `crminer` exploration/ package tutorials
      - determine the R skills needed to complete this activity

Search for Plos ONE publications in Linguistics?
--&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post I provided an overview to acquiring data from web service APIs through R packages. We took at closer look at the &lt;code&gt;gutenbergr&lt;/code&gt; package which provides programmatic access to works available on Project Gutenberg. Working with package interfaces requires more knowledge of R including loading/ installing packages, working with vectors and data frames, and exporting data from an R session. We touched on these programming concepts and also outlined a method to create a reproducible workflow.&lt;/p&gt;
&lt;p&gt;Our last step in this mini series on acquiring data for language research with R, we will explore methods for acquire language data from the browsable web. I will discuss using the &lt;code&gt;rvest&lt;/code&gt; package for downloading and isolating text elements from HTML pages and show how to organize and write the data to disk.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-R-gutenbergr&#34;&gt;
&lt;p&gt;Robinson, David. 2018. &lt;em&gt;Gutenbergr: Download and Process Public Domain Works from Project Gutenberg&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=gutenbergr&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=gutenbergr&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-R-tidyverse&#34;&gt;
&lt;p&gt;Wickham, Hadley. 2017. &lt;em&gt;Tidyverse: Easily Install and Load the ’Tidyverse’&lt;/em&gt;. &lt;a href=&#34;https://CRAN.R-project.org/package=tidyverse&#34; class=&#34;uri&#34;&gt;https://CRAN.R-project.org/package=tidyverse&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;code&gt;tidyverse&lt;/code&gt; is not a typical package. It is a set of packages: &lt;code&gt;ggplot2&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;tidyr&lt;/code&gt;, &lt;code&gt;readr&lt;/code&gt;, &lt;code&gt;purrr&lt;/code&gt;, and &lt;code&gt;tibble&lt;/code&gt;. These packages are all installed/ loaded with &lt;code&gt;tidyverse&lt;/code&gt; and form the backbone for the type of work you will typically do in most analyses.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;See &lt;a href=&#34;https://www.loc.gov/catdir/cpso/lcco/&#34;&gt;Library of Congress Classification&lt;/a&gt; documentation for a complete list of subject codes.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Acquiring data for language research (1/3): direct downloads</title>
      <link>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</link>
      <pubDate>Fri, 20 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/20/acquiring-data-for-language-research-direct-downloads/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;There are three main ways to acquire corpus data using R that I will introduce you to: &lt;strong&gt;direct download&lt;/strong&gt;, &lt;strong&gt;package interfaces&lt;/strong&gt;, and &lt;strong&gt;web scraping&lt;/strong&gt;. In this post we will start by directly downloading a corpus as it is the most straightforward process for the novice R programmer and incurs the least number of steps. Along the way I will introduce some key R coding concepts including control statements and custom functions.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The following code is available on GitHub &lt;code&gt;recipes-acquiring_data&lt;/code&gt; and is built on the &lt;code&gt;recipes-project_template&lt;/code&gt; I have discussed in detail &lt;a href=&#34;https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/&#34;&gt;here&lt;/a&gt; and made accessible &lt;a href=&#34;https://github.com/francojc/recipes-project_template.git&#34;&gt;here&lt;/a&gt;. I encourage you to follow along by downloading the &lt;code&gt;recipes-project_template&lt;/code&gt; with &lt;code&gt;git&lt;/code&gt; from the Terminal or create a new RStudio R Project and select the “Version Control” option.&lt;/p&gt;

&lt;/div&gt;

&lt;!-- TODO: add {#anchor} below --&gt;
&lt;div id=&#34;direct-downloads&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Direct downloads&lt;/h2&gt;
&lt;p&gt;Published corpus data found in repositories or individual sources are usually the easiest to start working with as it is generally a matter of identifying a resource to download and then downloading it with R. OK, there’s a little more involved, but that’s the basic idea.&lt;/p&gt;
&lt;p&gt;Let’s take a look at how this works starting with the a sample from the Switchboard Corpus, a corpus of 2,400 telephone conversations by 543 speakers. First we navigate to the site with a browser and download the file that we are looking for. In this case I found the Switchboard Corpus on the &lt;a href=&#34;http://www.nltk.org/nltk_data/&#34;&gt;NLTK data repository site&lt;/a&gt;. More often than not this file will be some type of compressed archive file with an extension such as &lt;code&gt;.zip&lt;/code&gt; or &lt;code&gt;.tz&lt;/code&gt;, which is the case here. Archive files make downloading multiple files easy by grouping files and directories into one file. In R we can used the &lt;code&gt;download.file()&lt;/code&gt; function from the base R library&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. There are a number of &lt;strong&gt;arguments&lt;/strong&gt; that a function may require or provide optionally. The &lt;code&gt;download.file()&lt;/code&gt; function minimally requires two: &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;destfile&lt;/code&gt;. That is the file to download and the location where it is to be saved to disk.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download .zip file and write to disk
download.file(url = &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;, destfile = &amp;quot;data/original/switchboard.zip&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once an archive file is downloaded, however, the file needs to be ‘decompressed’ to reveal the file structure. The file we downloaded is located on our disk at &lt;code&gt;data/original/switchboard.zip&lt;/code&gt;. To decompress this file we use the &lt;code&gt;unzip()&lt;/code&gt; function with the arguments &lt;code&gt;zipfile&lt;/code&gt; pointing to the &lt;code&gt;.zip&lt;/code&gt; file and &lt;code&gt;exdir&lt;/code&gt; specifying the directory where we want the files to be extracted to.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;I encourage you to use the &lt;code&gt;TAB&lt;/code&gt; key to expand the list of options of a function to avoid having to remember the arguments of a function and also to avoid typos. After typing the name of the function and opening &lt;code&gt;(&lt;/code&gt; hit &lt;code&gt;TAB&lt;/code&gt; to view and select the argument(s) you want. Furthermore, the &lt;code&gt;TAB&lt;/code&gt; key can also help you expand paths to files and directories. Note that the expansion will default to the current working directory.&lt;/p&gt;

&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Decompress .zip file and extract to our target directory
unzip(zipfile = &amp;quot;data/original/switchboard.zip&amp;quot;, exdir = &amp;quot;data/original/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The directory structure of &lt;code&gt;data/&lt;/code&gt; now should look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/
├── derived
└── original
    ├── switchboard
    │   ├── README
    │   ├── discourse
    │   ├── disfluency
    │   ├── tagged
    │   ├── timed-transcript
    │   └── transcript
    └── switchboard.zip

3 directories, 7 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have acquired the data programmatically and with this code as part of our workflow anyone could run this code and reproduce the same results. The code as it is, however, is not ideally efficient. Firstly the &lt;code&gt;switchboard.zip&lt;/code&gt; file is not strictly needed after we decompress it and it occupies disk space if we keep it. And second, each time we run this code the file will be downloaded from the remote serve leading to unnecessary data transfer and server traffic. Let’s tackle each of these issues in turn.&lt;/p&gt;
&lt;p&gt;To avoid writing the &lt;code&gt;switchboard.zip&lt;/code&gt; file to disk (long-term) we can use the &lt;code&gt;tempfile()&lt;/code&gt; function to open a temporary holding space for the file. This space can then be used to store the file, unzip it, and then the temporary file will be destroyed. We assign the temporary space to an R object we will name &lt;code&gt;temp&lt;/code&gt; with the &lt;code&gt;tempfile()&lt;/code&gt; function. This object can now be used as the value of the argument &lt;code&gt;destfile&lt;/code&gt; in the &lt;code&gt;download.file()&lt;/code&gt; function. Let’s also assign the web address to another object &lt;code&gt;url&lt;/code&gt; which we will use as the value of the &lt;code&gt;url&lt;/code&gt; argument.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a temporary file space for our .zip file
temp &amp;lt;- tempfile()
# Assign our web address to `url`
url &amp;lt;- &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;
# Download .zip file and write to disk
download.file(url, temp)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;In the previous code I’ve used the values stored in the objects &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;temp&lt;/code&gt; in the &lt;code&gt;download.file()&lt;/code&gt; function without specifying the argument names –only providing the names of the objects. R will assume that values of a function map to the ordering of the arguments. If your values do not map to ordering of the arguments you are required to specify the argument name and the value. To view the ordering of objects hit &lt;code&gt;TAB&lt;/code&gt; after entering the function name or consult the function documentation by prefixing the function name with &lt;code&gt;?&lt;/code&gt; and hitting &lt;code&gt;ENTER&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;At this point our downloaded file is stored temporarily on disk and can be accessed and decompressed to our target directory using &lt;code&gt;temp&lt;/code&gt; as the value for the argument &lt;code&gt;zipfile&lt;/code&gt; from the &lt;code&gt;unzip()&lt;/code&gt; function. I’ve assigned our target directory path to &lt;code&gt;target_dir&lt;/code&gt; and used it as the value for the argument &lt;code&gt;exdir&lt;/code&gt; to prepare us for the next tweak on our approach.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Assign our target directory to `target_dir`
target_dir &amp;lt;- &amp;quot;data/original/&amp;quot;
# Decompress .zip file and extract to our target directory
unzip(zipfile = temp, exdir = target_dir)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data/
├── derived
└── original
    └── switchboard
        ├── README
        ├── discourse
        ├── disfluency
        ├── tagged
        ├── timed-transcript
        └── transcript

3 directories, 6 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second issue I raised concerns the fact that running this code as part of our project will repeat the download each time. Since we would like to be good citizens and avoid unnecessary traffic on the web it would be nice if our code checked to see if we already have the data on disk and if it exists, then skip the download, if not then download it. To achieve this we need to introduce two new functions &lt;code&gt;if()&lt;/code&gt; and &lt;code&gt;dir.exists()&lt;/code&gt;. &lt;code&gt;dir.exists()&lt;/code&gt; takes a path to a directory as an argument and returns the logical value, &lt;code&gt;TRUE&lt;/code&gt;, if that directory exists, and &lt;code&gt;FALSE&lt;/code&gt; if it does not. &lt;code&gt;if()&lt;/code&gt; evaluates logical statements and processes subsequent code based on the logical value it is passed as an argument. Let’s look at a toy example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num &amp;lt;- 1
if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1 is 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I assigned &lt;code&gt;num&lt;/code&gt; to the value &lt;code&gt;1&lt;/code&gt; and created a logical evaluation &lt;code&gt;num ==&lt;/code&gt; whose result is passed as the argument to &lt;code&gt;if()&lt;/code&gt;. If the statement returns &lt;code&gt;TRUE&lt;/code&gt; then the code withing the first set of curly braces &lt;code&gt;{...}&lt;/code&gt; is run. If &lt;code&gt;num == 1&lt;/code&gt; is false, like in the code below, the code withing the braces following the &lt;code&gt;else&lt;/code&gt; will be run.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;num &amp;lt;- 2
if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function &lt;code&gt;if()&lt;/code&gt; is one of various functions that are called &lt;strong&gt;control statements&lt;/strong&gt;. Theses functions provide a lot of power to make dynamic choices as code is run.&lt;/p&gt;
&lt;p&gt;Before we get back to our key objective to avoid downloading resources that we already have on disk, let me introduce another strategy to making code more powerful and ultimately more efficient and as well as more legible –the &lt;strong&gt;custom function&lt;/strong&gt;. Custom functions are functions that the user writes to create a set of procedures that can be run in similar contexts. I’ve created a custom function named &lt;code&gt;eval_num()&lt;/code&gt; below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num &amp;lt;- function(num) {
  if(num == 1) { 
  cat(num, &amp;quot;is 1&amp;quot;) 
  } else {
  cat(num, &amp;quot;is not 1&amp;quot;)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s take a closer look at what’s going on here. The function &lt;code&gt;function()&lt;/code&gt; creates a function in which the user decides what arguments are necessary for the code to perform its task. In this case the only necessary argument is the object to store a numeric value to be evaluated. I’ve called it &lt;code&gt;num&lt;/code&gt; because it reflects the name of the object in our toy example, but there is nothing special about this name. It’s only important that the object names be consistently used. I’ve included our previous code (except for the hard-coded assignment of &lt;code&gt;num&lt;/code&gt;) inside the curly braces and assigned the entire code chunk to &lt;code&gt;eval_num&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now use the function &lt;code&gt;eval_num()&lt;/code&gt; to perform the task of evaluating whether a value of &lt;code&gt;num&lt;/code&gt; is or is not equal to &lt;code&gt;1&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 1 is 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 2 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;eval_num(num = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 3 is not 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve put these coding strategies together with our previous code in a function I named &lt;code&gt;get_zip_data()&lt;/code&gt;. There is a lot going on here. Take a look first and see if you can follow the logic involved given what you now know.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_zip_data &amp;lt;- function(url, target_dir) {
  # Function: to download and decompress a .zip file to a target directory
  
  # Check to see if the data already exists
  if(!dir.exists(target_dir)) { # if data does not exist, download/ decompress
    cat(&amp;quot;Creating target data directory \n&amp;quot;) # print status message
    dir.create(path = target_dir, recursive = TRUE, showWarnings = FALSE) # create target data directory
    cat(&amp;quot;Downloading data... \n&amp;quot;) # print status message
    temp &amp;lt;- tempfile() # create a temporary space for the file to be written to
    download.file(url = url, destfile = temp) # download the data to the temp file
    unzip(zipfile = temp, exdir = target_dir, junkpaths = TRUE) # decompress the temp file in the target directory
    cat(&amp;quot;Data downloaded! \n&amp;quot;) # print status message
  } else { # if data exists, don&amp;#39;t download it again
    cat(&amp;quot;Data already exists \n&amp;quot;) # print status message
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;OK. You should have recognized the general steps in this function: the argument &lt;code&gt;url&lt;/code&gt; and &lt;code&gt;target_dir&lt;/code&gt; specify where to get the data and where to write the decompressed files, the &lt;code&gt;if()&lt;/code&gt; statement evaluates whether the data already exists, if not (&lt;code&gt;!dir.exists(target_dir)&lt;/code&gt;) then the data is downloaded and decompressed, if it does exist (&lt;code&gt;else&lt;/code&gt;) then it is not downloaded.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The prefixed &lt;code&gt;!&lt;/code&gt; in the logical expression &lt;code&gt;dir.exists(target_dir)&lt;/code&gt; returns the opposite logical value. This is needed in this case so when the target directory exists, the expression will return &lt;code&gt;FALSE&lt;/code&gt;, not &lt;code&gt;TRUE&lt;/code&gt;, and therefore not proceed in downloading the resource.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;There are a couple key tweaks I’ve added that provide some additional functionality. For one I’ve included the function &lt;code&gt;dir.create()&lt;/code&gt; to create the target directory where the data will be written. I’ve also added an additional argument to the &lt;code&gt;unzip()&lt;/code&gt; function, &lt;code&gt;junkpaths = TRUE&lt;/code&gt;. Together these additions allow the user to create an arbitrary directory path where the files, and only the files, will be extracted to on our disk. This will discard the containing directory of the &lt;code&gt;.zip&lt;/code&gt; file which can be helpful when we want to add multiple &lt;code&gt;.zip&lt;/code&gt; files to the same target directory.&lt;/p&gt;
&lt;p&gt;A practical scenario where this applies is when we want to download data from a corpus that is contained in multiple &lt;code&gt;.zip&lt;/code&gt; files but still maintain these files in a single primary data directory. Take for example the &lt;a href=&#34;http://www.linguistics.ucsb.edu/research/santa-barbara-corpus&#34;&gt;Santa Barbara Corpus&lt;/a&gt;. This corpus resource includes a series of interviews in which there is one &lt;code&gt;.zip&lt;/code&gt; file, &lt;code&gt;SBCorpus.zip&lt;/code&gt; which contains the &lt;a href=&#34;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip&#34;&gt;transcribed interviews&lt;/a&gt; and another &lt;code&gt;.zip&lt;/code&gt; file, &lt;code&gt;metadata.zip&lt;/code&gt; which organizes the &lt;a href=&#34;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip&#34;&gt;meta-data&lt;/a&gt; associated with each speaker. Applying our initial strategy to download and decompress the data will lead to the following directory structure:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── SBCorpus
    │   ├── TRN
    │   └── __MACOSX
    │       └── TRN
    └── metadata
        └── __MACOSX

8 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By applying our new custom function &lt;code&gt;get_zip_data()&lt;/code&gt; to the transcriptions and then the meta-data we can better organize the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download corpus transcriptions
get_zip_data(url = &amp;quot;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/SBCorpus.zip&amp;quot;, target_dir = &amp;quot;data/original/sbc/transcriptions/&amp;quot;)

# Download corpus meta-data
get_zip_data(url = &amp;quot;http://www.linguistics.ucsb.edu/sites/secure.lsit.ucsb.edu.ling.d7/files/sitefiles/research/SBC/metadata.zip&amp;quot;, target_dir = &amp;quot;data/original/sbc/meta-data/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now our &lt;code&gt;data/&lt;/code&gt; directory is better organized; both the transcriptions and the meta-data are housed under &lt;code&gt;data/original/sbc/&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    └── sbc
        ├── meta-data
        └── transcriptions

5 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we add data from other sources we can keep them logical separate and allow our data collection to scale without creating unnecessary complexity. Let’s add the Switchboard Corpus sample using our &lt;code&gt;get_zip_data()&lt;/code&gt; function to see this in action.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download corpus
get_zip_data(url = &amp;quot;https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/switchboard.zip&amp;quot;, target_dir = &amp;quot;data/original/scs/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our corpora our housed in their own directories and the files are clearly associated.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;data
├── derived
└── original
    ├── sbc
    │   ├── meta-data
    │   └── transcriptions
    └── scs

6 directories&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;At this point we have what we need to continue to the next step in our data analysis project. But before we go, we should do some housekeeping to document and organize this process to make our work reproducible. We will take advantage of the &lt;code&gt;project-template&lt;/code&gt; directory structure, seen below.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── README.md
├── _pipeline.R
├── code
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data
│   ├── derived
│   └── original
├── figures
├── functions
├── log
├── recipes-acquire-data.Rproj
└── report
    ├── article.Rmd
    ├── bibliography.bib
    ├── slides.Rmd
    └── web.Rmd

8 directories, 13 files&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First it is good practice to separate custom functions from our processing scripts. We can create a file in our &lt;code&gt;functions/&lt;/code&gt; directory named &lt;code&gt;acquire_functions.R&lt;/code&gt; and add our custom function &lt;code&gt;get_zip_data()&lt;/code&gt; there. We then use the &lt;code&gt;source()&lt;/code&gt; function to read that function into our current script to make it available to use as needed. It is good practice to source your functions in the SETUP section of your script.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load custom functions for this project
source(file = &amp;quot;functions/acquire_functions.R&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second it is advisable to log the structure of the data in plain text files. You can create a directory tree (as those seen in this post) with the bash command &lt;code&gt;tree&lt;/code&gt; on the command line. R provides a function &lt;code&gt;system()&lt;/code&gt; which will interface the command line. Adding the following code to the LOG section of your &lt;code&gt;acquire_data.R&lt;/code&gt; R script will generate the directory structure for each of the corpora that we have downloaded in this post in the files &lt;code&gt;data_original_sbc.log&lt;/code&gt; and &lt;code&gt;data_original_scs.log&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Log the directory structure of the Santa Barbara Corpus
system(command = &amp;quot;tree data/original/sbc &amp;gt;&amp;gt; log/data_original_sbc.log&amp;quot;)
# Log the directory structure of the Switchboard Corpus sample
system(command = &amp;quot;tree data/original/scs &amp;gt;&amp;gt; log/data_original_scs.log&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our project directory structure now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.
├── README.md
├── _pipeline.R
├── code
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data
│   ├── derived
│   └── original
├── figures
├── functions
│   └── acquire_functions.R
├── log
│   ├── data_original_sbc.log
│   └── data_original_scs.log
├── recipes-acquire-data.Rproj
└── report
    ├── article.Rmd
    ├── bibliography.bib
    ├── slides.Rmd
    └── web.Rmd

8 directories, 15 files&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we’ve covered how to access, download, and organize data contained in .zip files; the most common format for language data found on repositories and individual sites. This included an introduction to a few key R programming concepts and strategies including using functions, writing custom functions, and controlling program flow with control statements. Our approach was to gather data while also keeping in mind the reproducibility of the code. To this end I introduced programming strategies for avoiding unnecessary web traffic (downloads), scalable directory creation, and data documentation.&lt;/p&gt;
&lt;p&gt;In the next post in this three part mini-series I will cover acquiring data from web services such as Project Gutenberg, Twitter, and Facebook through R packages. Using package interfaces will require additional knowledge of R objects. I will discuss vector types and data frames and show how to manipulate these objects in practical situations like filtering data and writing data to disk in plain-text files.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Remember base R packages are installed by default with R and are loaded and accessible by default in each R session.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data for language research -types and sources</title>
      <link>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/10/04/data-for-language-research-types-and-sources/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this Recipe you will learn about the types of data available for language research and where to find data. The goal, then, is to introduce you to the landscape of language data available and provide a general overview of the characteristics of language data from a variety of sources providing you with resources to begin your own quantitative investigations.&lt;/p&gt;
&lt;div id=&#34;data-for-language-research&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data for language research&lt;/h2&gt;
&lt;p&gt;Language research can include data from a variety of sources, linguistic and non-linguistic, that record observations about the world. A typical type of data used in quantitative language research is a &lt;strong&gt;corpus&lt;/strong&gt;. In short, a corpus is set of machine-readable texts that have been compiled with an eye towards linguistic research. All corpora are not created equal, however, in content and/or format. A corpus may aim to represent a wide swath of language behavior or very specific aspects. It can be language specific (English or French), target a particular modality (spoken or written), or approximate domains of language use (medicine, business, etc.). A corpus that aims to represent a language (including modalities, registers, and sub-domains), for example, is known as a &lt;em&gt;generalized corpus&lt;/em&gt;. Corpora that aim to capture a snapshot of a particular modality or sub-domain of language use are known as &lt;em&gt;specialized corpora&lt;/em&gt;. Each corpus will have an underlying target population and the sampling process will reflect the authors’ best attempt (given the conditions at the point the corpus was compiled) at representing the stated target population. Whether a corpus is generalized or specialized can become difficult to nail down between the extremes. As such, it is key to be clear about the scope of a particular corpus to be able to ascertain its potential applications and gauge the extent to which these applications entail the research goals of your particular project.&lt;/p&gt;
&lt;p&gt;A corpus will often include various types of non-linguistic attributes, or &lt;em&gt;meta-data&lt;/em&gt;, as well. Ideally this will include information regarding the source(s) of the data, dates when it was acquired or published, and other author or speaker information. It may also include any number of other attributes that were identified as potentially important in order to appropriately document the target population. Again, it is key to match the available meta-data with the goals of your research. In some cases a corpus may be ideal in some aspects but not contain all the key information to address your research question. This may mean you will need to compile your own corpus if there are fundamental attributes missing. Before you consider compiling your own corpus, however, it is worth investigating the possibility of augmenting an available corpus to bring it inline with your particular goals. This may include adding new language sources, harnessing software for linguistic annotation (part-of-speech, syntactic structure, named entities, etc.), or linking available corpus meta-data to other resources, linguistic or non-linguistic.&lt;/p&gt;
&lt;p&gt;Corpora come in various formats, the main three being: running text, structured documents, and databases. The format of a corpus is often influenced by characteristics of the data but may also reflect an author’s individual preferences as well. It is typical for corpora with few meta-data characteristics to take the form of running text. In corpora with more meta-data a header may be appended to the top of each running text document or the meta-data may be contained in a separate file with appropriate coding to coordinate meta-data attributes with each text in the corpus. When meta-data increases in complexity it is common to structure each corpus document more explicitly with a markup language such as XML (Extensible Markup Language) or organize relationships between language and meta-data attributes in a database. Although there has been a push towards standardization of corpus formats, most available resources display some degree of idiosyncrasy. Being able to parse the structure of a corpus is a skill that will develop with time. With more experience working with corpora you will become more adept at identifying how the data is stored and whether its content and format will serve the needs of your analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sources-of-language-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sources of language data&lt;/h2&gt;
&lt;p&gt;The most common source of data used in contemporary quantitative research is the internet. On the web an investigator can access corpora published for research purposes and language used in natural settings that can be coerced by the investigator into a corpus. Many organizations exist around the globe that provide access to corpora in browsable catalogs, or &lt;strong&gt;repositories&lt;/strong&gt;. There are repositories dedicated to language research, in general, such as the &lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Language Data Consortium&lt;/a&gt; or for specific language domains, such as the language acquisition repository &lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;. It is always advisable to start looking for the available language data in a repository. The advantage of beginning your data search in repositories is that a repository, especially those geared towards the linguistic community, will make identifying language corpora faster than through a general web search. Furthermore, repositories often require certain standards for corpus format and documentation for publication. A standardized resource many times will be easier to interpret and evaluate for its appropriateness for a particular research project.&lt;/p&gt;
&lt;p&gt;In the table below I’ve compiled a list of some corpus repositories to help you get started.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-repositories&#34;&gt;Table 1: &lt;/span&gt;A list of some language corpora repositories.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus.byu.edu/&#34;&gt;BYU corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A repository of corpora that includes billions of words of data.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://corporafromtheweb.org/&#34;&gt;COW (COrpora from the Web)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of linguistically processed gigatoken web corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://wortschatz.uni-leipzig.de/en/download/&#34;&gt;Leipzig Corpora Collection&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpora in different languages using the same format and comparable sources.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.ldc.upenn.edu/&#34;&gt;Linguistic Data Consortium&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language corpora&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.resourcebook.eu/searchll.php#&#34;&gt;LRE Map&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language resources collected during the submission process for the Language Resource and Evaluation Conference (LREC).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.nltk.org/nltk_data/&#34;&gt;NLTK language data&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of corpora and language datasets included with the Python package NLTK.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/&#34;&gt;OPUS - an open source parallel corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of translated texts from the web.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Repository of language collections dealing with conversation, acquisition, multilingualism, and clinical contexts.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://corpus1.mpi.nl/ds/asv/?4&#34;&gt;The Language Archive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Various corpora and language datasets&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://ota.ox.ac.uk/&#34;&gt;The Oxford Text Archive (OTA)&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of thousands of texts in more than 25 different languages.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Repositories are by no means the only source of corpora on the web. Researchers from around the world provide access to corpora and other data sources on their own sites or through data sharing platforms. Corpora of various sizes and scopes will often be accessible on a dedicated homepage or appear on the homepage of a sponsoring institution. Finding these resources is a matter of doing a web search with the word ‘corpus’ and a list of desired attributes, including language, modality, register, etc. As part of a general movement towards reproducible more corpora are available on the web than ever before. Therefore data sharing platforms supporting reproducible research, such as &lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt;, &lt;a href=&#34;https://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;, &lt;a href=&#34;http://www.re3data.org/&#34;&gt;Re3data&lt;/a&gt;, etc., are a good place to look as well, if searching repositories and targeted web searches do not yield results.&lt;/p&gt;
&lt;p&gt;In the table below you will find a list of corpus resources and datasets.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-corpora&#34;&gt;Table 2: &lt;/span&gt;Corpora and language datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.socsci.uci.edu/~lpearl/CoLaLab/CHILDESTreebank/childestreebank.html&#34;&gt;CHILDES Treebank&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://catalog.ldc.upenn.edu/docs/LDC97S62/&#34;&gt;The Switchboard Dialog Act Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus of 1155 5-minute conversations in American English, comprising 205,000 utterances and 1.4 million words, from the Switchboard corpus of telephone conversations.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://storage.googleapis.com/books/ngrams/books/datasetsv2.html&#34;&gt;Google Ngram Viewer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Google web corpus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~enron/&#34;&gt;Enron Email Dataset&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Enron email data from about 150 users, mostly senior management.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://cesa.arizona.edu/&#34;&gt;Corpus of Spanish in Southern Arizona&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Spanish varieties spoken in Arizona.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://opus.lingfil.uu.se/OpenSubtitles_v2.php&#34;&gt;OpenSubtitles2011&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A collection of documents from &lt;a href=&#34;http://www.opensubtitles.org/&#34; class=&#34;uri&#34;&gt;http://www.opensubtitles.org/&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.statmt.org/europarl/&#34;&gt;Europarl Parallel Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A parallel corpus based on the proceedings of the European Parliament&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.lllf.uam.es/~fmarcos/informes/corpus/coarginl.html&#34;&gt;Corpus Argentino&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Corpus of Argentine Spanish&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.ruscorpora.ru/en/&#34;&gt;Russian National Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus of modern Russian language incorporating over 300 million words.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html&#34;&gt;Cornell Movie-Dialogs Corpus&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A corpus containing a large metadata-rich collection of fictional conversations extracted from raw movie scripts.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is important to note that there can be access and use restrictions for data from particular sources. Compiling, hosting, and maintaining corpus resources can be costly. To gain full access to data, some repositories and homepages of larger corpora require a fee to offset these costs. In other cases, resources may require individual license agreements to ensure that the data is not being used in ways it was not intended or to ensure potentially sensitive participant information will be treated appropriately. You can take a look at a &lt;a href=&#34;https://www.corpusdata.org/restrictions.asp&#34;&gt;license agreement for the BYU Corpora&lt;/a&gt; as an example. If you are a member of an academic institution and aim to conduct research for scholarly purposes licensing is often easily obtained. Fees, on the other hand, may present a more challenging obstacle. If you are an affiliate of an academic institution it is worth checking with your library to see if there are funds for acquiring licensing for you as an individual, a research group or lab or, for the institution.&lt;/p&gt;
&lt;p&gt;If your corpus search ends in a dead-end, either because a suitable resource does not appear to exist or an existing resource is unattainable given licensing restrictions or fees, it may be time to compile your own corpus. Turning to machine readable texts on the internet is usually the logical first step to access language for a new corpus. Language texts may be found on sites as uploaded files, such as pdf or doc (Word) documents, or found displayed as the primary text of a site. Given the wide variety of documents uploaded and language behavior recorded daily on social media, news sites, blogs and the like, compiling a corpus has never been easier. Having said that, how the data is structured and how much data needs to be retrieved can pose practical obstacles to collecting data from the web, particularly if the approach is to acquire the data by hand instead of automating the task. Our approach here, however, will be to automate the process as much as possible whether that means leveraging R package interfaces to language data, converting hundreds of pdf documents to plain text, or scraping content from web documents.&lt;/p&gt;
&lt;p&gt;The table below lists some R packages that serve to interface language data directly through R.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-apis&#34;&gt;Table 3: &lt;/span&gt;R Package interfaces to language corpora and datasets.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/crminer&#34;&gt;crminer&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface focusing on getting the user full text via the Crossref search API.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/arxiv_tutorial.html&#34;&gt;aRxiv&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query arXiv, a repository of electronic preprints for computer science, mathematics, physics, quantitative biology, quantitative finance, and statistics.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/internetarchive_tutorial.html&#34;&gt;internetarchive&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/dvn&#34;&gt;dvn&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to access to the Dataverse Network APIs.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/gutenbergr_tutorial.html&#34;&gt;gutenbergr&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to download and process public domain works from the Project Gutenberg collection.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ropensci.org/tutorials/fulltext_tutorial.html&#34;&gt;fulltext&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query open access journals, such as PLOS.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/hrbrmstr/newsflash&#34;&gt;newsflash&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the Internet Archive and GDELT Television Explorer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/oai&#34;&gt;oai&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query any OAI-PMH repository, including Zenodo.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/rfigshare&#34;&gt;rfigshare&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to query the data sharing platform FigShare.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Data for language research is not limited to (primary) text sources. Other sources may include processed data from previous research; word lists, linguistic features, etc.. Alone or in combination with text sources this data can be a rich and viable source of data for a research project.&lt;/p&gt;
&lt;p&gt;Below I’ve included some processed language resources.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-experimental&#34;&gt;Table 4: &lt;/span&gt;Language data from previous research and meta-studies.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/ropensci/lingtypology&#34;&gt;lingtypology&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R package interface to connect with the Glottolog database and provides additional functionality for linguistic mapping.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://elexicon.wustl.edu/WordStart.asp&#34;&gt;English Lexicon Project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Access to a large set of lexical characteristics, along with behavioral data from visual lexical decision and naming studies.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://icon.shef.ac.uk/Moby/&#34;&gt;The Moby lexicon project&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Language wordlists and resources from the Moby project.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The list of data available for language research is constantly growing. I’ve document very few of the wide variety of resources. Below I’ve included attempts by others to provide a summary of the corpus data and language resources available.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:scrape-pinboard-listing&#34;&gt;Table 5: &lt;/span&gt;Lists of corpus resources.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Resource&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://uclouvain.be/en/research-institutes/ilc/cecl/learner-corpora-around-the-world.html&#34;&gt;Learner corpora around the world&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;A listing of learner corpora around the world&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://makingnoiseandhearingthings.com/2017/09/20/where-can-you-find-language-data-on-the-web/&#34;&gt;Where can you find language data on the web?&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of various corpora and language datasets.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;http://nlp.stanford.edu/links/statnlp.html#Corpora&#34;&gt;Stanford NLP corpora&lt;/a&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Listing of corpora and language resources aimed at the NLP community.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we have covered some of the basics on data for language research including types of data and sources to get you started on the path of identifying a viable source for your data analysis project. In the next post we will begin working directly with R code to access and acquire data through R. Along the way I will introduce fundamental programming concepts of the language you will use throughout your project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- Topics:

- natural and naturalistic
modality, register (formal/ informal), 
  - spoken (Callhome -LDC, ...)
  - written (literature -gutenbergr, periodicals -LDC, blogs, social media -streamR, ...)
  - other (tv/film closed captions -newsflash, Brysbaert/ ACTIV-ES, translation -OPEL?, ...)
- elicited data
  - essays (-BELC, )
  - interviews (Santa Barbara Corpus)
  - surveys (US Census -acs, Harvard Dialect Survey, Language attitude)
  - experimental findings
    - response times (MRC lexicon), eye-gaze, acceptability ratings, etc. 
    
- 


- Types of data
  - Uncurated
  - Curated 
- Sources of data
    - Repositories
    - Sources
    - Web
- Data formats
  - Plain text
    - .txt
    - .csv/ .tsv
    - .xml/ .json
  - Other files
    - .doc(x)
    - .pdf
  - Databases
- How to acquire data
    - Package interface (`gutenbergr`)
    - API interface (`streamR`)
    - Download, read (multiple files, w/ lapply and scan?
    - Web scraping (`rvest`)
--&gt;
&lt;!-- ## Types of data --&gt;
&lt;!-- In the last post we discussed what data is and the importance of data sampling and organization is for subsequent data analysis. We touched briefly on an example in which we were working with files which our language data was in running text format. Running text is one of the types of data that you will encounter when you look to obtain data to conduct research into the topic you are interested in knowing something more about. In this post we leared that text in this format needed to be organized into a format that was more conducive for statistical analysis. The aim, then, was to **curate** the **uncurated** data. Athough data is often talked about in terms of being curated or uncurated, it is important to understand that this is less a dicotomy and more of a continuum. Since each analysis has specific goals the primary data is always in need of some amount of curation to prepare the data in the particular ways necessary to faciliate the particular goals of the particular analysis.   --&gt;
&lt;!-- ## Sources of data --&gt;
&lt;!-- Having said that it is convenient to talk about curation in binary terms as it facilitates a discussion of the sources of data out there and to what degree the researcher needs to manipulate the data from particular sources to be able to conduct an analysis. When looking at the landscape of language data available, sources that have been prepared to some degree to facilitate analysis are often found in *repositories*. There are countless repositories that can be accessed on the web.  --&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-mcenery:2012&#34;&gt;
&lt;p&gt;McEnery, Tony, and Andrew Hardie. 2012. &lt;em&gt;Corpus Linguistics: Method, Theory and Practice&lt;/em&gt;. Cambridge University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to statistical thinking</title>
      <link>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</link>
      <pubDate>Fri, 15 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/09/15/introduction-to-statistical-thinking/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Before we begin working on the specifics of our data project, it is important to have a clear understanding of some of the basic concepts that need to be in place to guide our work. In this post I will cover some of these topics including the importance of identifying a research question, how different statistical approaches relate to different types of research, and understanding data from a sampling and organizational standpoint. I will also provide some examples of linking research questions with variables in a toy dataset as we begin to discuss how to approach data analysis, primarily through visualization techniques.&lt;/p&gt;
&lt;div id=&#34;research-aims&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Research aims&lt;/h2&gt;
&lt;!-- TODO: pepper in examples of the types of questions and methods one might find --&gt;
&lt;p&gt;Before jumping into the code, every researcher must come to a project with a clear idea about the purpose of the analysis. This means doing your homework in order to understand what it is exactly that you want to achieve; that is, you need to identify a &lt;strong&gt;research question&lt;/strong&gt;. The first step is become versed in the previous literature on the topic. What has been written? What are the main findings? Secondly, it is important to become familiar with the standard methods for approaching the topic of interest. How has the topic been approached methodologically? What are the types, sources, and quality of data employed? What have been the statistical approaches employed? What particular statistical tests have been chosen? Getting an overview not only of the domain-specific findings in the literature but also the methodological choices will help you identify promising plan for carrying out your research.&lt;/p&gt;
&lt;!-- All other steps in an analysis will be guided by this question and hinge on the choices you make conceptually to carry out a data analysis plan.  --&gt;
&lt;!-- * Identify a research question --&gt;
&lt;!--   - research previous literature --&gt;
&lt;!--   - get to know the nature of the phenomenon --&gt;
&lt;/div&gt;
&lt;div id=&#34;choosing-a-statistical-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Choosing a statistical approach&lt;/h2&gt;
&lt;p&gt;With a research question in hand and a sense of how similar studies have approached the topic methodologically, it’s time to make a more refined decision about how the data is to be analyzed. This decision will dictate all other methodological choices from data collection to interpreting results.&lt;/p&gt;
&lt;p&gt;There are three main statistical approaches:&lt;/p&gt;
&lt;!-- * Identify goals of your analysis --&gt;
&lt;div id=&#34;inference&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Inference&lt;/h3&gt;
&lt;!-- TODO: find a way to introduce the concept &#39;model&#39;. this could make it easier to speak about statistical approaches, here and later in the post --&gt;
&lt;!-- TODO: pepper in examples of each type of statistical approach to ground these concepts a bit --&gt;
&lt;p&gt;Also commonly known as hypothesis testing or confirmation, statistical inference aims to establish whether there is a reliable and generalizable relationship given patterns in the data. The approach makes the starting assumption that there is no relationship, or that the null hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_0\)&lt;/span&gt;) is true. A relationship is only reliable, or &lt;em&gt;significant&lt;/em&gt;, if the chance that the null hypothesis is false is less than some predetermined threshold; in which case we accept the alternative hypothesis (&lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt;). The standard threshold used in the Social Sciences, Linguistic included, is the famous p-value &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .05\)&lt;/span&gt;. Without digging into the deeper meaning of a p-value, in a nutshell a p-value is a confidence measure to suggest that the relationship you are investigating is robust and reliable given the data. In an inference approach all the data is used and is used &lt;em&gt;only&lt;/em&gt; once. This is not the case for the other two statistical approaches we will cover, Exploration and Prediction. For this reason it is vital to identify your statistical approach from the beginning. In the case of inference tests, failing to make a clear hypothesis often leads to p-hacking; a practice of running multiple tests and/or parameters on the same data (i.e. reusing the data) until evidence for the alternative hypothesis appears.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Exploration&lt;/h3&gt;
&lt;p&gt;One of two statistical learning approaches, this statistical method is used to uncover potential relationships in the data and gain new insight in an area where predictions and hypotheses cannot be clearly made. In statistical learning, exploration is a type of &lt;strong&gt;unsupervised learning&lt;/strong&gt;. Supervision here, and for Prediction, refers to the presence or absence of an outcome variable. By choosing exploration as our approach we make no assumptions (or hypotheses) about the relationships between any of the particular variables in the data. Rather we hope to investigate the extent to which we can induce meaningful patterns wherever they may lie. Findings from exploratory analyses can provide valuable insight for future study but they cannot be safely used to generalize to the larger population, which is why exploratory analyses are often known as hypothesis generating analyses (rather than hypothesis confirming). Given our generalizing power is curtailed, the data &lt;em&gt;can&lt;/em&gt; be reused multiple times trying out various tests. While it is not strictly required, data for exploratory analysis is often partitioned into two sets, training and validation, at roughly an 80%/20% split. The training set is used for refining statistical measures and the test set is used to evaluate the refined measures. Although the evaluation results still cannot be used to generalize, the insight can be taken as stronger evidence that there is a potential relationship, or set of relationships, worthy of further study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prediction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Prediction&lt;/h3&gt;
&lt;p&gt;The other statistical learning approach, Prediction, aims to uncover relationships in our data as they pertain to a particular outcome variable. This approach is known as &lt;strong&gt;supervised learning&lt;/strong&gt;. Similar to Exploration in many ways, this approach also makes no assumptions about the potential relationships between variables in our data and the data can be used multiple times to refine our statistical tests in order to tease out the most effective method for our goals. Where an exploratory analysis aims to uncover meaningful patterns of any sort, prediction, however, is more focused in that the main aim is to ascertain the extent to which the variables in the data pattern, individually or together, in such a way to make reliable associations to a particular outcome variable in unseen data. To evaluate the robustness of a prediction model the data is partitioned into training and validation sets. Depending on the application and the amount of available data, a third ‘development’ set is sometimes created as a pseudo test set to facilitate the testing of multiple approaches before the final evaluation. The proportions vary, but it a good rule of thumb is to reserve 60% of the data for training, 20% for development, and 20% for validation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;understanding-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Understanding data&lt;/h2&gt;
&lt;p&gt;Knowing the statistical approach to take then frames the next conceptual steps: &lt;strong&gt;data sampling&lt;/strong&gt; and &lt;strong&gt;organization of data&lt;/strong&gt;. But what is data anyway? Abstractly it is some set of empirical observations about the world. There are innumerable types of observations, as you can imagine, which can be used to describe objects and events. Our scientific aim is to systematically attempt to relate these observations and deduce the nature of their relationships to gain a better understanding of how our world works.&lt;/p&gt;
&lt;p&gt;Language research aims to understand a subset of these observations, namely those that concern linguistic behavior. The psycholinguist may observe the reaction times in a lexical decision task, eye-gaze in a visual world paradigm, or electro-magnetic brain activation in an ERP study. A sociolinguist may conduct interviews with members of a community, solicit language attitude responses to a language attitude survey, or ethnographically record face-to-face encounters. A syntactician may solicit acceptability ratings, calculate the frequency of a syntactic structure in a corpus, or document the permutations of subject-verb-object order in the world’s languages. As language is a defining characteristic of our species, language-related observations feature many other disciplines as well such as Anthropology, History, Neurology, Mathematics, and Biology. Linguistic inquiry, then, is not isolated to linguistic form, but rather the connection between linguistic form and other non-linguistic objects and events in the world at large –wherever that may take us.&lt;/p&gt;
&lt;!-- * Emperical observations --&gt;
&lt;!--   - Data is empirical observations about the world --&gt;
&lt;!--   - Innumberable types of observations: linguistic and non-linguistic --&gt;
&lt;!--   - Examples (acceptability ratings, reaction times, words in a corpus, lengths of words, age, sex, occupation, (non)-native speaker, etc.) --&gt;
&lt;div id=&#34;sampling&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Sampling&lt;/h3&gt;
&lt;p&gt;One major limitation inherent to most data sampling, and a primary reason why statistics are so important to doing and interpreting science, is the fact that our vantage point to the observing the world is restricted. We can only work with the data at our disposal, a &lt;strong&gt;sample&lt;/strong&gt;, even when it is clear that there is a much larger existing world, or &lt;strong&gt;population&lt;/strong&gt;. Ideally we would have access to the entire population of interest, but in most cases this is either not physically possible to obtain (or even store) the data or it is conceptually impossible to ever observe the entire population. As an example, say we wanted to catalog all the words in the English language. From a logistics point of view, where would we start? Any given dictionary only catalogs a subset of the words in a language –many words that are used in English-speaking communities, especially those from spoken language, will not appear. A corpus may capture linguistic diversity that does not appear in a dictionary, but it too will fall short of our lofty goal. But for argumentation sake, let’s imagine we could somehow capture all the words. What happens to our population in a day, a week, or a month from now? It quickly becomes a sample because new words are created all the time and some words are lost. Our population of words in the English language, then, is a moving target.&lt;/p&gt;
&lt;p&gt;This transitory property of populations is well-known and methods for obtaining reliable, or externally valid, samples is an area of study in its own right. In short, we aim for a sample to be balanced and representative of the idealized population. &lt;em&gt;Representativeness&lt;/em&gt; is the extent to which a sample reflects the total diversity in the population. &lt;em&gt;Balance&lt;/em&gt; is concerned with modeling the proportions of that diversity. An ideal sample combines both.&lt;/p&gt;
&lt;p&gt;The first strategy most often applied to obtaining a valid sample is to increase &lt;em&gt;sample size&lt;/em&gt;. This is an intuitive technique whose logic appeals to the notion that more is better. More is better, clearly. But more data alone does not always ensure an externally valid sample. For example, say we want to know something about the frequencies of words in written Spanish. Our target population is, then, words written in Spanish. It occurs to us that we can access a lot of written Spanish online via &lt;a href=&#34;http://www.gutenberg.org/&#34;&gt;Project Gutenberg&lt;/a&gt;. We download works from many authors over a span of many years. After doing some calculations our sample contains around 100 million words. That’s a lot of words, and surely more indicative of the population than say 100 thousand words. But we have potentially overlooked something very important: all of the data in our sample comes from literature, specifically literature in the public domain. In other words, our sample is not random.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;random sample&lt;/em&gt; will help increase the potential diversity in any sample. In our sample this means drawing data from a number of written sources of Spanish at random. This strategy will increase our chances to capture written Spanish from other genres and registers. Now a 100 million word sample randomly selected from genres and registers of written Spanish is bound to be more representative of the population, but we run into another conceptual snag. Our sample is large and randomly selected from the population, but does it reflect the proportion each subgroup (genres and registers) contributes to the idealized population?&lt;/p&gt;
&lt;p&gt;There is no absolute way of knowing if the proportions of each subgroup are balanced, or even what all the subgroups may be for that matter, but in most cases we can make an educated guess on both these fronts that will allow us to increase the validity of our sample. For example, the literary genre ‘self-help’ intuitively constitutes a smaller portion of our target population than say ‘news’. Ideally we would want to reflect this understanding in our sample. Applying this logic is known as &lt;em&gt;stratified sampling&lt;/em&gt;. A large, stratified random sample is always at least as valid as an equally sized large random sample with the added benefit that we are safeguarded from large skews that a large random sample may potentially produce. Now it is important to keep in mind that stratified sampling has its limitations as well. The difficulties posed in obtaining a valid sample from the macro view (i.e. the total population is never observable) are present at the micro view as well (i.e. sub- and sub-substrata are equally illusive). Again, there are no absolutes in sampling. The key is to keep the aim of the research question clear during the sampling process and strive for sizable, randomly stratified samples to minimize sampling error to the extent that it is feasible –and then work from there.&lt;/p&gt;
&lt;p&gt;This lack of certainty in sampling may seem troublesome. Sampling uncertainty, however, does not mean we cannot gain insight into the essence of the objects and events in the world we aim to understand. It just means we need to be aware of any given sample’s limitations, document these limitations, and always approach statistical findings based on this data with caution; suspending generalizations of the absolute nature. This is why science, contrary to popular belief, does not ‘prove’ anything. Rather science aims to collect evidence for or against a hypotheses. Since the data is always changing there are no absolute conclusions. As the evidence grows, so does the case for a particular view of how the world works. It is this systematic approach which makes science so powerful.&lt;/p&gt;
&lt;!-- * Populations and samples --&gt;
&lt;!--   - Ideally we would have access to the entire record of those observation that are of potential interest to us in our analysis --&gt;
&lt;!--   - Reality is we, more often than not, cannot feasibly acquire the observations of an entire population. --&gt;
&lt;!--   - The best we can do is shoot for a sample of that population which aims to model the population as appropriately possible given the particular research question to be addressed --&gt;
&lt;!--   - In the end we will always strive for a balanced and representative sample. A balanced sample contains the predicted major features indicative of the population. A representative sample contains these features in a way that reflect the proportion of these features.  --&gt;
&lt;!--   - Attaining the most ideal sample given the potential limitations is key to gaining robust and generalizable insight into the nature of the phenomenon we are investigating.  --&gt;
&lt;/div&gt;
&lt;div id=&#34;organization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Organization&lt;/h3&gt;
&lt;p&gt;Identifying and capturing a data sample moves us one step closer to performing our data analysis but the format of the raw or original data is often not in a format conducive for visualization nor statistical tests. The hypothetical written Spanish data we identified to sample in the previous section would most likely take the form of documents of running text with potentially some meta-data about the text (author, title of the work, date published, genre, etc.) in the header of the file and/or the name of each file.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;Title: Cuando los robots tomen el mando y hagan la guerra
Date: 3 AGO 2015 - 00:00 CEST
Genre: News
Source: El País
Tags: Científicos, Isaac Asimov, Robótica, Gente, Tecnología, Informática, Ciencia, Sociedad, Industria

La primera reflexión abarcadora sobre la coexistencia entre los robots y los humanos no fue obra de un científico de la computación ni de un filósofo ético, sino de un novelista. Isaac Asimov formuló las tres “leyes de la robótica” que deberían incorporarse en la programación de cualquier autómata lo bastante avanzado como para suponer un peligro: “No dañar a los humanos, obedecerles salvo conflicto con lo anterior y autoprotegerse salvo conflicto con todo lo anterior”. Las tres leyes de Asimov configuran una propuesta sólida y autoconsistente, y cuentan con apoyo entre la comunidad de la inteligencia artificial, que reconoce, por ejemplo, que cualquier sistema autónomo funcional debe ser capaz de autoprotegerse.
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As raw data this format is fine, but to gain insight from this data, we will need to explicitly organize the attributes of our data that are key to our analysis. Our data should be in tabular, or &lt;a href=&#34;www.jstatsoft.org/v59/i10/paper&#34;&gt;‘tidy’ format&lt;/a&gt; where each row is an observation, or &lt;strong&gt;case&lt;/strong&gt; and each column, or &lt;strong&gt;variable&lt;/strong&gt; is a list of attributes of the observation. Each cell, then, is a particular attribute of a particular observation, or &lt;strong&gt;data point&lt;/strong&gt;. Say our objective is to perform an exploratory analysis to evaluate the potential similarities and differences in word frequencies between genres. For this particular analysis we will want to extract and organize the title of each document (&lt;code&gt;doc_id&lt;/code&gt;), the genre it is from (&lt;code&gt;genre&lt;/code&gt;), and each word (&lt;code&gt;word&lt;/code&gt;) as a single, row, or observation, in our tidy dataset.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:spanish-works&#34;&gt;Table 1: &lt;/span&gt;Tidy dataset&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;doc_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genre&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sociales&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;anterior&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;el&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;sus&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;también&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;aun&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;así&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;nieves&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;encerrar&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;adiós&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;por&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This tidy organization may seem somewhat redundant; a single value for &lt;code&gt;doc_id&lt;/code&gt; is repeated for each value of &lt;code&gt;word&lt;/code&gt; and a single value of &lt;code&gt;genre&lt;/code&gt; is repeated for each value of &lt;code&gt;doc_id&lt;/code&gt;. However tidy data, although visually redundant, is an explicit description of the relationship between our variables. Each row corresponds to all of the necessary attributes to describe a particular observation. In this data, the occurrence of a word is associated with the file it appears in and the genre that file is associated with.&lt;/p&gt;
&lt;!-- TODO: consider the format that is required for doing a cluster analysis. Is my discussion leading in that direction, or is it complicated by potentially needing to create a term-document matrix? Look at the `tidytext` package vignette. Might need to change the discussion to focus on an inference analysis: assume a hypothesis that a specific register is more lexically diverse than another. Include genre as a factor? --&gt;
&lt;p&gt;Our objective in this toy example is to explore the relationship between word frequencies and genres, yet at this point there is no explicit variable for the frequencies of words. The information we need, however, is in the data and since we have an organized, tidy dataset, calculating &lt;code&gt;word_freq&lt;/code&gt; is a matter of tabulating the occurrences of each word. This can be done easily with R, as we will see in detail in future posts, but for our discussion on data organization let’s skip the details and jump to the new dataset with a column for &lt;code&gt;word_freq&lt;/code&gt;.&lt;/p&gt;
&lt;!-- example tabular data: doc_id, genre, word. Adding the word_freq column --&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:spanish-works-freq&#34;&gt;Table 2: &lt;/span&gt;Tidy dataset with &lt;code&gt;word_freq&lt;/code&gt; variable.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;doc_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;genre&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;word&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;word_freq&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;747&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;la&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;559&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;el&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;376&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;en&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;309&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cosmografía&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Astronomy&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;306&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;la&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;y&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;los&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Cuando los robots tomen el mando y hagan la guerra&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;News&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;to&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;314&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;de&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;217&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;206&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;que&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;175&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Heath’s Modern Language Series: El trovador&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Opera&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;_m&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;172&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Other measures and/or attributes can be added as necessary to this tabular format and in some cases we may convert our tidy tabular dataset to other data formats that may be required for some particular statistic approaches but at all times the relationship between the variables should be maintained in line with our research purpose. We will touch on examples of other types of data formats when we dive into particular statistical approaches that require them later in the series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;informational-value&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Informational value&lt;/h3&gt;
&lt;!-- levels of measurement --&gt;
&lt;p&gt;Let’s turn now to the informational nature of our variables as it will set up how we implement our data analysis. Taking our variable &lt;code&gt;word_freq&lt;/code&gt; as an example, it is important to point out there are many ways to define ‘frequency’. Some frequency measures are more appropriate than others given the statistical approach we intend to apply to our data. Our current dataset contains raw frequency scores, that is the frequency is measured in observed counts for each word in each file of our data. We could, for example, instead bin our frequency scores under the labels “high” and “low” frequency converting frequency from counts to labels. In this case we change the &lt;strong&gt;informational value&lt;/strong&gt; of &lt;code&gt;word_freq&lt;/code&gt;. Some variables in our dataset, on the other hand, cannot be converted. Take for example, &lt;code&gt;genre&lt;/code&gt;. The values for &lt;code&gt;genre&lt;/code&gt; label the genre of the file from which the word was observed. We could of course summarize the genres under meta-genres, but we maintain labeled data; the same informational value as before.&lt;/p&gt;
&lt;p&gt;Understanding the informational value of variables in key to organizing and preparing your data for analysis as it has implications for what insight we can gain from the data and what visualization techniques and statistical measures we can use to interrogate the data. There are four potential informational values for all data: nominal, ordinal, interval, and ratio.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Nominal variables&lt;/em&gt; contain attributes which are labels denoting the membership in a class in which there is no relationship between the labels. Examples of nominal data include part-of-speech labels, the sex of a participant, the genre of a text, etc.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Ordinal variables&lt;/em&gt; also contain labels of classes, but in contrast to nominal variables, there is a relationship between the classes, namely one in which there is a precedence relationship or rank. Our frequency conversion from scores to high- and low-frequency bins is a type of ordinal data –there is an explicit ordering of these two categories. Grouping participants in a study as “young”, “middle-aged”, and “old” would also be ordinal values; again, each value can be interpreted in relationship to the other values.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Interval variables&lt;/em&gt; are like ordinal variables in which there is an explicit precedence relationship, but in addition the values describe precise intervals between each value. So take our earlier operationalization of age as “young”, “middle-aged”, and “old”. As an ordinal variable no assumption is made that the differences in age between young and middle-aged are the same as between middle-aged and old –only that one class is ordered before or after another. If our criterion to code our values of age, however, were based regular intervals between age groups, not some non-regular assignment, then our values of age would be interval-valued.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Ratio variables&lt;/em&gt; have all the properties of interval variables but also include a non-arbitrary definition of zero. Frequency counts are ratio variables as it is clear that there is a potential value for 0 and any value greater can be interpreted in reference to this anchor. A word with a frequency of 100 is two times as large as a word with frequency 50. By the same token, a participant that is 20 years old is half the age of a 40 year old participant.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These informational types are often described in macro terms grouping nominal and ordinal variables as &lt;strong&gt;categorical variables&lt;/strong&gt; and interval and ratio variables as &lt;strong&gt;continous variables&lt;/strong&gt;. All continuous variables can be converted to categorical variables, but the reverse is not true. In most cases it is preferred to cast your data as continuous, if the nature of the variable permits it, as the recasting of continuous data to categorical data results in a loss of information –which will result in a loss of statistical power and may lead to results that obscure meaningful patterns in the data &lt;span class=&#34;citation&#34;&gt;(Baayen 2004)&lt;/span&gt;. &lt;!-- Baayen 2004 --&gt;&lt;/p&gt;
&lt;!-- The structure of raw, or original, data sampled varies  --&gt;
&lt;!-- * Informational value of variables --&gt;
&lt;!--   - In preparation for analysis, the research must be cognicent of the informational status of the variables of the data --&gt;
&lt;!--   - Variables are the observational features, attributes, and measures --&gt;
&lt;!--   - Ultimately visualizing and performing statistical operations on your data to analyze your data will depend on these variables and the information they contain. --&gt;
&lt;!--   - There are four informational types of variables: --&gt;
&lt;!--     * Nominal --&gt;
&lt;!--     * Ordinal --&gt;
&lt;!--     * Ratio --&gt;
&lt;!--     * Interval --&gt;
&lt;!--   - For convenience, we often talk about two major classes --&gt;
&lt;!--     * Categorical (nominal/ ordinal) --&gt;
&lt;!--     * Continuous (ratio/ interval) --&gt;
&lt;!--   - (Examples of real data sets) --&gt;
&lt;!-- * Operationalizing variables --&gt;
&lt;!--   - While some variables and their informational value are self-evident (such as sex, or age), some need to be interpreted and explicitly formulated --&gt;
&lt;!--   - Measurements --&gt;
&lt;!--     * Some variables contain information that needs to be calculated (frequencies, dispersion, etc.) or transformed (log, normalization (z-scores, etc.)) to be meaningful for analysis --&gt;
&lt;/div&gt;
&lt;div id=&#34;dependent-and-independent-variables&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dependent and independent variables&lt;/h3&gt;
&lt;p&gt;The last step before we move to visualization and statistical tests is to identify our &lt;strong&gt;dependent variable&lt;/strong&gt; and/ or &lt;strong&gt;independent variables&lt;/strong&gt;. A dependent variable is the outcome variable that is used in inference and prediction analyses that reflects the observations of the behavior we want to gain understanding about. The identification of a dependent variable should be guided by your research question; it is the measure of the phenomenon in question. An independent variable is a predictor variable, or a variable which we assume will be related to the values of the dependent variable in some systematic way. There is typically only one dependent variable in an analysis, but there can be multiple independent variables. In an exploratory analysis, however, all the variables are independent variables as this approach assumes no particular relationship between the variables; the goal in this approach, remember, is to uncover patterns that may suggest a relationship between particular set of variables.&lt;/p&gt;
&lt;!-- * Dependent and independent variables --&gt;
&lt;!--   - Another key distiction to be made with our variables concerns the linking of our variables with the research question and statistical approach we intend to use to explore this question and it&#39;s predictions.  --&gt;
&lt;!--   - A dependent variable is the variable that is considered the outcome measurement to be understood.  --&gt;
&lt;!--   - Independent variables are those variables that we aim to use to understand the nature and variability of the dependent variable --&gt;
&lt;!--   - In hypothesis testing or prediction a dependent variable must be identified (in prediction this variable is also called the class or outcome variable) --&gt;
&lt;!--   - For exploration, where the goal is to discover patterns, the dependent variable is unknown and is the feature that we aim to discover --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data analysis&lt;/h2&gt;
&lt;p&gt;The primary goal of a data analysis is to reduce the observed data to a human-interpretable summary that best approximates the nature of the phenomenon we are investigating. With well-sampled data in a tidy dataset in hand where observations and variables are explicitly related, identified for their informational value, and the dependent and/or independent variables are clear, we can now proceed to visualizing and applying the appropriate statistical tests to the data to come to some more concrete, actionable insight.&lt;/p&gt;
&lt;div id=&#34;visualization&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualization&lt;/h3&gt;
&lt;p&gt;It is always key to gain insight into the behavior of the data visually before jumping in to the statistical analysis. Using our research aim as our guide, we will choose the most appropriate visualization to use given the number and informational value of our target variables. To get a sense of how this looks, let’s work with an example dataset and pose different questions to the data with an eye towards seeing how various combinations of variables are visualized.&lt;/p&gt;
&lt;p&gt;The dataset we will use here is from the &lt;a href=&#34;http://talkbank.org/&#34;&gt;TalkBank repository&lt;/a&gt; which provides data from various language learning contexts.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The specific data we will use is the ‘narratives’ section of the &lt;a href=&#34;http://talkbank.org/access/SLABank/English/BELC.html&#34;&gt;BELC (Barcelona English Language Corpus)&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Muñoz 2006)&lt;/span&gt;. It is a corpus of writing samples from second language learners of English at different ages. Participants were given the task of writing for 15 minutes on the topic of “Me: my past, present and future”. Data was collected for many (but not all) participants up to four times over the course of seven years. The entire dataset includes 123 observations from 54 participants. Below I’ve included the first 10 observations from the dataset which reflects some data cleaning I’ve done so we start with a tidy dataset.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:view-belc-dataset&#34;&gt;Table 3: &lt;/span&gt;BELC dataset for demonstration.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;participant_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;learner_group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tokens&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The variables &lt;code&gt;participant_id&lt;/code&gt;, &lt;code&gt;sex&lt;/code&gt;, and &lt;code&gt;age&lt;/code&gt; should be self-explanatory. &lt;code&gt;learner_group&lt;/code&gt; contains the values 1-4 which record the stage for each participant formally learning English. The number of words written in each sample is listed for each participant at each stage in the variable &lt;code&gt;tokens&lt;/code&gt;. We should also note the informational value of these variables. &lt;code&gt;participant_id&lt;/code&gt;, &lt;code&gt;sex&lt;/code&gt;, and &lt;code&gt;learner_group&lt;/code&gt; are categorical variables; both &lt;code&gt;participant_id&lt;/code&gt; and &lt;code&gt;sex&lt;/code&gt; are nominal and &lt;code&gt;learner_group&lt;/code&gt; is ordinal. &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; are continuous variables; both of the ratio type as they are scaled in relation to a non-arbitrary value for zero.&lt;/p&gt;
&lt;p&gt;With general understanding of the data, let’s run through various data analysis scenarios and their corresponding visualizations grouping them by the information value of the dependent variable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Categorical dependent variable&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Starting basic, let’s say we are interested in investigating the difference in the number of &lt;code&gt;males&lt;/code&gt; and &lt;code&gt;females&lt;/code&gt; in our study. This is not a particularly interesting question, but it allow us to illustrate a scenario in which we have a single dependent variable, &lt;code&gt;sex&lt;/code&gt;, which is categorical. When summarizing categorical data we produce counts of each of the levels of that variable. We can visualize this summary in one of two ways, textually and graphically. A text summary would look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sex
female   male 
    67     56 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A graphic display does not necessarily facilitate a better understanding, in such a simple case, but let’s graphically visualize this scenario anyway. The type of plot we want to use is a ‘bar plot’, which simply plots the dependent variable on the x-axis and the counts on the y-axis.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-1.png&#34; alt=&#34;Bar plot of the categorical variable `sex`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Bar plot of the categorical variable &lt;code&gt;sex&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Inspecting these visualizations it is clear that there is a numeric difference between the number of writing samples in the data written by women. At this point, however, we only have a trend. To decide whether this is a reliable contrast is the purpose of our statistical tests, but we’ll leave the details of statistical testing for this scenario, and those that follow, for subsequent posts.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One categorical independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A more common scenario is one in which we have a categorical dependent variable and a categorical independent variable. With our data we can investigate the relationship between &lt;code&gt;sex&lt;/code&gt; and the &lt;code&gt;learner_group&lt;/code&gt;. Are there more males than females in a particular learner group? In this case both variables are categorical and the dimensions are such that we can textually represent them and gain some insight.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;        learner_group
sex       1  2  3  4
  female 20 24 13 10
  male   15 23 13  5&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s more difficult to see the pattern here than in the basic single dependent variable scenario for two reasons: 1) as the number of independent variables and/or the levels within an independent variable increase, our ability to interpret the results decreases. 2) the relationship between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt; does not take into account that there are more female samples than males, and therefore the raw counts here can be misleading.&lt;/p&gt;
&lt;p&gt;A graphic representation of this contrast will be a bit easier to interpret; although it is important to be aware that more variables and levels always leads to interpretability problems. The bar plot below reflects the raw counts from the cross-tabulation of the variables &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-graph-1.png&#34; alt=&#34;Bar plot of the variable `sex` and `learner_group`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Bar plot of the variable &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Adjusting the bar plot to account for the proportions of males to females in each group provides a clearer picture of the relationship between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-graph-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-graph-2-1.png&#34; alt=&#34;Bar plot of the variable `sex` and `learner_group` proportionally scaled.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Bar plot of the variable &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;learner_group&lt;/code&gt; proportionally scaled.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From this visualization it appears that there are more females in the first and last learner groups.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Two categorical independent variables&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let’s look at a more complex case in which we have two categorical independent variables. Now the dataset, as is, does not have a third categorical variable for us to explore but we can recast the continuous &lt;code&gt;tokens&lt;/code&gt; variable as a categorical variable if we bin the scores into groups. I’ve binned &lt;code&gt;tokens&lt;/code&gt; into three score groups with equal ranges in a new variable called &lt;code&gt;token_bins&lt;/code&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:token-bins&#34;&gt;Table 4: &lt;/span&gt;&lt;code&gt;belc&lt;/code&gt; dataset with categorical &lt;code&gt;token_bins&lt;/code&gt; variable added.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;participant_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;sex&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;learner_group&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;age&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;tokens&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;token_bins&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;120&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;78&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;43&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;80&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;02&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;female&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;26&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;low&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;male&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;73&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;mid&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Adding a second categorical independent variable ups the complexity of our analysis and as a result our visualization strategy will change. As text our data will include individual two-way cross-tabulations for each of the levels for the third variable. In this case it is often best to use the variable with the fewest levels as the third variable.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;, , sex = female

             token_bins
learner_group low mid high
            1  18   2    0
            2  18   6    0
            3   9   4    0
            4   7   2    1

, , sex = male

             token_bins
learner_group low mid high
            1  13   2    0
            2  15   7    1
            3   8   5    0
            4   2   3    0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To graphically visualize three categorical variables we turn to a mosaic plot.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-cat-ind-cat-2-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-cat-ind-cat-2-graph-1.png&#34; alt=&#34;Mosaic plot contrasting the categorical variables `sex`, `learner_group`, and `token_bins`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Mosaic plot contrasting the categorical variables &lt;code&gt;sex&lt;/code&gt;, &lt;code&gt;learner_group&lt;/code&gt;, and &lt;code&gt;token_bins&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From these visualizations we can see there is a general trend for the tokens from writing samples to increase in higher learner groups. There are some apparent divergent scores from this trend to be cautious of, however.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Continuous dependent variable&lt;/strong&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;No independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Working with a single continuous dependent variable means that the only practical way to summarize the data is graphically –as textual visualization will be very verbose and by and large uninterpretable. Plotting a single continuous variable often takes the form of a histogram which summarizes the frequency of the values of the dependent variable. So from our dataset, we may want to know what the distribution of &lt;code&gt;token&lt;/code&gt; scores looks like. That is, are they normally distributed (‘bell-shaped’), or skewed to the left or right (more values in the low or high range), or some other type of distribution (i.e. bi-modal, etc.)?&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-graph-1.png&#34; alt=&#34;Histogram (with and without a density line) for the continuous variable `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Histogram (with and without a density line) for the continuous variable &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The plot on the left is a standard histogram and the plot on the right is the same histogram with a density line added to highlight the distribution. From these plots we see that token counts are slightly left skewed. The longer tail to the right for higher token scores shows some evidence of outliers –that is, scores that are uncharacteristic of the general data distribution. For many analyses plotting a histogram is a key first step to identifying they type of statistical test to use on the data as certain test have assumptions about how the data should be distributed for their results to be reliable. For example, a class of tests called &lt;em&gt;parametric&lt;/em&gt; assume that continuous data is normally distributed (&lt;em&gt;non-parametric&lt;/em&gt; tests do not make this assumption). Having plotted the data we can see that it is probably not normally distributed. All is not lost, however. There are methods for &lt;em&gt;transforming&lt;/em&gt; the data that can often times take mildly skewed data and coerce it into a normal distribution.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One categorical independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Another common data analysis scenario is one in which we have a continuous dependent variable and one categorical independent variable. Say we wanted to know the average number of &lt;code&gt;tokens&lt;/code&gt; used by men and by women. We would use the &lt;code&gt;tokens&lt;/code&gt; variable as our dependent variable and and &lt;code&gt;sex&lt;/code&gt; as the independent variable. We can visualize these means textually, as we are calculating the mean for each level of &lt;code&gt;sex&lt;/code&gt;,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  female     male 
57.14925 60.80357 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or graphically with a box plot, in which we get a host of information about the distribution.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-cat-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-cat-graph-1.png&#34; alt=&#34;Box plot summarizing the contrast between `sex` and `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: Box plot summarizing the contrast between &lt;code&gt;sex&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you take a look at the results in the box plot you will see that the medians (the &lt;strong&gt;bold horizontal lines&lt;/strong&gt;) are not that different. Note that the mean and the median measure different things. And the mean is more sensitive to outliers –and the plot shows that there are some outliers in the male and female data. When we statistically analyze the data these types of outliers contribute to unexplained variation, or ‘noise’ at it is often called. Noise has the effect of reducing our confidence that the differences between populations are real, and not likely due to chance. We see much more on this later in the series.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;One continuous independent variable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The behavior of two continuous variables, one dependent and the other independent is represented using a scatter plot. The value for each variable for each observation is plotted as a coordinate pair. From this mapping we evaluate the relationship, or correlation, between the variables. Sticking with &lt;code&gt;tokens&lt;/code&gt; as our measure, let’s explore the extent to which &lt;code&gt;age&lt;/code&gt; conditions the number of tokens used by a participant.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens`.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From a visual inspection it appears that there is a slight effect for &lt;code&gt;age&lt;/code&gt; on &lt;code&gt;tokens&lt;/code&gt;, namely that the older the participant the more tokens they tend to use.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; It is often helpful, and/ or necessary to add a trend line to the plot to help see the relationship more clearly. Note that the ribbon (in grey) surrounding the trend line is the ‘standard error’, or SE. The SE is a confidence interval suggesting that the trend line could have been drawn within any part of this space. You will note that a larger ribbon width corresponds with more variability. This is clearly the case for the tokens used by participants at age 14.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-2-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` with a trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; with a trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Another note on the trend line. The trend line drawn here is a non-linear, which is why you see the line is bendy. Often times when we are doing hypothesis testing we will be making the assumption that the relationship between two or more variables is linear, not non-linear. We can add a linear trend line to get a better understanding of the linear relationship between our variables.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-3-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` with a linear trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; with a linear trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Interpreting the linear representation we see there is an apparent trend for increasing values for &lt;code&gt;tokens&lt;/code&gt; as &lt;code&gt;age&lt;/code&gt; increases.&lt;/p&gt;
&lt;p&gt;Say our aim was to understand the relationship between &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; as a potential function of &lt;code&gt;sex&lt;/code&gt;. We can incorporate &lt;code&gt;age&lt;/code&gt; as a categorical variable. The result provides us a scatter plot with two trend lines, one for each level of &lt;code&gt;sex&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:dep-con-ind-con-graph-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-09-15-introduction-to-statistical-thinking_files/figure-html/dep-con-ind-con-graph-4-1.png&#34; alt=&#34;Scatter plot visualizing the correlation between the continuous variables `age` and `tokens` by `sex` with a linear trend line.&#34; width=&#34;80%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: Scatter plot visualizing the correlation between the continuous variables &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;tokens&lt;/code&gt; by &lt;code&gt;sex&lt;/code&gt; with a linear trend line.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;From this visualization we see there is an apparent difference between males and females. Namely, males appear to increase their token production more than females of the same age. The SE ribbons here, however, are telling. Since they overlap we should be very cautious in interpreting the difference the trend line shows. Overlapping SE ribbons suggest the trend line could have been drawn in this space and therefore there is a likely probability that the visual difference will not result in a statistical difference. Again, this is another example of why visualization is such an integral first step in data analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;statistical-tests&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Statistical tests&lt;/h3&gt;
&lt;p&gt;In the previous section various visualization strategies were illustrated through the lens of typical data analysis scenarios. To gain confidence that the trends in the data that we observe are reliable we submit the data to statistical tests. There are numerous tests available, too many to discuss at this point. But it is important to understand that much like our visualization choices, the test we choose depends on the number of variables we are investigating and the information values of these variables. However, particular statistical tests also potentially require a number of test specific assumptions (such as whether the data is normally distributed, for example). We will cover these on a case by case basis later on in the series.&lt;/p&gt;
&lt;!-- * Goal is to reduce the data into a summary of the data that best approximates the nature of the phenomenon that we are investigating.  --&gt;
&lt;!--   - To do this we start with visualization to gain an intuitive understanding of the distribution of the data and variables of interest. This exploratory process should be guided by your research question and statistical aims (hypothesis in hypothesis testing). It is fundamental to build up this graphic and mental picture of our data before moving to statistically evaluate the relationships of interest. --&gt;
&lt;!--   - (Examples) --&gt;
&lt;!--   - With an intuition of the data, awareness of the informational values of our variables, and having identified dependent and independent variables, we proceed to statistical analysis to develop a model that achieves these goals. Ultimately we want to know if the nature of the connections between our variables are reliable. That&#39;s what statistics will do for us. --&gt;
&lt;!--   - (Examples) --&gt;
&lt;!-- Keep these examples brief, as we will deal with data analysis head on later in the series.  --&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we covered some foundational topics for any data analysis project. Guiding the entire process is a clear research question. From there we can proceed to acquire data that is relevant and reliable on the phenomenon we hope to understand better, choose a statistical approach which matches our analysis goals, and organize our data into a format conducive for achieving these goals. Only at this point can we confidently move to analyzing our data, first beginning with appropriate visualization techniques to get a feel for the trends and then moving to performing the relevant statistical tests to provide confidence that the trends captured in our visualizations are in fact reliable.&lt;/p&gt;
&lt;p&gt;There is still much left to discuss, in particular what statistical tests to apply to a given data analysis scenario and the assumptions behind statistical tests. We will address these topics later in the series when we have established a stronger and practical understanding of the preceding project steps and increase our proficiency programming in R. The next steps in this journey will be to learn about data types and sources and the specifics of acquiring data for language research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- OTHER APPROACHES TO THE TOPIC

GRIES, Statistics for Linguists with R
CHAPTER 1

* Design and logic of quantitative studies
  - Linguistics is an empirical science; thus understanding empirical methods and statistics is key to undertanding scientific and linguistic argumentation
  - Choosing a topic -consult the literature and get a feel for the area
* Reseach aims 
  - Hypotheses
  - Exploratory analysis
  - (Prediction -not covered but would logically fall here)
* Variables and measures
  - Operationalizing variables
  - Informational value (level of measurement)
  - Dependent and independent variables
    * Measure of dependent variable
    * Number of independent variables
* Data
  - Populations and samples (aiming for a balanced, representative sample)
  - Format for analysis (tidy, basically)


WICKHAM, R for Data Science
EXPLORE

* Data science workflow (overview)
* Data visualization (ggplot2, grammar of graphics fundamentals)
* Exploratory data analysis (EDA)
  - Variation
  - Covariation
  - Patterns and models (modelling concepts)

MODEL

* Hypothesis generating (exploration) vs. hypothesis confirmation (testing/ inference)
  - Observations may be used only once for inference, but multiple times for exploration
  - Modelling: creating a low-dimension summary of the dataset
  - Choosing a model (model selection)

JOHNSON, Quantitative Methods in Linguistics

Chapter 1:

* Goals of quantitative analyses
  - Data reduction
  - Inference (confirm a hypothesized relationship)
  - Exploration (discover relationships)
  - Explore processes (? not sure I like this; I would add Prediction (leverage/ harness relationships for predicting events) )
* Observations
  - What is an observation?
  - Informational values of observations (variables)
* Measures
  - Frequency distributions
  - Shapes of distributions
    * Uniform, skewed (right or left), bimodal, normal, J-shaped, U-shaped
  - Importance of the normal distribution
    * What is the normal distribution and why is it important?
      * Assumption for many statistical tests
    * How do we know if the data is normal?
    * Transforming data to fit the normal distribution
  - Measures of central tendency
    * Mean (arithmatic), median, and mode
  - Measure of dispersion
    * Variance
    * Standard deviation

Chapter 2:

* Sampling
  - What makes a good sample?
* Data
  - Observations
  - Good samples are critical to a sound analysis (your findings cannot be more reliable than your data!)
* Hypothesis testing
  - Central Limit Theorem
  - Null and alternative hypotheses
  - Type I and Type II error
  - Correlation
    
BAAYEN, Analyzing linguistic data: A practical introduction to statistics using R

* Not (obviously) a good source for intro statistical thinking

STANTON, An Introduction to Data Science

Data Science: Many Skills
* 

GRIES, Quantitative Corpus Linguistics with R

Chapter 5: Some statistics for Corpus Linguistics

* Intro to statistical thinking
  - Variables and their roles in an analysis
  - Variables and their informational value
  - Hypotheses: formulation and operationalization
  - Data analysis
* Categorical dependent variables
  - 1 DV, no IV
  - 1 DV, 1 IV (categorical)
  - 1 DV, 2+ IV (categorical)
* Interval/ ratio-scaled dependent variables
  - 1 DV, no IV
  - 1 DV, 1 IV (categorical)
  - 1 DV, 1 IV (interval/ratio)
  - 1 DV, 2+ IV (mixed)

--&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Baayen2004&#34;&gt;
&lt;p&gt;Baayen, R. Harald. 2004. “Statistics in Psycholinguistics: A critique of some current gold standards.” &lt;em&gt;Mental Lexicon Working Papers&lt;/em&gt; 1 (1):1–47.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Gries2013a&#34;&gt;
&lt;p&gt;Gries, ST. 2013. &lt;em&gt;Statistics for Linguistics with R. A Practical Introduction&lt;/em&gt;. 2nd revise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Johnson:2008&#34;&gt;
&lt;p&gt;Johnson, K. 2008. &lt;em&gt;Quantitative methods in linguistics&lt;/em&gt;. Blackwell Pub.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Munoz2006&#34;&gt;
&lt;p&gt;Muñoz, Carme, ed. 2006. &lt;em&gt;Age and the rate of foreign language learning&lt;/em&gt;. Clevedon: Multilingual Matters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wickham2017&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Garrett Grolemund. 2017. &lt;em&gt;R for Data Science&lt;/em&gt;. First edit. O’Reilly Media. &lt;a href=&#34;http://r4ds.had.co.nz/&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;The &lt;a href=&#34;http://talkbank.org/&#34;&gt;HomeBank&lt;/a&gt; section of TalkBank is restricted to members only. Membership is free but you will need to contact the repository and request membership.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that I’ve added a ‘jitter’ to the data points in this scatter plot to avoid overplotting.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project management for scalable data analysis</title>
      <link>https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/</link>
      <pubDate>Thu, 31 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/08/31/project-management-for-scalable-data-analysis/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;!-- TODO:
- Consider setting up projects as packages. One advantage would be to be able to document the data sources with data defintions. There may be other advantages, as well as complications associated with this type of approach that I&#39;m not thinking about right now. Worth a look at least.
- 
--&gt;
&lt;div id=&#34;project-management&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Project management&lt;/h2&gt;
&lt;p&gt;This post can really be seen as an extension of the last post &lt;a href=&#34;https://francojc.github.io/2017/08/14/getting-started-with-r-and-rstudio/&#34;&gt;Getting started with R and RStudio&lt;/a&gt; in that we will be getting to know some more advanced, but indispensable features of RStudio. These features, in combination with some organizational and programming strategies, will enable us to conduct efficient data analysis and set the stage for research is is both scalable and ready for sharing with either collaborators or the research community.&lt;/p&gt;
&lt;p&gt;To understand the value of this approach to project management we need to get a bird’s eye view of the key steps in a data science project. There are three main areas that any research project includes: &lt;strong&gt;data organization&lt;/strong&gt;, &lt;strong&gt;data analysis&lt;/strong&gt;, and &lt;strong&gt;reporting results&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;These three main areas have important subareas as well:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Data organization:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;acquiring data&lt;/em&gt; whether that be from a database or through a download or webscrape&lt;/li&gt;
&lt;li&gt;&lt;em&gt;curating data&lt;/em&gt; so that it will be reliable for subsequent steps in the process&lt;/li&gt;
&lt;li&gt;&lt;em&gt;transforming the data&lt;/em&gt; into a format that will facilitate the analysis of the data including the generation of new variables and measures.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Data analysis:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;visualizing data&lt;/em&gt; in summary tables and graphics to gain a better sense of the distribution of the question that we are aiming to learn about.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;statistical tests&lt;/em&gt; to provide confirmation of the distribution(s) that we are investigating and/or a more robust understanding of the relationships between variables in our data&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Reporting results:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Communicating findings&lt;/em&gt; in an appropriate format for the venue. This can be standard article format, or slides, or as a webpage.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Preparing reproducible results&lt;/em&gt; by ensuring that our work is well documented and capable of being replicated&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By taking these steps into account in the organization of our project, we will be able to work more efficiently and effectively with R and RStudio. In the next section we will get set up with a model template for organizing a data science project. This structure will serve as our base for working in subsequent posts in this series.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;project-structure&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Project structure&lt;/h2&gt;
&lt;p&gt;As a starting point, we will download an existing project that I have created. This project will work as a template for implementing good project management with RStudio. The project template is housed remotely on the code sharing platform &lt;a href=&#34;https://github.com/&#34;&gt;GitHub&lt;/a&gt; which leverages the &lt;a href=&#34;https://git-scm.com/&#34;&gt;&lt;code&gt;git&lt;/code&gt; versioning software&lt;/a&gt; to make local projects remotely accessible. After copying the project to your local machine, we will link RStudio to the project and continue our exploration of the various features available in this software to manage projects.&lt;/p&gt;
&lt;div id=&#34;downloading-the-template&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Downloading the template&lt;/h3&gt;
&lt;p&gt;To download this project template, we will need to set up &lt;code&gt;git&lt;/code&gt; on your machine first. I will have much more to say about &lt;code&gt;git&lt;/code&gt; and GitHub in a later post in the series that will directly concern the process of versioning and sharing reproducible research, but for now we’ll only cover what is needed to get our project template from GitHub. If you are on a Mac, &lt;code&gt;git&lt;/code&gt; is most likely already on your machine. To verify that this is the case you can open up the ‘Terminal.app’ on your machine and type this command at the prompt:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;which git&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;The ‘Terminal.app’ on Mac, ‘Terminal’ on Linux, and ‘git bash’ on Windows all interface what is called the ‘Shell’, or &lt;a href=&#34;https://en.wikipedia.org/wiki/Command-line_interface&#34;&gt;command-line interface&lt;/a&gt;. This is a interface to your computer not unlike the Console is to R in the R GUI Application or RStudio. There are various environments with particular syntax conventions for working with your computer through this interface, the most common being the ‘Bash’ shell. I encourage you to learn some the &lt;a href=&#34;https://www.davidbaumgold.com/tutorials/command-line/&#34;&gt;basics of using the command-line&lt;/a&gt;.&lt;br /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;If &lt;code&gt;git&lt;/code&gt; is already installed, then a path to this software, similar to the one below, will be returned.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;/usr/local/bin/git&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If a path is not returned, or you are on a PC, then you will need to download and install the software from the &lt;code&gt;git&lt;/code&gt; homepage. Follow this &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;link to the &lt;code&gt;git&lt;/code&gt; downloads page&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:git-install-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-31-project-management-for-scalable-data-analysis_files/figure-html/git-install-1-1.png&#34; alt=&#34;Git downloads page.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Git downloads page.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Download the installer for you operating system and run this installer with the default installation recommendations.&lt;/p&gt;
&lt;p&gt;Now that you have &lt;code&gt;git&lt;/code&gt; on your machine, let’s set some global options that will personalize your software while since we are already working at the command line. If you are on a Mac or Linux, open the ‘Terminal’. If you are on a Windows machine, navigate to the programs menu and open ‘git bash’.&lt;/p&gt;
&lt;p&gt;At the terminal, enter the following commands –replacing ‘Your Name’ and ‘your@email.com’ with your information.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git config --global user.name &amp;#39;Your Name&amp;#39;
git config --global user.email &amp;#39;your@email.com&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s download the project, ‘recipes-project_template’ from &lt;a href=&#34;https://github.com/francojc?tab=repositories&#34;&gt;my personal GitHub repository&lt;/a&gt;. To do this, you will want to first decide where on your machine you will like to store this project. If you are following the Recipe series, I recommend that you create a new directory called &lt;code&gt;Recipes/&lt;/code&gt; somewhere convenient on your hard disk. You can then use this directory to house this and other upcoming projects associated with posts in this series.&lt;/p&gt;
&lt;p&gt;To create this directory, I’ll use the &lt;code&gt;mkdir&lt;/code&gt; command, or ‘make directory’. The &lt;code&gt;~&lt;/code&gt; is a shortcut operator for the current users home directory. On my Mac the full path would be &lt;code&gt;/Users/francojc/Documents/Recipes/&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;mkdir ~/Documents/Recipes/&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Quick tip: When typing a path at the command line you can start typing a directory name and hit the &lt;code&gt;tab&lt;/code&gt; key on your keyboard to autofill the full name. If you hit &lt;code&gt;tab&lt;/code&gt; twice in a row, the &lt;code&gt;bash&lt;/code&gt; shell will list the available subdirectory paths. This can speed up navigation at the command line and help avoid typographical errors.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Once we have created the main directory &lt;code&gt;Recipes/&lt;/code&gt; to house the repository, we need to navigate to that directory using &lt;code&gt;cd&lt;/code&gt;, or ‘change directory’.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd ~/Documents/Recipes/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Verify that your current working directory is correct by entering &lt;code&gt;pwd&lt;/code&gt;, or ‘path to working directory’ to get the current directory’s path.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;pwd&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The result should print the path to your &lt;code&gt;Recipes/&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;Now we are ready to use &lt;code&gt;git&lt;/code&gt; to copy the remote project from my GitHub repository &lt;code&gt;recipes-project_template&lt;/code&gt; to our current directory. Enter the following command at the terminal prompt.&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git clone https://github.com/francojc/recipes-project_template.git&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should get some information about the download, or clone, that looks something similar to the output below.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;Cloning into &amp;#39;recipes-project_template&amp;#39;...
remote: Counting objects: 48, done.
remote: Compressing objects: 100% (27/27), done.
remote: Total 48 (delta 21), reused 38 (delta 15), pack-reused 0
Unpacking objects: 100% (48/48), done.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now &lt;code&gt;cd&lt;/code&gt; into the &lt;code&gt;recipes-project_template/&lt;/code&gt; directory that you just cloned into your &lt;code&gt;Recipes/&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;You can now inspect the new directory, subdirectories, and files that now reside in the &lt;code&gt;recipes-project_template/&lt;/code&gt; directory with either at the command line, or with your operating systems file explorer. At the command line you can use the &lt;code&gt;ls&lt;/code&gt;, or ‘list structure’ command like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ls&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should see the following output.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;README.md   code        figures     log
_pipeline.R data        functions   report&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now leave the Terminal, or git bash, and return to RStudio. We are now ready to link an R project to our cloned project.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-an-r-project-within-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating an R Project within RStudio&lt;/h3&gt;
&lt;p&gt;As we have seen, RStudio provides a of host tools for facilitating work with R. A feature that makes working with the sometimes numerous data files, scripts, and other resources more manageable is the ‘R Project’ tool. In a nutshell, this RStudio tool allows us to select a directory where our project files live and effectively group these files and the work we do as a unit. At a basic level it simply helps manage individual projects more easily. As we move on to other posts in this series, and particularly when we discuss creating reproducible research, we will see that this feature will really prove its worth. For now, let’s make the project template an R project and turn to focus on the file and directory structure as it relates to doing efficient and reproducible data analysis in R.&lt;/p&gt;
&lt;p&gt;To link our template to an R Project, start up R and select the ‘New Project…’ dialogue from the RStudio toolbar menu. You will be presented with various options as seen in Figure &lt;a href=&#34;#fig:r-project-1&#34;&gt;2&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-project-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-31-project-management-for-scalable-data-analysis_files/figure-html/r-project-1-1.png&#34; alt=&#34;Options for creating an R Project in RStudio.&#34; width=&#34;400&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Options for creating an R Project in RStudio.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The first option is for starting a project from scratch. The last option is for cloning a project from a versioning repository like GitHub. With versioning software, like &lt;code&gt;git&lt;/code&gt;, on our machine, we can clone and create an R Project in one step. We will make use of this option in future posts now that we have a basic understand of &lt;code&gt;git&lt;/code&gt; and GitHub. For now, however, we want to link the project template we cloned manually using the command line, so select ‘Existing Directory’ from this menu.&lt;/p&gt;
&lt;p&gt;Next navigate to the directory which we cloned either typing the path to the directory, or more conveniently using the ‘Browse’ button. Once we have selected the directory and create the R Project, RStudio will open a new session with our directories and files listed in the Files pane.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-project-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-31-project-management-for-scalable-data-analysis_files/figure-html/r-project-2-1.png&#34; alt=&#34;View of the project template as an R Project.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: View of the project template as an R Project.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You will notice that RStudio has created a file named &lt;code&gt;recipes-project_template.Rproj&lt;/code&gt;. From now on you can navigate to this file using your operating system’s file explorer and open it to return working on this project. Your workspace settings, history, environment variables, etc. will be restored to the last time you were working on the project –more on this later.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;saffolding-for-a-scalable-project&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Saffolding for a scalable project&lt;/h2&gt;
&lt;p&gt;Now let’s turn to the files and directories of our project template and discuss how this structure is associated to the steps listed earlier to conduct a data science project. Below you will see the complete structure of the template.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;├── README.md
├── _pipeline.R
├── code/
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data/
│   ├── derived/
│   └── original/
├── figures/
├── functions/
├── log/
├── recipes-project_template.Rproj
└── report/
    ├── article.Rmd
    ├── bibliography.bib
    ├── slides.Rmd
    └── web.Rmd&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;directories&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Directories&lt;/h3&gt;
&lt;p&gt;This template includes directories for data (&lt;code&gt;data&lt;/code&gt;), code (&lt;code&gt;code&lt;/code&gt;), and communicating findings (&lt;code&gt;report&lt;/code&gt;). These directories are core to your project and where the heavy lifting takes place. The &lt;code&gt;data&lt;/code&gt; and &lt;code&gt;report&lt;/code&gt; directories have important subdirectories that separate key stages in your analysis. &lt;code&gt;data/original&lt;/code&gt; is where the data in its raw form will be stored and &lt;code&gt;data/derived&lt;/code&gt; is where any changes you make to the data for the particular analysis are stored. This distinction is an important one; to safeguard our analysis and to ensure that our analysis is reproducible we do not want to make changes to the original data that are not reflected in the project itself. The subdirectories of &lt;code&gt;report&lt;/code&gt; separate the potential formats that we may use to communicate insight generated from this analysis.&lt;/p&gt;
&lt;p&gt;Before moving on to discuss the files included in the template, let’s discuss the other three supporting directories: &lt;code&gt;figures&lt;/code&gt;, &lt;code&gt;log&lt;/code&gt;, and &lt;code&gt;functions&lt;/code&gt;. You will most likely generate figures in the course of the analysis. Grouping them together in the &lt;code&gt;figures&lt;/code&gt; directory enables us to quickly reference them visually and also include them in any one or all of the reports that may be generated. The &lt;code&gt;log&lt;/code&gt; directory is a convenient and easily identifiable place to document meta aspects of your analysis that will may not picture in your reports. Finally, a directory for housing custom functions you may write to facilitate particular stages of the analysis is provided, &lt;code&gt;functions&lt;/code&gt;. We will soon see how powerful and indispensable custom functions are but for now just let me say that keeping them organized in a separate directory will enhance the legibility of your code and help you take full advantage of their power.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;files&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Files&lt;/h3&gt;
&lt;p&gt;This template also includes various file templates that are associated with the tasks typically performed in a data analysis project. The R scripts in the &lt;code&gt;code/&lt;/code&gt; directory are script templates to carry out the sub-tasks of our three main project steps: organize data; get the original data (&lt;code&gt;acquire_data.R&lt;/code&gt;), clean and prepare key features of the data (&lt;code&gt;curate_data.R&lt;/code&gt;), manipulate the data creating the needed variables and structure for the data analysis (&lt;code&gt;transform_data.R&lt;/code&gt;), data analysis; visualize and perform statistical analyses on the data (&lt;code&gt;analyze_data.R&lt;/code&gt;), and communicating findings; report results in appropriate formats (&lt;code&gt;generate_reports.R&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Each of these R scripts has a common structure which is outlined using code commenting. Take a look at the structure of the &lt;code&gt;acquire_data.R&lt;/code&gt; script, copied below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ABOUT -----------------------------------------------------------

# Description: &amp;lt;The aim of this script&amp;gt;
# Usage: &amp;lt;How to run this script: what input it requires and output produced&amp;gt;
# Author: &amp;lt;Your name&amp;gt;
# Date: &amp;lt;current date&amp;gt;

# SETUP -----------------------------------------------------------

# Script-specific options or packages

# RUN -------------------------------------------------------------

# Steps involved in acquiring and organizing the original data

# LOG -------------------------------------------------------------

# Any descriptives that will be helpful to understand the results of this
# script and how it contributes to the aims of the project

# CLEAN UP --------------------------------------------------------

# Remove all current environment variables
rm(list = ls())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The template shown here leverages code commenting to separate the script into meaningful tasks. The &lt;code&gt;ABOUT&lt;/code&gt; section is where you will provide an overview of the purpose of the script in your project, how to use it, who created it, and when it was created. The &lt;code&gt;SETUP&lt;/code&gt; section provides a space to load any required packages, source any required custom functions, and configure various other options. &lt;code&gt;RUN&lt;/code&gt; is where the bulk of your code will be entered. As it has been stated various times, commenting is a vital part of sound coding practices. It is often helpful not only to comment the particular line of code, which we do by adding the &lt;code&gt;#&lt;/code&gt; symbol to the immediate right of the code and then describe the task, but also to group coding sub-tasks in this section. RStudio provides a tool to create comment sections. You can use this tool by selecting ‘Code &amp;gt; Insert Section…’ or the keyboard shortcut &lt;code&gt;shift + command + R&lt;/code&gt; (Mac) or &lt;code&gt;shift + ctrl + R&lt;/code&gt; (PC). Either approach will trigger a dialogue box to enter the name of the section. Once you have entered the name it will then appear in the section listing, as seen in Figure &lt;a href=&#34;#fig:r-project-3&#34;&gt;4&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-project-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-31-project-management-for-scalable-data-analysis_files/figure-html/r-project-3-1.png&#34; alt=&#34;View of the section listing in RStudio.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: View of the section listing in RStudio.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You can use this listing to skip from section to section which can be very helpful as your script becomes more complicated with subsequent code.&lt;/p&gt;
&lt;p&gt;The last two sections &lt;code&gt;LOG&lt;/code&gt; and &lt;code&gt;CLEANUP&lt;/code&gt; are good-housekeeping sections. &lt;code&gt;LOG&lt;/code&gt; is where you can divert any meta-information about the analysis to the &lt;code&gt;log/&lt;/code&gt; directory. This is an important step to take at this point as the last section, &lt;code&gt;CLEANUP&lt;/code&gt;, is where you will remove any of the objects your script has created with the &lt;code&gt;rm(list = ls()&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;Although removing objects created is not strictly required, it has two key benefits: it will free up any memory that these objects claim from our R session and it helps keep each script as modular as possible. Freeing up memory after an object is no longer needed is good practice as memory handling in R is not one of the language’s strong points. Striving for modularity speaks more to reproducibility and analysis workflow. If a subsequent step relies on an object generated by a previous script that is held in memory, we must run the previous script in the workflow each time before the next. As the number of tasks increases and as these tasks become more processing intensive, it will lead to an unnecessary loss of computing resources and time. To avoid this scenario, each script in our analysis should only require input that is read from an session-external source; that is from a resource online or from the hard disk. This means that if an object created in a script will be needed a some point in our analysis, it should be written to disk –preferably in a plain-text version.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The last of the three main steps in a data analysis project, ‘Reporting results’, is associated with the file &lt;code&gt;generate_reports.R&lt;/code&gt; in the &lt;code&gt;code/&lt;/code&gt; directory which is tied to the various files in the &lt;code&gt;report/&lt;/code&gt; directory. These later files have the extension &lt;code&gt;.Rmd&lt;/code&gt;, not &lt;code&gt;.R&lt;/code&gt;. This distinction reflects the fact that these files are a special type of R script: an &lt;a href=&#34;http://rmarkdown.rstudio.com/&#34;&gt;RMarkdown script&lt;/a&gt;. RMarkdown is a variant of the popular markup language &lt;a href=&#34;https://en.wikipedia.org/wiki/Markdown&#34;&gt;Markdown&lt;/a&gt;. RMarkdown goes beyond standard Markdown documents in that they allow for the intermingling of code, prose, and graphics to dynamically create reports in &lt;a href=&#34;http://rmarkdown.rstudio.com/pdf_document_format.html&#34;&gt;PDF document format&lt;/a&gt; (&lt;code&gt;report/article/report.Rmd&lt;/code&gt;), &lt;a href=&#34;http://rmarkdown.rstudio.com/ioslides_presentation_format.html&#34;&gt;presentation slides&lt;/a&gt; (&lt;code&gt;report/slides/presentation.Rmd&lt;/code&gt;), and &lt;a href=&#34;http://rmarkdown.rstudio.com/html_document_format.html&#34;&gt;interactive web pages&lt;/a&gt; (&lt;code&gt;report/web/webpage.Rmd&lt;/code&gt;). Data and figures generated in by the R scripts in your analysis can be included in these documents along with citations and a corresponding bibliography sourced from a Bibtex file &lt;code&gt;report/bibliography.bib&lt;/code&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Together these features a provide powerful tool belt for creating publication quality reports. The &lt;code&gt;generate_reports.R&lt;/code&gt; file simply runs the commands to render these files in their specific formats.&lt;/p&gt;
&lt;p&gt;I have provided rough outlines for each of these RMarkdown output formats. We will explore the details of creating reports later in the series. But for now, I encourage you to browse the &lt;a href=&#34;http://rmarkdown.rstudio.com/gallery.html&#34;&gt;RMarkdown gallery&lt;/a&gt; and explore the &lt;a href=&#34;http://rmarkdown.rstudio.com/articles.html&#34;&gt;documentation&lt;/a&gt; to get a sense of what RMarkdown can do.&lt;/p&gt;
&lt;p&gt;The last two files in this template are the &lt;code&gt;_pipeline.R&lt;/code&gt; script and the &lt;code&gt;README.md&lt;/code&gt; document. &lt;code&gt;README.md&lt;/code&gt; is the file used to describe the project in general terms including the purpose of the project, data sources, analysis methods, and other relevant information to help clarify how to reproduce the research with these project files. The &lt;code&gt;README&lt;/code&gt; file may or may not include the &lt;code&gt;.md&lt;/code&gt; extension. If it does, as in the example in this template, you will have access to the Markdown syntax options to provide word processing style formatting, if needed. If you end up storing you project on a code repository site, such as GitHub, this file will be rendered as a web document and be used as the introduction to your project.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;_pipeline.R&lt;/code&gt; script is the master script for your analysis. It is a standard R script and includes the same internal commenting sections as the other &lt;code&gt;.R&lt;/code&gt; scripts in the the &lt;code&gt;code/&lt;/code&gt; directory (i.e. &lt;code&gt;ABOUT&lt;/code&gt;, &lt;code&gt;SETUP&lt;/code&gt;, &lt;code&gt;RUN&lt;/code&gt;, &lt;code&gt;LOG&lt;/code&gt;, and &lt;code&gt;CLEANUP&lt;/code&gt;). This script, however, allows you to run the entire project from data to report in sequential order. In the &lt;code&gt;RUN&lt;/code&gt; section you will find sub-steps which call the &lt;code&gt;source()&lt;/code&gt; function on each of our processing scripts. Since each script representing a step in our analysis is modular, only the required input is read and output generated for each script. As logging step-specific information is taken care of in each particular script, the &lt;code&gt;LOG&lt;/code&gt; section in the &lt;code&gt;_pipeline.R&lt;/code&gt; script will most typically only include a call to the &lt;code&gt;sessionInfo()&lt;/code&gt; function which reports details on the operating system and the packages and the versions of the packages used in the analysis. This information is vital for reproducing research as it documents the specific conditions that successfully generated the analysis.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Quick note: there is nothing special about the names of the files in the template. You can edit and modify these file names as you see fit. You should, however, take note of good file naming practices. Names should be descriptive and short. Whitespace is traditionally avoided, but is not explicitly required. I have employed ‘snake case’ here by using an underscore (&lt;code&gt;_&lt;/code&gt;) to mark spaces in file names. There are various style conventions used to avoid whitespace and for other coding practices. I recommend following the suggestions provided by Hadley Wickham in his book &lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34;&gt;‘Advanced R’&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(Wickham 2014)&lt;/span&gt;. Whatever style you choose, the most important thing is to be consistent.&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;div id=&#34;r-project-sessions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R Project sessions&lt;/h3&gt;
&lt;p&gt;Once the files and directories are linked to an R Project your workspace settings, command history, and objects can be saved at any point and restored to continue working on the analysis. To see this in action, let’s do a little work with this project template. Let’s run the &lt;code&gt;_pipeline.R&lt;/code&gt; file. There isn’t much to our analysis at this point, as it is just boilerplate material for the most part, but it will serve to highlight how to work with an R Project session. To run this file, open it from the Files pane. It will appear in the Editor pane where we can now use the keyboard shortcut &lt;code&gt;option + command + R&lt;/code&gt; (Mac) or &lt;code&gt;alt + ctrl + R&lt;/code&gt; (PC) to run the entire master script.&lt;/p&gt;
&lt;p&gt;Once you have run the &lt;code&gt;_pipeline.R&lt;/code&gt; script, some new files will appear in your directory structure, seen below.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;├── README.md
├── _pipeline.R
├── code/
│   ├── acquire_data.R
│   ├── analyze_data.R
│   ├── curate_data.R
│   ├── generate_reports.R
│   └── transform_data.R
├── data/
│   ├── derived
│   └── original
├── figures/
├── functions/
├── log/
│   └── session_info.txt
├── recipes-project_template.Rproj
└── report/
    ├── article.Rmd
    ├── article.html
    ├── bibliography.bib
    ├── slides.Rmd
    ├── slides.html
    ├── web.Rmd
    └── web.html&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The new files appear in the &lt;code&gt;log/&lt;/code&gt; and &lt;code&gt;report/&lt;/code&gt; directories. The &lt;code&gt;session_info.txt&lt;/code&gt; file is our log of the session information. The &lt;code&gt;article.html&lt;/code&gt;, &lt;code&gt;slides.html&lt;/code&gt;, and &lt;code&gt;web.html&lt;/code&gt; files are the rendered versions of the RMarkdown templates. If you browse to the History tab in the Environment pane you will see that we have one line in our history –the code that ran the &lt;code&gt;_pipeline.R&lt;/code&gt; file.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;The &lt;code&gt;article.Rmd&lt;/code&gt; file is set to render a web document by default in this template. If you want to render a PDF document, you will need to have a working Latex installation. For those who would like to set up PDF rendering of RMarkdown documents, here are &lt;a href=&#34;https://yihui.name/tinytex/&#34;&gt;instructions&lt;/a&gt; on how to set up Latex.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Let’s quit our R session now by closing RStudio. When prompted, choose ‘Save’ from the ‘Quit R session’ dialogue box. Now reopen our R project by either starting RStudio then choosing ‘File &amp;gt; Recent Projects’ in the RStudio toolbar or by navigating to the &lt;code&gt;recipes-project_template.Rproj&lt;/code&gt; file with your operating system’s file explorer and double-clicking it.&lt;/p&gt;
&lt;p&gt;Choosing to save our project before closing RStudio has the effect of taking a snapshot of the current workspace. The files that were open on closing the session are returned to the workspace. Any variables we had in memory and the command history are also returned. The details of this snapshot are stored in the files you will now find at the root of our project directory: &lt;code&gt;.RData&lt;/code&gt; and &lt;code&gt;.Rhistory&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-project-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-31-project-management-for-scalable-data-analysis_files/figure-html/r-project-4-1.png&#34; alt=&#34;View of the R project snapshot files `.RData` and `.Rhistory`.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: View of the R project snapshot files &lt;code&gt;.RData&lt;/code&gt; and &lt;code&gt;.Rhistory&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;You might be wondering why these files are prefixed with &lt;code&gt;.&lt;/code&gt;. Using a period before file names is not specific to RStudio. It is &lt;a href=&#34;https://en.wikipedia.org/wiki/Hidden_file_and_hidden_directory&#34;&gt;a convention used in programming to hide a file from system file explorers&lt;/a&gt;. These types of files are often used for configuration and application resources and are not meant to be edited by the average user.&lt;br /&gt;&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;If you choose not to save the workspace when quitting RStudio, these files will not be generated, if they do not already exist, or they will not be overwritten by the current session if they already exist.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;In this post we have discussed setting up and managing an R Project in RStudio. Along the way I provided a sample template for structuring your data analysis project based on the common steps in data science research. You have seen how this template is associated to each step and learned about some important conventions and guidelines for maintaining an efficient workflow. These principles are fundamental to creating an internally consistent and reproducible project.&lt;/p&gt;
&lt;p&gt;Later on in the series we will discuss project versioning and packaging to make a project fully reproducible for you and future collaborators. We will leave that discussion for now and turn our attention in the next post in the series which addresses one of the main conceptual underpinnings of quantitative research: statistical thinking.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;!-- Ideas for next posts:

* Introduction to statistical thinking
- What is data?
  * Observations
- Informational types
  * The nature of observation
  * Measurements
- Relationships
  * Establishing connections between observations
  * Is a connection reliable?
- Comparisons
  * Generalizing relationships between data collections with similar but unique observationsz

# Sub-task posts

... to working through each of the sub-tasks of the data science workflow focusing on typical contexts and applications for quantitative language and linguistic research. ...

* Acquiring data in R: various sources and types
  - Types of data
    * Curated to uncurated
  - Data sources
    * Locations
    - Files and file types
      * Files and file extensions
      * Plain text
      * Delimited files

* Curating data: cleaning, organizing, and annotating  

* Transforming data: preparing data for analysis  

* Visualization: exploring distributions in graphical and tabular form 

* Analysis:  

* Reporting
  - Creating reports in various formats with RMarkdown
  - Creating reproducible research: project versioning and packaging
    - Project backups, versioning, and sharing
      * git and Github


# Later...




# Book chapters

* Quantitative language research: why does it matter?
- Gains made across the board in quantitative research
- Approaching language research as a quantitative science
- Frequency matters (cognitive rationale for quantitative research)
- Case studies

--&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Wickham2014&#34;&gt;
&lt;p&gt;Wickham, H. 2014. &lt;em&gt;Advanced R&lt;/em&gt;. CRC Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Plain text files, in essence, are the &lt;em&gt;lingua franca&lt;/em&gt; of the computing world. They are the type of files that can be readily accessed through a plain-text editor such as ‘TextEdit’ (Mac) or ‘Notepad’ (PC). Importantly these files are not compiled by nor bound to any particular software, such as a document generated by Word or Excel. We will see how to write objects to disk as plain text files in subsequent posts.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;You can &lt;a href=&#34;http://www.bibtex.org/Using/&#34;&gt;generate your own Bibtex file&lt;/a&gt; or generate one using bibliographic management software such as &lt;a href=&#34;https://blog.mendeley.com/2011/10/25/howto-use-mendeley-to-create-citations-using-latex-and-bibtex/&#34;&gt;Mendeley&lt;/a&gt; or &lt;a href=&#34;http://libguides.mit.edu/c.php?g=176000&amp;amp;p=1159208&#34;&gt;Zotero&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting started with R and RStudio</title>
      <link>https://francojc.github.io/2017/08/14/getting-started-with-r-and-rstudio/</link>
      <pubDate>Mon, 14 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/08/14/getting-started-with-r-and-rstudio/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;why-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why R?&lt;/h2&gt;
&lt;p&gt;The R programming language is free software developed with an eye towards statistical computing and data visualization that has has taken off in popularity over the last decade and is now finds itself among the most used programming languages, in &lt;a href=&#34;http://spectrum.ieee.org/computing/software/the-2017-top-programming-languages&#34;&gt;general&lt;/a&gt; and is often the &lt;a href=&#34;https://medium.com/@MarutiTech/which-are-the-popular-languages-for-data-science-8e67fb5ef1ff&#34;&gt;go-to language for data science&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So what’s all the fuss about? Among the things that you will come to love about R, you will be hard pressed to find a more active community surrounding a programming language.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; The size and activity of this community means that R stays free, the software improves, and almost anything you are looking to do in your analysis has a user-developed ‘package’ &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; that facilitates getting any analysis done at various stages in the process. Packages are contributed code from R users that encapsulate a particular and cohesive set of ‘functions’, or sub-tasks; but we will get more into this later in this post and in detail later in the series. &lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The number of R packages grows quickly everyday and now applies to many more tasks that just statistics. To get a sense of this activity you can take a look at an interactive display of current downloads from the R package repository (or store) called &lt;a href=&#34;https://cloud.r-project.org/&#34;&gt;CRAN&lt;/a&gt; that lives on the R Project website.&lt;/p&gt;
&lt;iframe src=&#34;https://gallery.shinyapps.io/087-crandash/?showcase=0&#34; width=&#34;100%&#34; height=&#34;600&#34;&gt;
&lt;/iframe&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;This display, by the way, was created with an R package called &lt;a href=&#34;http://shiny.rstudio.com/&#34;&gt;Shiny&lt;/a&gt;. We’ll get to building interactive websites (like this one) and documents later on in the series.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;R, then, is a widely used open source programming language with a thriving community base that is among the best languages to do the robust and sophisticated analyses that are required for data science. This is an exciting time to be an R programmer and a data scientist, so let’s get started!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-and-installing-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Downloading and installing R&lt;/h2&gt;
&lt;p&gt;The beginning, of course, is downloading and installing the software. Follow this link to the R Project website: &lt;a href=&#34;https://www.r-project.org/&#34;&gt;Download R&lt;/a&gt;. This is the home of the R programming language. There are many resources you can find here including a current list of available R packages, resources for getting help (we will see soon, however, that RStudio will have these built right into its user interface), and other various other resources. For now click on the ‘Download R’ link as seen in Figure &lt;a href=&#34;#fig:r-install-1&#34;&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-install-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-install-1-1.png&#34; alt=&#34;R Project main page&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: R Project main page
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The list of sites here are ‘mirrors’, or alternative servers all over the world where the software is available. Any of these links will do the job, so let’s just select the first mirror ‘&lt;a href=&#34;https://cloud.r-project.org/&#34; class=&#34;uri&#34;&gt;https://cloud.r-project.org/&lt;/a&gt;’ for simplicity sake.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-install-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-install-2-1.png&#34; alt=&#34;Available mirror sites to download R&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Available mirror sites to download R
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Next select your operating system. I am working on a Mac so I will select the ‘OS X’ option.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-install-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-install-3-1.png&#34; alt=&#34;Operating system selection&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Operating system selection
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You will now need to scroll down the page a bit and find the link to most recent version of R. The current version at publication is &lt;code&gt;R-3.4.1.pkg&lt;/code&gt;. Download the &lt;code&gt;.pkg&lt;/code&gt; file to your hard drive.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-install-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-install-4-1.png&#34; alt=&#34;Download the most recent `.pkg` file&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Download the most recent &lt;code&gt;.pkg&lt;/code&gt; file
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;You can download the &lt;code&gt;.pkg&lt;/code&gt; file to any folder on your hard drive. After running this installation file, accept the prompt to move this file to the trash. You will not need it again.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Once it is on your hard drive, open the file by double-clicking it and follow the default installation instructions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-install-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-install-5-1.png&#34; alt=&#34;Run the `.pkg` file to install R on your machine.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 5: Run the &lt;code&gt;.pkg&lt;/code&gt; file to install R on your machine.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;To see what get’s installed on your machine you can click the ‘Customize’ link and see the the various pieces of software that are bundled with the &lt;code&gt;.pkg&lt;/code&gt; installer. Go ahead and do that and take a look.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-install-6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-install-6-1.png&#34; alt=&#34;View the default software installed with the R `.pkg` installer.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 6: View the default software installed with the R &lt;code&gt;.pkg&lt;/code&gt; installer.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now you can see the list of software. Let’s focus on two items in the list, seen in Figure &lt;a href=&#34;#fig:r-install-7&#34;&gt;7&lt;/a&gt;. First, you will see the R Framework, which is R itself. Among the other items we see the ‘R GUI’ in the list. This is a basic application interface to R. We’ll take a quick look at that application in the next section to begin our understanding of how to access R through applications and more robust IDEs.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-install-7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-install-7-1.png&#34; alt=&#34;List of software installed on your machine as part of the installation process.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 7: List of software installed on your machine as part of the installation process.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We won’t spend much time working with the R Application GUI in our series, and quickly move to using RStudio to interface R, but it’s worth knowing that it’s there and it will help us make a point about the distinction between R, the framework, and interfaces to R like this GUI and RStudio. Go ahead and finish the installation and move to the next section.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;working-with-the-r-console-in-the-default-r-application-gui&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Working with the R console in the default R Application GUI&lt;/h2&gt;
&lt;p&gt;Before we get to downloading and working with the RStudio IDE, let’s take a quick look at an alternative way of connecting to R: the R Application GUI. The R Application GUI is installed with together with R itself as mentioned in the previous section. Opening that software we can get our first glimpse at the R console and will run our first line of code.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-gui-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-gui-1-1.png&#34; alt=&#34;A view of the default R Application GUI.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 8: A view of the default R Application GUI.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The console is where we run R interactively. That is to say that the software interprets our code on a line by line basis as we go; a sort of call and response method. The prompt is the line where the computer is ready waiting for the user to enter code to be sent off to the interpreter. Let’s show you what this looks like in action with the most simple of all code; adding &lt;code&gt;1 + 1&lt;/code&gt;. So at the prompt type exactly that, then hit enter/ return and see what happens.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:r-gui-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/r-gui-2-1.png&#34; alt=&#34;Running `1 + 1` at the prompt in the R Application.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 9: Running &lt;code&gt;1 + 1&lt;/code&gt; at the prompt in the R Application.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Admittedly this is not the kind of stuff that will lead you to a romance with R! But it’s our first step and now we know what the console is, what a prompt is, and what an interactive session is.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-rstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why RStudio?&lt;/h2&gt;
&lt;p&gt;So to be clear, R is not an application. It is a programming language that resides deep on your computer. The R Application GUI is merely an interface to that language. The small bit of code we ran on through the console here is a direct link to the interpreter that talks to R itself. This is an important concept as you will see as we turn to working with a more powerful interface to R, RStudio. RStudio will provide a console interface to R as well as host of other extremely helpful features into our GUI experience to write code offline in the form of scripts, manage our scripts and other data resources, view data and plots, get help, and much, much more that will make working with R more efficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;downloading-and-installing-rstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Downloading and installing RStudio&lt;/h2&gt;
&lt;p&gt;To download RStudio, we navigate to the &lt;a href=&#34;https://www.rstudio.com/&#34;&gt;RStudio website’s homepage&lt;/a&gt;. Scroll down a bit and you will see the ‘Download’ button for RStudio.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-install-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-install-1-1.png&#34; alt=&#34;RStudio homepage.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 10: RStudio homepage.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Follow the ‘Download’ button. On this page you will be presented with various versions of the RStudio software. We want the ‘RStudio Desktop - Open source license’ version. Scroll down to the bottom of this table showing the various options you get with the other versions and click the big green ‘Download’ button.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-install-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-install-2-1.png&#34; alt=&#34;List of R Studio versions.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 11: List of R Studio versions.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Now select the installer that matches your operating system.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-install-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-install-3-1.png&#34; alt=&#34;List of R Studio installers for various operating systems.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 12: List of R Studio installers for various operating systems.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Again, download this file to any place on your hard drive that you desire. Double-click this file on your computer and follow the instructions to add the application to your machine.&lt;/p&gt;
&lt;p&gt;That’s it. You now have the most current version of R and RStudio on your machine. We’re ready to get familiar with the features of RStudio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-to-know-rstudio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting to know RStudio&lt;/h2&gt;
&lt;p&gt;Open RStudio. You will see an interface similar to the one below in Figure &lt;a href=&#34;#fig:rstudio-1&#34;&gt;13&lt;/a&gt;. Of course the aesthetics will vary depending on the operating system you are on but there may be other small differences in the tabs in each of the panes. As you work with RStudio you may add new functionality to facilitate certain tasks.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-1-1.png&#34; alt=&#34;First look at RStudio.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 13: First look at RStudio.
&lt;/p&gt;
&lt;/div&gt;
&lt;!-- Keep the description brief, as real understanding will come as we work on pratical applications of programming in R.  --&gt;
&lt;div id=&#34;rstudio-panes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;RStudio Panes&lt;/h3&gt;
&lt;p&gt;The layout of RStudio includes four main ‘panes’: Console, Editor, Files, and the Environment panes. You already are familiar with the idea of the console, seen here in Figure &lt;a href=&#34;#fig:rstudio-2&#34;&gt;14&lt;/a&gt;. Remember that this is the area that is a direct line between the IDE and the R interpreter. When RStudio is first launched, it will start an R session and this has the effect of reporting some details about the version of R that you are running and the operating system that it is running on.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-2-1.png&#34; alt=&#34;RStudio &amp;quot;Console&amp;quot; pane.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 14: RStudio “Console” pane.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The next logical pane to introduce is the Editor pane, seen in Figure &lt;a href=&#34;#fig:rstudio-3&#34;&gt;15&lt;/a&gt;. Whereas commands written in the console are run on a line by line basis interactively, the Editor will be a useful space for us to view, edit, and write R scripts, as well as other files. Scripts are simply files that contain a series of R commands that we can then run together at once, instead of line by line as we do in the console. This will be the main way we leverage the power of R; by create scripts that run through a data analysis workflow we can then save these scripts as files to run at a future point, continue to develop, revise, and even share them.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-3-1.png&#34; alt=&#34;RStudio &amp;quot;Editor&amp;quot; pane.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 15: RStudio “Editor” pane.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The Files pane has various tabs associated with it. When the ‘Files’ tab is selected we will see files and folders (aka directories) on our machine. As we begin working with R in the next section, we will begin to see how this tab will be extremely useful to see, edit, and delete our scripts, data files, and other associated project files without having to browse our system through as we would normally. There is also a set default tabs: ‘Plots’, ‘Packages’, and ‘Help’ in this pane. Plots will show and export any plots we create during our R session. The Packages tab allows us to manage packages either downloading or installing them. And the Help tab is where we access information about R in general, or particular packages, or functions. You will find that Help is really a godsend as very few R programmers can write code for a project from A to Z without refreshing their memory or exploring documentation on the usage of packages and functions.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-4-1.png&#34; alt=&#34;RStudio &amp;quot;Files&amp;quot; pane.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 16: RStudio “Files” pane.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The last of the four panes is the Environment pane. It includes a set of tabs as well. The main two are ‘Environment’ and ‘History’. The Environment tab allows us to see variables and objects that we create during our session. We haven’t discussed either of these concepts, but rest assured we will get there soon and you will understand how helpful this tab is to efficient R programming. History is just that; a history of the commands that have been sent to the R interpreter. These commands can come from our interactive session in the console or through running a series of commands in a script.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:rstudio-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/rstudio-5-1.png&#34; alt=&#34;RStudio &amp;quot;Environment&amp;quot; pane.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 17: RStudio “Environment” pane.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So there you have it a brief overview of RStudio panes and tabs. Together these tools will provide us ample resources for just about everything you will ever need to do data analysis with R. In the next section we will begin to see how we can leverage these resources in a basic R session.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;It’s worth noting that RStudio is highly customizable; panes can be customized in appearance, placement, and tabs available. But for now we will leave the default layout to maintain a consistent format throughout this series. If you do want to experiment with the look and feel of RStudio, you can peruse the options through the ‘Tools &amp;gt; Global Options’ in the application dropdown menu at the top of your screen.&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;div id=&#34;r-sessions-in-rstudio&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;R sessions in RStudio&lt;/h3&gt;
&lt;p&gt;To get a basic feel for working with R and RStudio let’s run through a basic example that will highlight each of the main panes and tabs that we covered in the previous section.&lt;/p&gt;
&lt;p&gt;To get started let’s run the same code we ran before in the R Application GUI console but now in the RStudio console. Type the following code in the Console and hit Enter on your keyboard.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;1 + 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On hitting Enter, the code is sent to the R interpreter which responds with the result; &lt;code&gt;2&lt;/code&gt;. For now ignore the &lt;code&gt;[1]&lt;/code&gt; that prefixes the line where the result is displayed.&lt;/p&gt;
&lt;p&gt;Now let’s run three separate lines of code in the Console in sequence. Take care to enter this and all subsequent code input correctly computers do not tolerate typos. Remember you’re the brains here, the computer is the brawn!&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Quick tip: You can add the assignment operator &lt;code&gt;&amp;lt;-&lt;/code&gt; via a keyboard shortcut by hitting &lt;code&gt;option + -&lt;/code&gt; (Mac) and &lt;code&gt;alt + -&lt;/code&gt; (PC).&lt;/p&gt;

&lt;/div&gt;

&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- 1:26&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y &amp;lt;- letters&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste(x, &amp;quot;-&amp;quot;, y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will notice that the first two commands did not return anything. This is because we sent the results from each of these commands to the variables &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;, respectively. A variable is a data container that is named. This container, or variable, holds the result in memory and gives us access to the result when we use the variable name later on. As you will soon see, you will inevitably create a number of variables in any given data analysis. There are two ways to see a listing of variables you have created. The first can be done in the Console by typing the function &lt;code&gt;ls()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ls()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;x&amp;quot; &amp;quot;y&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second is to browse to the Environment pane under the Environment tab. You will see that the Environment tab provides much more information that just the names of the variables in memory. It also indicates the type of each variable, its dimensions (in this case length as we are working with vectors), its memory size, and a summary of the values contained within.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-1&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-1-1.png&#34; alt=&#34;Viewing variables in the Environment tab.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 18: Viewing variables in the Environment tab.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In the third command we make use of the variables &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt;: pasting (with the built-in, or ‘base R’, function &lt;code&gt;paste()&lt;/code&gt;) the results from &lt;code&gt;x&lt;/code&gt; (a series of numbers from 1 to 26) and &lt;code&gt;y&lt;/code&gt; (the letters of the alphabet) together with a hyphen as a separator.&lt;/p&gt;
&lt;p&gt;If that was the result we were looking for then great. Job done. Imagine, however, that we wanted the result to return contiguous ‘number-hyphen-letter’ strings, like &lt;code&gt;1-a&lt;/code&gt;. Is that possible? Yes, it is. But don’t take my word for it, let’s find out using RStudio’s Help resources. Again, there are two ways to do this. The first is by inputting the code &lt;code&gt;?paste&lt;/code&gt; in the Console.&lt;/p&gt;
&lt;p&gt;By appending the &lt;code&gt;?&lt;/code&gt; operator to a function, package, or dataset name the Help tab in the Files pane is opened to the R documentation page.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-2&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-2-1.png&#34; alt=&#34;Viewing R documentation in the Help tab.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 19: Viewing R documentation in the Help tab.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The second approach is to manually browse to the Help tab and search for the function &lt;code&gt;paste&lt;/code&gt;. Either works, you might find yourself alternating between the two.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;In some cases you might not remember the name of the function, package, or dataset that you want to find documentation for. For a more general search you can use the &lt;code&gt;??&lt;/code&gt; operator instead of the &lt;code&gt;?&lt;/code&gt;. Or as an alternative use the search function in the Help tab.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Back to our code. The R documentation shows that &lt;code&gt;paste()&lt;/code&gt; can take an optional sub-command, or argument, that will allow us to choose how our paste variables are separated. With this knowledge in hand we can update our code. We will use the argument &lt;code&gt;sep = &amp;quot;&amp;quot;&lt;/code&gt; to remove white space between the concatenated elements like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;paste(x, &amp;quot;-&amp;quot;, y, sep = &amp;quot;&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;1-a&amp;quot;  &amp;quot;2-b&amp;quot;  &amp;quot;3-c&amp;quot;  &amp;quot;4-d&amp;quot;  &amp;quot;5-e&amp;quot;  &amp;quot;6-f&amp;quot;  &amp;quot;7-g&amp;quot;  &amp;quot;8-h&amp;quot;  &amp;quot;9-i&amp;quot;  &amp;quot;10-j&amp;quot;
## [11] &amp;quot;11-k&amp;quot; &amp;quot;12-l&amp;quot; &amp;quot;13-m&amp;quot; &amp;quot;14-n&amp;quot; &amp;quot;15-o&amp;quot; &amp;quot;16-p&amp;quot; &amp;quot;17-q&amp;quot; &amp;quot;18-r&amp;quot; &amp;quot;19-s&amp;quot; &amp;quot;20-t&amp;quot;
## [21] &amp;quot;21-u&amp;quot; &amp;quot;22-v&amp;quot; &amp;quot;23-w&amp;quot; &amp;quot;24-x&amp;quot; &amp;quot;25-y&amp;quot; &amp;quot;26-z&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With time you will become more accustomed to reading and making sense of the R documentation. For now, however, it is sufficient to have learned how and where to access this documentation.&lt;/p&gt;
&lt;p&gt;The previous code we have run is short and simple. Working with the Console is great for this type of quick and dirty exploring, or doing some introspection of variables using &lt;code&gt;ls()&lt;/code&gt; or the help operator &lt;code&gt;?&lt;/code&gt; to view the R help documentation. However, data analysis will involve many more lines of code and using the Console directly will become cumbersome. A more convenient way to write code is to use the Editor pane.&lt;/p&gt;
&lt;p&gt;Let’s run through a more involved and practical example of creating a basic word frequency analysis. Before we get to the code below, let me introduce you to installing and managing R packages. As we have seen a couple times now there is a Console-based method and an GUI-based method. This time I will start with the GUI method as it is very convenient and tends to be the method most use. First step is to open the Packages tab in the Files pane, seen below.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-3&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-3-1.png&#34; alt=&#34;The Packages tab.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 20: The Packages tab.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;You will see a list of the R packages that are already installed on your machine. The base installation of R comes with a default set so you will already see some packages listed. However, you may not see all of the packages that appear in Figure &lt;a href=&#34;#fig:R-basic-3&#34;&gt;20&lt;/a&gt; if you have not already manually installed new packages (for example the &lt;code&gt;acs&lt;/code&gt; package for accessing US Census data).&lt;/p&gt;
&lt;p&gt;Let’s install a few packages we need for the upcoming code: &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, and &lt;code&gt;tidytext&lt;/code&gt;. Installing packages with the Packages tab is easy. First, click on the ‘Install’ button within the tab.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-4&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-4-1.png&#34; alt=&#34;The packages installation dialogue box.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 21: The packages installation dialogue box.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Type the names &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;ggplot2&lt;/code&gt;, and &lt;code&gt;tidytext&lt;/code&gt; into the ‘Packages’ field leaving the other default configuration fields. You will notice as you type the package names, RStudio will pattern match the name which can be helpful to make sure you type the names correctly. Once you have the names entered, click the ‘Install’ button. At this point RStudio will run the installation code for these packages. As they install there will be quite a bit of output, some of it in red font, in the Console. When the installation is complete the prompt &lt;code&gt;&amp;gt;&lt;/code&gt; at the Console will appear; ready to take another command.&lt;/p&gt;
&lt;p&gt;The Console approach leverages the function &lt;code&gt;install.packages()&lt;/code&gt;. To find out more about how to use this function, I encourage you to look at the R documentation using &lt;code&gt;?install.packages&lt;/code&gt;. I will leave this as an exercise for you to complete. Now back to our example code that we will enter in the Editor pane.&lt;/p&gt;
&lt;p&gt;Copy and paste the following code into the Editor pane.&lt;/p&gt;
&lt;pre class=&#34;plain&#34;&gt;&lt;code&gt;library(dplyr) # data manipulation
library(ggplot2) # package for generating a word frequency plot
library(tidytext) # package for doing a word frequency analysis

text &amp;lt;- c(&amp;quot;The Fulton County Grand Jury said Friday an investigation of Atlanta&amp;#39;s recent primary election produced no evidence that any irregularities took place.&amp;quot;,
&amp;quot;The jury further said in term-end presentments that the City Executive Committee, which had over-all charge of the election, deserves the praise and thanks of the City of Atlanta for the manner in which the election was conducted.&amp;quot;,
&amp;quot;The September-*october term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible irregularities in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr..&amp;quot;,
&amp;quot;Only a relative handful of such reports was received, the jury said, considering the widespread interest in the election, the number of voters and the size of this city.&amp;quot;) # first 4 lines from the Brown Corpus

text_df &amp;lt;- data_frame(line = 1:4, text = text) # create a tabular dataset with the columns `line` and `text`

text.word.freq &amp;lt;- unnest_tokens(tbl = text_df, input = text, output = words) %&amp;gt;% # tokenize `text` into `words`
  count(words, sort = TRUE) %&amp;gt;% # count `words` and sort them
  head(10) # return only the first ten lines in the dataset

ggplot(data = text.word.freq, aes(x = reorder(words, desc(n)), y = n)) + geom_bar(stat = &amp;quot;identity&amp;quot;) + labs(x = &amp;quot;Words&amp;quot;, y = &amp;quot;Count&amp;quot;, title = &amp;quot;Word frequency plot&amp;quot;) # plot the `words` on the x-axis and the count `n` on the y-axis using a bar plot ordered by `n`&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Your code should look something similar to what you see in Figure &lt;a href=&#34;#fig:R-basic-5&#34;&gt;22&lt;/a&gt;. Note that I have resized the Editor pane to view most of the code.&lt;/p&gt;
&lt;!-- say something about code highlighting? --&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-5&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-5-1.png&#34; alt=&#34;Code for a basic word frequency analysis in the Editor pane.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 22: Code for a basic word frequency analysis in the Editor pane.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can run the code in the Editor pane either line by line or as a complete script. Running code line by line can be useful when you are composing a script and want to test the results incrementally. To do this you move your cursor to the line you would like to run and then simply hit the keystroke &lt;code&gt;command + enter&lt;/code&gt; (Mac) or &lt;code&gt;ctrl + enter&lt;/code&gt; (PC). Run line 1 and see what happens!&lt;/p&gt;
&lt;p&gt;Line 1 loads the &lt;code&gt;dplyr&lt;/code&gt; package from our package library with the command &lt;code&gt;library(dplyr)&lt;/code&gt;. If you resize the Console pane you will see that the command was run and resulted in some details about the package printed to the Console. We also notice that the cursor has moved to the next line in the Editor pane, ready for us to run this line.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-6&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-6-1.png&#34; alt=&#34;Running the first line of our basic word frequency analysis.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 23: Running the first line of our basic word frequency analysis.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In this script I’ve also added comments to each line of code describing what each command does in our script. Any time R finds a &lt;code&gt;#&lt;/code&gt; symbol it ignores everything to the right on the same line. We can then use plain language to annotate our code. Using commenting is a key best practice for coding in any programming language as it will make your code more legible to other users as well as the future you who might come back to this code sometime down the road to realize you have forgotten what your code does!&lt;/p&gt;
&lt;p&gt;Returning to running our code from the Editor, we can also run multiple lines, or even the entire script from the Editor pane in a similar fashion by selecting multiple lines, or all the lines and using the previous keystroke (&lt;code&gt;command + enter&lt;/code&gt;, or &lt;code&gt;ctrl + enter&lt;/code&gt;). RStudio provides a button ‘Run’ that can be used to run a script line by line or the ‘Source’ button to run the entire script.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-7-1.png&#34; alt=&#34;Running the entire script using the RStudio tool Source.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 24: Running the entire script using the RStudio tool Source.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Go ahead and run the entire script now with by clicking ‘Source’. The script will be automatically sent line by line sequentially to the Console. The results of this script generate a word frequency plot for the first four lines from the well-known &lt;a href=&#34;https://en.wikipedia.org/wiki/Brown_Corpus&#34;&gt;Brown Corpus&lt;/a&gt;. The Plots tab in the Files pane will automatically open revealing our plot.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-8&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-8-1.png&#34; alt=&#34;Resulting plot from our basic word frequency analysis script.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 25: Resulting plot from our basic word frequency analysis script.
&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;There are many other options for running code from the Editor pane using keyboard shortcuts. Explore the ‘Code &amp;gt; Run region’ dropdown from the RStudio menu bar for more information.&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;div id=&#34;saving-our-work&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Saving our work&lt;/h3&gt;
&lt;p&gt;At this point we may want to save the code to our hard drive to make sure we don’t lose our work, or to move on and write some other code. To save this R script navigate to the RStudio tool menu ‘File &amp;gt; Save’ or hit the keyboard shortcut &lt;code&gt;command + s&lt;/code&gt; (Mac) or &lt;code&gt;ctrl + s&lt;/code&gt; (PC). The next step is to choose where to save this file. Select a directory for this file to live. For now you can choose any directory you feel fit. As we move forward in this series and in more involved projects that you will work on, however, it’s best to do some organizational planning upfront to set up your main project directory, sub-directories, and so on so that it is clear where to save each type of file. This topic will be covered in an upcoming Recipe post dealing with project management using the RStudio ‘Projects’ feature.&lt;/p&gt;
&lt;p&gt;For testing purposes, let’s save the file within the current working directory inside a directory we will create named &lt;code&gt;getting_started/&lt;/code&gt;. What is the working directory? Well, it is the place that RStudio regards as “Home base” on your hard drive. To find out what this is, enter the function &lt;code&gt;getwd()&lt;/code&gt; at the console. R will return a path to the current working directory. On my machine the path is &lt;code&gt;/Users/francojc/Documents/Recipes&lt;/code&gt;. This notation is describes the hierarchical scheme of directories. My current working directory, then is the &lt;code&gt;Recipes/&lt;/code&gt; directory which is located inside the &lt;code&gt;Documents/&lt;/code&gt; directory which itself is located within my home directory &lt;code&gt;francojc/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s go ahead and open the ‘File &amp;gt; Save’ tool RStudio menu and name our file &lt;code&gt;basic_frequency_analysis.R&lt;/code&gt;. The use of &lt;code&gt;_&lt;/code&gt; here is to avoid white space in file and directory names. This is good practice as working with paths in programming can be complicated by white space in some environments. So it’s good practice to avoid white space. There are various styles that are employed in programming, &lt;a href=&#34;https://en.wikipedia.org/wiki/Programming_style&#34;&gt;generally&lt;/a&gt;, and in &lt;a href=&#34;http://adv-r.had.co.nz/Style.html&#34;&gt;R programming specifically&lt;/a&gt;. Just like commenting code, as mentioned previously, adopting accepted style guidelines can increase your code’s legibility for you and those who may work with code you share.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-9&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-9-1.png&#34; alt=&#34;&amp;quot;Save&amp;quot; dialogue in RStudio.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 26: “Save” dialogue in RStudio.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Once we have saved our file in our working directory, it will now appear in the Files tab in RStudio. From this tab you can move, rename, and delete files as well as create, rename, and delete directories as well. It’s also worth pointing out that the path to our working directory is visible in this Files tab between the working directory listing and the button toolbar. This is a nice feature to see where the files and directories live on your machine without resorting to the OS file explorer.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-10&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-10-1.png&#34; alt=&#34;Viewing files in the Files tab.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 27: Viewing files in the Files tab.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Let’s create our &lt;code&gt;getting_started/&lt;/code&gt; directory inside of our working directory to house this file. Click the ‘New Folder’ button inside the Files tab and name the new directory. It will now appear in our listing. To move our file inside of this directory we can use the OS to manually move the file, but RStudio again provides tools to do this. Select the checkbox beside the file &lt;code&gt;basic_frequency_analysis.R&lt;/code&gt;, then select ‘Move…’ from the ‘More’ dropdown menu.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-11&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-11-1.png&#34; alt=&#34;Moving a file to a new directory in the Files tab.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 28: Moving a file to a new directory in the Files tab.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;After you make the move, RStudio will quickly ask you if you want to close the &lt;code&gt;basic_frequency_analysis.R&lt;/code&gt; script as it no longer exists. It does exist, just not in the same location as before. This underscores the importance of paths in programming and programming tools like RStudio. It is therefore of utmost importance to be aware of the paths to files and directories. Luckily, RStudio provides us ample ways to monitor the paths to working files and directories!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-an-r-session&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing an R session&lt;/h3&gt;
&lt;p&gt;At this point let’s say it’s time to close our session and quit RStudio. You can do this by simply navigating to the RStudio tool menu ‘File &amp;gt; Quit session…’. You will be presented with a dialogue box to save the workspace or not.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-12&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-12-1.png&#34; alt=&#34;Quiting an R session.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 29: Quiting an R session.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can choose not to save the workspace and our files will be preserved (as long as they have been previously saved to the hard disk). What we lose, however, are the variables and history that RStudio has in memory at the current moment in our session. If you want to begin your next session with these variables and history loaded from the get-go, then we will want to save the workspace. Let’s choose to save the workspace to see this in action. After quitting RStudio, restart the application.
Because we had chosen to save the workspace we now have our variables and history in this new R session. You can view them in the Environment pane. If you navigate to the working directory in the Files tab in the Files tab you will also see a couple new files have been added to our files and directory list: &lt;code&gt;.RData&lt;/code&gt; and &lt;code&gt;.Rhistory&lt;/code&gt;.&lt;/p&gt;
&lt;!-- speak about issues created by saving the workspace environment. Write scripts/ projects with assumption that nothing is saved in the workspace environment --&gt;
&lt;div class=&#34;figure&#34;&gt;&lt;span id=&#34;fig:R-basic-13&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://francojc.github.io/post/2017-08-14-getting-started-with-r-and-rstudio_files/figure-html/R-basic-13-1.png&#34; alt=&#34;Workspace files `.RData` and `.Rhistory`.&#34; width=&#34;100%&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 30: Workspace files &lt;code&gt;.RData&lt;/code&gt; and &lt;code&gt;.Rhistory&lt;/code&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;When you restart another session, R will look for these files and load them into the workspace for you to pick up and continue using. If you choose to quit an R session without saving the workspace these files will not be created, or in the case they already exist from a previous workspace session, will not be overwritten and the earlier variables and history will be loaded.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;p&gt;If you browse the working directory of a session with your operating system’s file explorer you may not see these files listed. Do not be alarmed. By default an operating system hides all files that start with &lt;code&gt;.&lt;/code&gt; as a convenience to user. There are many of these types of files hanging around your OS. They often contain application-specific configuration information which usually do not require direct editing.&lt;/p&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;round-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Round up&lt;/h2&gt;
&lt;p&gt;We have covered a lot of ground in this post. You should now have a working understanding of how R and RStudio are related and the various panes that are available to develop code and manage your workspace. We have also scratched the surface on some topics that will be covered in more depth in future posts including variables, paths, and workspace and project management, to name a few. The next post in the Recipe series ‘Project management for scalable data analysis’ will deal with many of these topics as we set the foundation for working with more complex and realistic analyses.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Follow this link to &lt;a href=&#34;http://rapporter.net/custom/R-activity/#score/6&#34;&gt;see estimated figures of the number of R users around the world&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Think of Steve Jobs’ famous sound bite: ‘There’s an app for that’, just replace ‘app’ with ‘package’ in the case of R.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Don’t fret over some of the terminology here, we will come back to these terms later on with detailed description and examples.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Introducing the Recipe series</title>
      <link>https://francojc.github.io/2017/08/03/introducing-the-recipe-series/</link>
      <pubDate>Thu, 03 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://francojc.github.io/2017/08/03/introducing-the-recipe-series/</guid>
      <description>&lt;link href=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;https://francojc.github.io/rmarkdown-libs/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-recipe-series-an-overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Recipe series: an overview&lt;/h2&gt;
&lt;p&gt;My goal in this series is to explore the ‘why’ and the ‘how’ of doing quantitative language research. The content of this series will, in large part, overlap with resources available on doing Data Science, generally (see &lt;span class=&#34;citation&#34;&gt;(Wickham and Grolemund 2017)&lt;/span&gt;), or in field-specific areas and domains &lt;span class=&#34;citation&#34;&gt;(Beckerman, Childs, and Petchey 2017; Hodeghatta and Nayak 2016; Perlin 2017)&lt;/span&gt;. However, this series will focus exclusively on issues and methods concerning language data and linguistic analyses through practical data sources and realistic examples.&lt;/p&gt;
&lt;p&gt;I will assume little computing, programming, or statistical knowledge, but I hope to provide useful information for even the more experienced researchers and practicioners. Doing quantitative language research does not require programming skills, as there are many tools and &lt;a href=&#34;https://en.wikipedia.org/wiki/Graphical_user_interface&#34;&gt;graphical user interfaces&lt;/a&gt; (GUI) available to do basic to even quite complex language analyses without these skills, but programming, for most practicing data scientists, is a more efficient and effective strategy for approaching data analysis. This series will make use of the &lt;a href=&#34;https://www.r-project.org/about.html&#34;&gt;R programming language&lt;/a&gt; and the powerful &lt;a href=&#34;https://en.wikipedia.org/wiki/Integrated_development_environment&#34;&gt;Integrated Development Environment&lt;/a&gt; (IDE) &lt;a href=&#34;https://www.rstudio.com/products/RStudio/&#34;&gt;RStudio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Through R and RStudio I will cover organizational topics like software installation, accessing files, and project management all the way through reporting results and creating reproducible research. Along the way I will cover the fundamental concepts and methods for statistical language analysis including data acquisition, preparation, transformation, and visualization as well as data modeling for hypothesis testing and exploratory and predictive analysis.&lt;/p&gt;
&lt;p&gt;As a primary goal this series represents an effort to document and share the knowledge and skills I have acquired over the years with those that have an interest in quantitative language and/or programming with R. There is, however, a second, more selfish goal: I aim to learn a lot in the process through consolidating and conveying my knowledge as well as recieving comments, feedback, and corrections from the community. To this end, please don’t hesitate to contact me with ideas for posts or suggestions or alternative approaches to existing posts.&lt;/p&gt;
&lt;p&gt;OK. With that in mind, let’s get staRted!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Beckerman2017&#34;&gt;
&lt;p&gt;Beckerman, Andrew P., Dylan Z. Childs, and Owen L. Petchey. 2017. &lt;em&gt;Getting Started with R: An Introduction for Biologists&lt;/em&gt;. Second edi. Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Hodeghatta2016&#34;&gt;
&lt;p&gt;Hodeghatta, Umesh R, and Umesha Nayak. 2016. &lt;em&gt;Business Analytics Using R - A Practical Approach&lt;/em&gt;. First edit. Apress.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Perlin2017&#34;&gt;
&lt;p&gt;Perlin, Marcelo S. 2017. &lt;em&gt;Processing and Analyzing Financial Data with R&lt;/em&gt;. First edit. Agencia Brasileira de ISBN.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-Wickham2017&#34;&gt;
&lt;p&gt;Wickham, Hadley, and Garrett Grolemund. 2017. &lt;em&gt;R for Data Science&lt;/em&gt;. First edit. O’Reilly Media. &lt;a href=&#34;http://r4ds.had.co.nz/&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
